{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d243dc8",
   "metadata": {},
   "source": [
    "# Data Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd2cdb4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1613e79e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'generated', 'author': 'LangChain', 'pages': 1, 'date_created': '2024-06-15'}, page_content='This is a sample document.')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = Document(page_content=\"This is a sample document.\", \n",
    "metadata={\n",
    "    \"source\": \"generated\",\n",
    "    \"author\": \"LangChain\",\n",
    "    \"pages\": 1,\n",
    "    \"date_created\": \"2024-06-15\"\n",
    "    })\n",
    "# print(doc)\n",
    "\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e71e302d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple Text file\n",
    "import os\n",
    "os.makedirs(\"../datas/text_files\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "514acead",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = {\n",
    "    \"python.txt\":\n",
    "    '''What is Python? Executive Summary\n",
    "Python is an interpreted, object-oriented, high-level programming language with dynamic semantics. Its high-level built in data structures, combined with dynamic typing and dynamic binding, make it very attractive for Rapid Application Development, as well as for use as a scripting or glue language to connect existing components together. Python's simple, easy to learn syntax emphasizes readability and therefore reduces the cost of program maintenance. Python supports modules and packages, which encourages program modularity and code reuse. The Python interpreter and the extensive standard library are available in source or binary form without charge for all major platforms, and can be freely distributed.\n",
    "\n",
    "Often, programmers fall in love with Python because of the increased productivity it provides. Since there is no compilation step, the edit-test-debug cycle is incredibly fast. Debugging Python programs is easy: a bug or bad input will never cause a segmentation fault. Instead, when the interpreter discovers an error, it raises an exception. When the program doesn't catch the exception, the interpreter prints a stack trace. A source level debugger allows inspection of local and global variables, evaluation of arbitrary expressions, setting breakpoints, stepping through the code a line at a time, and so on. The debugger is written in Python itself, testifying to Python's introspective power. On the other hand, often the quickest way to debug a program is to add a few print statements to the source: the fast edit-test-debug cycle makes this simple approach very effective.\n",
    "\n",
    "See also some comparisons between Python and other languages.\n",
    "\n",
    "1. Introduction\n",
    "Python, renowned for its simplicity and readability, stands as one of the most versatile and widely adopted programming languages in the world today. Created by Guido van Rossum in the late 1980s, Python was designed with a focus on code readability, enabling developers to express concepts in fewer lines of code compared to languages like C++ or Java.\n",
    "\n",
    "The Significance of Python\n",
    "The importance of Python transcends traditional coding realms. Its versatility allows it to be employed in a multitude of applications, ranging from web development and scientific computing to data analysis, artificial intelligence, and more. Python's extensive library ecosystem empowers developers with pre-built modules and packages, easing the implementation of complex functionalities. This language's adaptability is showcased by its role in some of the most prominent technological advances of our time.\n",
    "\n",
    "In web development, frameworks like Django and Flask have propelled Python to the forefront, enabling developers to build robust and scalable applications. In the realm of data science, Python, along with libraries like Pandas, NumPy, and Matplotlib, has become the de facto choice for data manipulation, analysis, and visualization. Additionally, Python's prowess in artificial intelligence and machine learning is exemplified by the popularity of libraries such as TensorFlow and PyTorch.\n",
    "\n",
    "Beyond these domains, Python finds application in automation, scripting, game development, and more. Its straightforward syntax and vast community support make it an ideal choice for both novice programmers and seasoned developers alike.\n",
    "\n",
    "In this article, we embark on a journey through the foundational aspects of Python programming, equipping you with the skills to leverage this versatile language for your own projects and endeavors.\n",
    "\n",
    "2. Getting Started with Python\n",
    "Python’s Genesis and Guido van Rossum\n",
    "Python, conceived in the late 1980s by Guido van Rossum, was designed with a vision to create a language that emphasized code readability and maintainability. Its name is a nod to the British comedy group Monty Python, highlighting the language's penchant for humor and accessibility.\n",
    "\n",
    "Installing Python\n",
    "Before we dive into Python programming, you'll need to set up Python on your system. Follow these steps based on your operating system:\n",
    "\n",
    "- For Windows:\n",
    "1. Visit the official Python website at python.org.\n",
    "2. Navigate to the \"Downloads\" section.\n",
    "3. Select the latest version compatible with your system (usually recommended for most users).\n",
    "4. Check the box that says \"Add Python X.X to PATH\" during installation.\n",
    "5. Click \"Install Now\" and follow the on-screen prompts.\n",
    "\n",
    "- For macOS:\n",
    "1. Also, Visit python.org.\n",
    "2. Navigate to the \"Downloads\" section.\n",
    "3. Select the latest version compatible with your system (usually recommended for most users).\n",
    "4. Run the installer and follow the on-screen instructions.\n",
    "\n",
    "- For Linux:\n",
    "- Python is often pre-installed in many Linux distributions. To check if it’s installed, open a terminal and type `python --version`. If Python is not installed, you can install it via your package manager (e.g., `sudo apt install python3` for Ubuntu).\n",
    "\n",
    "Choosing an IDE or Text Editor\n",
    "Once Python is installed, you'll need an environment to write and run your code. Here are a few popular options:\n",
    "\n",
    "- PyCharm:\n",
    "- PyCharm is a powerful IDE known for its intelligent code assistance, debugging capabilities, and extensive plugin ecosystem. It’s suitable for both beginners and experienced developers.\n",
    "\n",
    "- Jupyter Notebook:\n",
    "- Jupyter Notebook provides an interactive environment for running code snippets. It’s particularly useful for data analysis, experimentation, and creating interactive documents.\n",
    "\n",
    "- Visual Studio Code (VSCode):\n",
    "- VSCode is a lightweight, open-source code editor that supports Python with extensions. It offers a rich set of features, including debugging, version control, and a thriving community.\n",
    "\n",
    "Selecting the right environment largely depends on your personal preferences and the nature of your projects. Experiment with a few to find the one that best suits your workflow.\n",
    "\n",
    "3. Python Basics\n",
    "Writing and Running a Simple Python Program\n",
    "Let's kickstart your Python journey by writing and running a basic program. Open your chosen Python environment (IDE or text editor), and type the following:\n",
    "\n",
    "print(\"Hello, Python!\")\n",
    "Save this file with a `.py` extension (e.g., `hello.py`). Then, in your terminal or command prompt, navigate to the directory containing the file and execute it by typing `python hello.py`. You should see the output: `Hello, Python!`.\n",
    "\n",
    "Variables and Data Types\n",
    "In Python, variables are like containers that hold data. They can store various types of information such as numbers, text, and more. Here are some essential data types:\n",
    "\n",
    "- Integer (int): Represents whole numbers (positive or negative), e.g., `5`, `-10`.\n",
    "- Float (float): Represents decimal numbers, e.g., `3.14`, `-0.001`.\n",
    "- String (str): Represents text, enclosed in either single or double quotes, e.g., `’Hello’`, `\"Python\"`.\n",
    "\n",
    "To declare a variable, you simply assign a value to it:\n",
    "\n",
    "age = 25\n",
    "pi = 3.14\n",
    "name = 'Alice'\n",
    "Operators\n",
    "Python supports a variety of operators for performing operations on variables and values.\n",
    "\n",
    "- Arithmetic Operators (+, -, \\*, /, %,**):\n",
    "- Addition, subtraction, multiplication, division, modulus (remainder), exponentiation.\n",
    "\n",
    "- Comparison Operators (==, !=, <, >, <=, >=):\n",
    "- Compare values and return `True` or `False`.\n",
    "\n",
    "- Logical Operators (and, or, not):\n",
    "- Perform logical operations on `True` and `False` values.\n",
    "\n",
    "Here's a quick example illustrating these operators:\n",
    "\n",
    "x = 10\n",
    "y = 5\n",
    "\n",
    "# Arithmetic\n",
    "sum_result = x + y\n",
    "difference = x - y\n",
    "product = x * y\n",
    "quotient = x / y\n",
    "\n",
    "# Comparison\n",
    "is_equal = x == y\n",
    "is_greater = x > y\n",
    "\n",
    "# Logical\n",
    "logical_and = (x > 0) and (y < 10)\n",
    "logical_or = (x > 0) or (y > 10)\n",
    "logical_not = not(x > 0)\n",
    "Understanding and using these concepts will serve as a strong foundation for your Python programming journey.\n",
    "\n",
    "4. Control Flow\n",
    "Conditional Statements (if-else)\n",
    "Conditional statements allow your program to make decisions based on certain conditions. They are pivotal for executing different code blocks depending on the input or circumstances.\n",
    "\n",
    "# Example 1: Simple if-else statement\n",
    "age = 20\n",
    "\n",
    "if age >= 18:\n",
    "    print(\"You are eligible to vote.\")\n",
    "else:\n",
    "    print(\"You are not eligible to vote yet.\")\n",
    "In this example, if the condition `age >= 18` evaluates to `True`, the first block of code (indented under `if`) will be executed. Otherwise, the block of code under `else` will be executed.\n",
    "\n",
    "# Example 2: Chained conditions with elif\n",
    "score = 85\n",
    "\n",
    "if score >= 90:\n",
    "    print(\"You got an A!\")\n",
    "elif score >= 80:\n",
    "    print(\"You got a B.\")\n",
    "elif score >= 70:\n",
    "    print(\"You got a C.\")\n",
    "else:\n",
    "    print(\"You need to improve.\")\n",
    "Here, the program checks multiple conditions one after another. If the first condition is not met, it moves to the next `elif` statement. If none of the conditions are met, the code under `else` is executed.\n",
    "\n",
    "Loops (for, while)\n",
    "Loops are fundamental for executing a block of code repeatedly.\n",
    "\n",
    "For Loop Example:\n",
    "\n",
    "# Example 1: Iterating over a list\n",
    "fruits = ['apple', 'banana', 'cherry']\n",
    "\n",
    "for fruit in fruits:\n",
    "    print(fruit)\n",
    "This loop iterates through the list of fruits and prints each one.\n",
    "\n",
    "# Example 2: Using range() for a specified number of iterations\n",
    "for i in range(5):\n",
    "    print(i)\n",
    "This loop uses `range(5)` to iterate from `0` to `4`, printing each number.\n",
    "\n",
    "While Loop Example:\n",
    "\n",
    "# Example: Countdown using a while loop\n",
    "count = 5\n",
    "\n",
    "while count > 0:\n",
    "    print(count)\n",
    "    count -= 1\n",
    "This while loop counts down from 5 to 1.\n",
    "\n",
    "Choosing Between for and while Loops\n",
    "When deciding between a `for` loop and a `while` loop, consider the following:\n",
    "\n",
    "Use a `for` loop when:\n",
    "\n",
    "- You know the number of iterations in advance.\n",
    "- You're iterating over a sequence, like a list or a range of numbers.\n",
    "- You want to iterate through a collection or perform a specific action a fixed number of times.\n",
    "\n",
    "Example:\n",
    "\n",
    "for i in range(5):\n",
    "    print(i)\n",
    "Use a `while` loop when:\n",
    "\n",
    "- You don't know the number of iterations in advance or want to loop until a specific condition is met.\n",
    "- The condition for terminating the loop may change during runtime.\n",
    "\n",
    "Example:\n",
    "\n",
    "count = 5\n",
    "while count > 0:\n",
    "    print(count)\n",
    "    count -= 1\n",
    "Remember, `while` loops rely on a condition to stop execution, which means they can potentially run indefinitely if the condition is not met. Always ensure there's a mechanism to break out of a `while` loop.\n",
    "\n",
    "Loops provide a powerful mechanism for automating repetitive tasks in your programs.\n",
    "\n",
    "5. Data Structures in Python\n",
    "Lists, Tuples, and Dictionaries\n",
    "Python provides a rich set of data structures to handle different types of data efficiently.\n",
    "\n",
    "- Lists:\n",
    "- Lists are ordered collections of elements that can be of any data type (including mixed types).\n",
    "- They are mutable, meaning you can modify their contents after creation.\n",
    "\n",
    "- Example:\n",
    "\n",
    "numbers = [1, 2, 3, 4, 5]\n",
    "fruits = [’apple’, 'banana’, 'cherry’]\n",
    "- Use Cases: Storing and manipulating sequences of items, such as a list of numbers or names.\n",
    "\n",
    "- Tuples:\n",
    "- Tuples are similar to lists, but they are immutable, meaning their elements cannot be changed after creation.\n",
    "\n",
    "- Example:\n",
    "\n",
    "coordinates = (3, 5)\n",
    "colors = (’red’, 'green’, 'blue’)\n",
    "- Use Cases: Representing collections of related data, like coordinates or settings.\n",
    "\n",
    "- Dictionaries:\n",
    "- Dictionaries store data in key-value pairs, allowing for fast retrieval based on keys.\n",
    "\n",
    "- They are unordered and mutable.\n",
    "\n",
    "- Example:\n",
    "\n",
    "person = {’name’: 'John Doe’, 'age’: 30, 'city’: 'New York’}\n",
    "- Use Cases: Storing and retrieving information based on labels or IDs, like user profiles or configurations.\n",
    "\n",
    "Strings and String Manipulation\n",
    "Strings are sequences of characters, and Python provides powerful tools for working with them.\n",
    "\n",
    "- Concatenation:\n",
    "- Concatenation allows you to combine two or more strings into one.\n",
    "\n",
    "- Example:\n",
    "\n",
    "first_name = 'John'\n",
    "last_name = 'Doe'full_name = first_name + ' ' + last_name\n",
    "- Slicing:\n",
    "- Slicing allows you to extract parts of a string.\n",
    "\n",
    "- Example:\n",
    "\n",
    "message = 'Hello, Python!'\n",
    "sub_message = message[7:13]  # Output: 'Python'\n",
    "- String Formatting:\n",
    "- String formatting enables you to construct strings dynamically with variables.\n",
    "''',\n",
    "\n",
    "\"machine_learning.txt\":\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "Machine Learning (ML) is a branch of artificial intelligence (AI) that allows computers to learn and make predictions or decisions without being explicitly programmed for each task. Instead, use data to “train” to understand patterns, make predictions, and improve performance over time.\n",
    "\n",
    "In simpler terms, ML enables computers to automatically improve their performance on a task through experience. Just like humans learn from their experiences, machines learn from the data provided to them.\n",
    "\n",
    "Types of Machine Learning\n",
    "There are three main types of machine learning:\n",
    "1. Supervised Learning\n",
    "2. Unsupervised Learning\n",
    "3. Reinforcement Learning\n",
    "\n",
    "Let’s break down each type and see how they work.\n",
    "\n",
    "Supervised Learning\n",
    "In Supervised Learning, the model is trained on labeled data, meaning each input (the data) has a corresponding output (the label). The model learns the relationship between inputs and outputs and can then predict outputs for new, unseen data.\n",
    "\n",
    "Example:\n",
    "- Suppose you have a dataset of houses where each house is described by its features (size, number of rooms, location) and a label (price of the house). You can train a supervised learning model to predict the price of a new house based on its features.\n",
    "\n",
    "Simple Analogy:\n",
    "Think of a student learning math with a teacher. The teacher (data labels) gives the student (the model) correct answers during practice. Over time, the student learns to solve similar problems on their own.\n",
    "\n",
    "Unsupervised Learning\n",
    "In Unsupervised Learning, the data does not have labels. The model is given only the inputs and must find patterns or relationships between them. It often groups similar data points together.\n",
    "\n",
    "Example:\n",
    "- Imagine you have a large set of images of different animals, but none of the images are labeled (no “dog,” “cat,” etc.). An unsupervised learning algorithm could group similar images together, forming clusters, even if it doesn’t know what the animals are.\n",
    "\n",
    "Simple Analogy:\n",
    "It’s like a person sorting different types of fruits without knowing their names. The person groups similar-looking fruits together (apples in one group, oranges in another) without knowing exactly what they are.\n",
    "\n",
    "Reinforcement Learning\n",
    "Reinforcement Learning (RL) involves an agent (a model) that learns through trial and error by interacting with an environment. The agent receives rewards for good actions and penalties for bad actions and adjusts its behavior to maximize rewards over time.\n",
    "\n",
    "Example:\n",
    "- In a video game, an RL agent learns to play by making moves and receiving points (rewards) or losing lives (penalties). Over time, it figures out the best strategies to win the game.\n",
    "\n",
    "Simple Analogy:\n",
    "Imagine teaching a pet a new trick. Every time the pet performs the trick correctly, you give it a treat (reward). If the pet does something wrong, you don’t give a treat (penalty). Over time, the pet learns to perform the trick correctly to get the treat.\n",
    "\n",
    "Differences Between Supervised, Unsupervised, and Reinforcement Learning\n",
    "Press enter or click to view image in full size\n",
    "\n",
    "Differences Between Supervised, Unsupervised, and Reinforcement Learning\n",
    "Supervised Learning: Train and Test a Model (Simple Example)\n",
    "In Supervised Learning, we train a model using a dataset where we already know the correct answers (labels). After training, we evaluate the model’s performance on a separate “test set” to see how well it can predict new data.\n",
    "\n",
    "Steps:\n",
    "\n",
    "1. Train the Model: Use a portion of the data (the training set) to teach the model.\n",
    "2. Test the Model: Use the remaining data (the test set) to evaluate the model’s performance on unseen examples.\n",
    "\n",
    "Example (Predicting House Prices)\n",
    "\n",
    "Let’s use a simple example of training and testing a model to predict house prices based on features like size and number of rooms. We will use a supervised learning approach, and sklearn library.\n",
    "\n",
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Create a simple dataset for house prices\n",
    "# Features: size (in square feet), number of rooms\n",
    "# Target: house price in $1000s\n",
    "data = {\n",
    "    'size': [1500, 1800, 2400, 3000, 3500, 4000, 4500, 5000, 5500, 6000],\n",
    "    'rooms': [3, 3, 4, 4, 5, 5, 6, 6, 6, 7],\n",
    "    'price': [300, 320, 400, 450, 500, 540, 600, 620, 670, 700]\n",
    "}\n",
    "\n",
    "# Convert the dictionary to a pandas DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Define the features (X) and the target (y)\n",
    "X = df[['size', 'rooms']]  # Input features\n",
    "y = df['price']  # Target variable\n",
    "\n",
    "# Split the data into training and testing sets (80% training, 20% testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, \n",
    "                                                              random_state=42)\n",
    "\n",
    "# Train a Linear Regression model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model using Mean Squared Error (MSE)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "# Output the results\n",
    "print(f\"Predicted prices: {y_pred}\")\n",
    "print(f\"Actual prices: {y_test.values}\")\n",
    "print(f\"Mean Squared Error: {mse:.2f}\")\n",
    "Why Split Data?\n",
    "- Training Set: Used to teach the model by showing it examples with known answers.\n",
    "- Test Set: Used to see how well the model performs on data it has never seen before. This helps check if the model is overfitting (performing well on training but poorly on new data) or generalizing well.\n",
    "\n",
    "Predictions are made on the test set, and the Mean Squared Error (MSE) is calculated to measure how well the model is performing. The lower the MSE, the better the model’s predictions.\n",
    "\n",
    "Machine Learning offers powerful tools for making predictions and uncovering insights from data. By understanding the key types (Supervised, Unsupervised, and Reinforcement Learning) and using simple models, you can begin to explore its potential. Supervised learning is often the easiest to start with since you have labeled data, and you can quickly test models with real-world applications like house price prediction or flower classification.\n",
    "\n",
    "Regression vs. Classification\n",
    "In machine learning, there are two main types of tasks:\n",
    "\n",
    "Classification\n",
    "Regression\n",
    "1. Classification\n",
    "Classification involves predicting a category or class label. The output is discrete, meaning the model tries to classify data into predefined labels or groups.\n",
    "\n",
    "Example:\n",
    "\n",
    "Predicting whether an email is “spam” or “not spam” (two distinct classes).\n",
    "Classifying a flower as one of three species based on its features.\n",
    "2. Regression\n",
    "Regression involves predicting a continuous value or quantity. The output is continuous, meaning the model predicts a value on a numerical scale.\n",
    "\n",
    "Example:\n",
    "\n",
    "Predicting the price of a house based on its features (like size, number of rooms, location).\n",
    "Forecasting the temperature tomorrow based on historical data.\n",
    "Understanding Linear Regression\n",
    "Linear regression is one of the most basic types of regression analysis. It is used to predict a continuous value by modeling the relationship between an independent variable (input) and a dependent variable (output).\n",
    "\n",
    "Linear Regression Example\n",
    "Above example of predicting house prices based on the size of the house was an example of linear regression. Linear regression would model this relationship using a straight line:\n",
    "\n",
    "Press enter or click to view image in full size\n",
    "\n",
    "Linear Regression\n",
    "Press enter or click to view image in full size\n",
    "\n",
    "Linear Regression\n",
    "We plot the original data and the fitted line to visually see how well the model captures the relationship.\n",
    "\n",
    "Multivariable (Multiple) Linear Regression\n",
    "Multiple Linear Regression is an extension of simple linear regression where we use multiple input features (variables) to predict the output.\n",
    "\n",
    "Instead of just predicting house prices based on size, we could also consider the number of rooms, location, or age of the house. The equation would now look like this:\n",
    "\n",
    "Press enter or click to view image in full size\n",
    "\n",
    "Multivariable Regression\n",
    "Understanding the Cost Function in Machine Learning\n",
    "A cost function (sometimes called a loss function or error function) is a mathematical function that tells us how far our model’s predictions are from the actual results. It essentially gives a score to a model’s performance: the higher the score, the worse the model; the lower the score, the better the model.\n",
    "\n",
    "The ultimate goal is to minimize the cost function, which would mean that the model’s predictions are as close to the actual values as possible.\n",
    "\n",
    "In the case of Gradient Descent, the algorithm adjusts the parameters in such a way that the cost function decreases with each iteration until it reaches a minimum value (the lowest point in the cost function). At this minimum, the model is making the best possible predictions given the training data.\n",
    "\n",
    "Types of Cost Functions in Machine Learning\n",
    "1. For Regression (e.g., Linear Regression)*:\n",
    "The most common cost function for linear regression is the Mean Squared Error (MSE), which looks at the difference between the predicted and actual values, squares them (to avoid negative differences), and averages them over the entire dataset.\n",
    "\n",
    "Press enter or click to view image in full size\n",
    "\n",
    "MSE\n",
    "Where:\n",
    "\n",
    "n is the number of training examples.\n",
    "y(i) is the predicted value for the i-th training example.\n",
    "y(i)^ is the actual value for the i-th training example.\n",
    "\n",
    "2. For Classification (e.g., Logistic Regression):\n",
    "The cost function is often the Logarithmic Loss (also known as Cross-Entropy Loss), which measures the error in classifying between categories. For binary classification, this function looks like:\n",
    "Press enter or click to view image in full size\n",
    "\n",
    "log loss\n",
    "Complexity with Multiple Variables (Features)\n",
    "As we move from simple linear regression (with one feature) to multiple linear regression (with several features), the complexity of the model increases. When we add more features to our dataset, it becomes challenging to compute the optimal values for the parameters (coefficients) using analytical methods like the Gradient descent.\n",
    "\n",
    "Limitations of Linear Regression\n",
    "Linear regression, though a very powerful algorithm, has certain disadvantages\n",
    "\n",
    "Get Rishabh Singh’s stories in your inbox\n",
    "Join Medium for free to get updates from this writer.\n",
    "\n",
    "Enter your email\n",
    "Subscribe\n",
    "1. Main limitation of Linear Regression is the assumption of linearity between the dependent variable and the independent variables. In the real world, the data is almost never linearly separable. The assumption that there is a straight-line relationship is usually wrong.\n",
    "\n",
    "2. Prone to noise and overfitting: If the number of observations are lesser than the number of features, Linear Regression should not be used, otherwise it may lead to overfit, and the relationship thus formed will be noisy.\n",
    "\n",
    "3. Prone to outliers: Linear regression is very sensitive to outliers. An outlier can be considered as an anomaly. It refers to a datapoint which has no clear relationship with any other data point in the data. So, outliers should be analyzed and removed before applying Linear Regression to the dataset, or the linear relationship formed would be highly skewed.\n",
    "\n",
    "\n",
    "Linear line formed will not correctly predict the results of data points (shown in blue)\n",
    "Gradient Descent\n",
    "Gradient Descent is an optimization algorithm used to minimize the cost function by iteratively updating the parameters (coefficients). Gradient Descent iteratively moves toward the optimal solution by following the slope of the cost function.\n",
    "\n",
    "Press enter or click to view image in full size\n",
    "\n",
    "Intuition Behind Gradient Descent\n",
    "Imagine you are standing on a mountain peak and want to reach the lowest point (valley). You can’t see where the valley is because you’re blindfolded. The only thing you can do is feel the ground near you and step in the direction where the slope decreases.\n",
    "\n",
    "Press enter or click to view image in full size\n",
    "\n",
    "In this analogy:\n",
    "\n",
    "The mountain is the cost function.\n",
    "The goal is to minimize the cost, i.e., find the point with the lowest value (the valley).\n",
    "Each step you take corresponds to updating the parameters (coefficients) of the model.\n",
    "How Gradient Descent Works:\n",
    "Initialize Parameters: Start by assigning random values to the parameters (coefficients).\n",
    "Compute the Cost Function: The cost function represents the error between the predicted values and actual values. In linear regression, this is often the Mean Squared Error.\n",
    "Calculate the Gradient (Slope): Compute the slope of the cost function with respect to each parameter. This slope tells us the direction to move the parameters to reduce the cost.\n",
    "Update Parameters: Adjust the parameters using the gradient and a learning rate (which controls the size of the steps).\n",
    "Repeat: Continue this process until the parameters converge (when further updates make minimal improvements).\n",
    "Importance of Learning Rate\n",
    "The learning rate (α) is a crucial hyperparameter that controls how large each update step is. Learning rate controls how much the coefficients can change on each iteration. If the learning rate is too large, the algorithm might overshoot the minimum and fail to converge. If it’s too small, the algorithm might take too long to find the minimum.\n",
    "\n",
    "Press enter or click to view image in full size\n",
    "\n",
    "Learning Rate\n",
    "Small Learning Rate: Slow convergence, but more precise.\n",
    "Large Learning Rate: Faster, but risks overshooting and not finding the minimum.\n",
    "\n",
    "Feature Scaling in Machine Learning\n",
    "When working with machine learning algorithms, the features (input variables) can often have different scales. For instance, if one feature is measured in kilometers and another in meters, the range of values can differ significantly. Feature scaling helps normalize or standardize these features, ensuring that the model treats them equally during training.\n",
    "\n",
    "Why Feature Scaling is Important?\n",
    "Many machine learning algorithms, especially those based on distance or gradient descent, are sensitive to the scale of the input features. Some examples include:\n",
    "\n",
    "Gradient Descent: The convergence of gradient descent is faster when features are on a similar scale. Without scaling, features with larger ranges dominate the optimization process, making it inefficient.\n",
    "Types of Feature Scaling\n",
    "Min-Max Normalization (Rescaling)\n",
    "Standardization (Z-score scaling)\n",
    "Let’s explore both techniques with simple examples.\n",
    "\n",
    "1. Min-Max Normalization\n",
    "Min-Max normalization scales the data to a fixed range, typically between 0 and 1. Each feature’s minimum value becomes 0, and the maximum value becomes 1.\n",
    "\n",
    "Press enter or click to view image in full size\n",
    "\n",
    "Where:\n",
    "\n",
    "X is the original feature value,\n",
    "Xmin, Xmax​ are the minimum and maximum values of that feature.\n",
    "When to Use Min-Max Scaling?\n",
    "Use Min-Max scaling when you know that the distribution of your data does not contain extreme outliers and is relatively uniform.\n",
    "It’s commonly used in algorithms like KNN, which are based on distances between data points.\n",
    "2. Standardization (Z-score Scaling)\n",
    "Standardization (also called Z-score scaling) transforms the data to have a mean of 0 and a standard deviation of 1. It centers the data by subtracting the mean and then scales by dividing by the standard deviation.\n",
    "\n",
    "\n",
    "Where:\n",
    "\n",
    "X is the original feature value,\n",
    "μ is the mean of the feature,\n",
    "σ is the standard deviation of the feature.\n",
    "When to Use Standardization?\n",
    "Standardization is useful when your data has outliers or when the distribution of features is not uniform.\n",
    "It’s widely used in algorithms that rely on the Gaussian distribution, such as logistic regression, linear regression, and support vector machines.\n",
    "When is Feature Scaling Not Necessary?\n",
    "Not all algorithms are sensitive to the scale of features. For example:\n",
    "\n",
    "Tree-based models (like Decision Trees, Random Forests) do not require feature scaling since they are based on splitting points in the data and are not sensitive to the relative scales of the features.\n",
    "Naive Bayes is also insensitive to feature scaling because it relies on probabilities rather than distance or magnitude.\n",
    "Using Min-Max Scaling with scikit-learn:\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "\n",
    "# Original data\n",
    "data = np.array([[50, 30], [60, 90], [70, 100]])\n",
    "# Initialize MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "# Scale the data\n",
    "scaled_data = scaler.fit_transform(data)\n",
    "print(\"Scaled Data using Min-Max Scaling:\\n\", scaled_data)\n",
    "Using Standardization with scikit-learn:\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# Initialize StandardScaler\n",
    "scaler = StandardScaler()\n",
    "# Scale the data\n",
    "scaled_data = scaler.fit_transform(data)\n",
    "print(\"Scaled Data using Standardization:\\n\", scaled_data)\n",
    "This is just introduction to ML, we will learn more about ML is detail in future blogs…Stay tuned'''\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "49ec6eb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created file: python.txt\n",
      "Created file: machine_learning.txt\n",
      "All sample text files created successfully.\n"
     ]
    }
   ],
   "source": [
    "for filename, content in sample_text.items():\n",
    "    with open(f\"../datas/text_files/{filename}\", \"w\") as f:\n",
    "        f.write(content)\n",
    "    print(f\"Created file: {filename}\")\n",
    "\n",
    "print(\"All sample text files created successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e9caec",
   "metadata": {},
   "source": [
    "## Read using TextLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "61b859df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "loader = TextLoader(\"../datas/text_files/python.txt\")\n",
    "\n",
    "documents = loader.load()\n",
    "\n",
    "# print(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "50c3329c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c48507fc",
   "metadata": {},
   "source": [
    "## Read using DirectoryLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1e7e5b7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 861.78it/s]\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "directory_loader = DirectoryLoader(\n",
    "    \"../datas\", \n",
    "    glob=\"**/*.txt\",\n",
    "    loader_cls=TextLoader,\n",
    "    loader_kwargs={'encoding': 'utf-8'},\n",
    "    show_progress=True\n",
    "    )\n",
    "docs = directory_loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3b27f6c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': '../datas/text_files/machine_learning.txt'}, page_content='\\n\\n\\n\\nMachine Learning (ML) is a branch of artificial intelligence (AI) that allows computers to learn and make predictions or decisions without being explicitly programmed for each task. Instead, use data to “train” to understand patterns, make predictions, and improve performance over time.\\n\\nIn simpler terms, ML enables computers to automatically improve their performance on a task through experience. Just like humans learn from their experiences, machines learn from the data provided to them.\\n\\nTypes of Machine Learning\\nThere are three main types of machine learning:\\n1. Supervised Learning\\n2. Unsupervised Learning\\n3. Reinforcement Learning\\n\\nLet’s break down each type and see how they work.\\n\\nSupervised Learning\\nIn Supervised Learning, the model is trained on labeled data, meaning each input (the data) has a corresponding output (the label). The model learns the relationship between inputs and outputs and can then predict outputs for new, unseen data.\\n\\nExample:\\n- Suppose you have a dataset of houses where each house is described by its features (size, number of rooms, location) and a label (price of the house). You can train a supervised learning model to predict the price of a new house based on its features.\\n\\nSimple Analogy:\\nThink of a student learning math with a teacher. The teacher (data labels) gives the student (the model) correct answers during practice. Over time, the student learns to solve similar problems on their own.\\n\\nUnsupervised Learning\\nIn Unsupervised Learning, the data does not have labels. The model is given only the inputs and must find patterns or relationships between them. It often groups similar data points together.\\n\\nExample:\\n- Imagine you have a large set of images of different animals, but none of the images are labeled (no “dog,” “cat,” etc.). An unsupervised learning algorithm could group similar images together, forming clusters, even if it doesn’t know what the animals are.\\n\\nSimple Analogy:\\nIt’s like a person sorting different types of fruits without knowing their names. The person groups similar-looking fruits together (apples in one group, oranges in another) without knowing exactly what they are.\\n\\nReinforcement Learning\\nReinforcement Learning (RL) involves an agent (a model) that learns through trial and error by interacting with an environment. The agent receives rewards for good actions and penalties for bad actions and adjusts its behavior to maximize rewards over time.\\n\\nExample:\\n- In a video game, an RL agent learns to play by making moves and receiving points (rewards) or losing lives (penalties). Over time, it figures out the best strategies to win the game.\\n\\nSimple Analogy:\\nImagine teaching a pet a new trick. Every time the pet performs the trick correctly, you give it a treat (reward). If the pet does something wrong, you don’t give a treat (penalty). Over time, the pet learns to perform the trick correctly to get the treat.\\n\\nDifferences Between Supervised, Unsupervised, and Reinforcement Learning\\nPress enter or click to view image in full size\\n\\nDifferences Between Supervised, Unsupervised, and Reinforcement Learning\\nSupervised Learning: Train and Test a Model (Simple Example)\\nIn Supervised Learning, we train a model using a dataset where we already know the correct answers (labels). After training, we evaluate the model’s performance on a separate “test set” to see how well it can predict new data.\\n\\nSteps:\\n\\n1. Train the Model: Use a portion of the data (the training set) to teach the model.\\n2. Test the Model: Use the remaining data (the test set) to evaluate the model’s performance on unseen examples.\\n\\nExample (Predicting House Prices)\\n\\nLet’s use a simple example of training and testing a model to predict house prices based on features like size and number of rooms. We will use a supervised learning approach, and sklearn library.\\n\\n# Import necessary libraries\\nimport numpy as np\\nimport pandas as pd\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.linear_model import LinearRegression\\nfrom sklearn.metrics import mean_squared_error\\n\\n# Create a simple dataset for house prices\\n# Features: size (in square feet), number of rooms\\n# Target: house price in $1000s\\ndata = {\\n    \\'size\\': [1500, 1800, 2400, 3000, 3500, 4000, 4500, 5000, 5500, 6000],\\n    \\'rooms\\': [3, 3, 4, 4, 5, 5, 6, 6, 6, 7],\\n    \\'price\\': [300, 320, 400, 450, 500, 540, 600, 620, 670, 700]\\n}\\n\\n# Convert the dictionary to a pandas DataFrame\\ndf = pd.DataFrame(data)\\n\\n# Define the features (X) and the target (y)\\nX = df[[\\'size\\', \\'rooms\\']]  # Input features\\ny = df[\\'price\\']  # Target variable\\n\\n# Split the data into training and testing sets (80% training, 20% testing)\\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, \\n                                                              random_state=42)\\n\\n# Train a Linear Regression model\\nmodel = LinearRegression()\\nmodel.fit(X_train, y_train)\\n\\n# Make predictions on the test set\\ny_pred = model.predict(X_test)\\n\\n# Evaluate the model using Mean Squared Error (MSE)\\nmse = mean_squared_error(y_test, y_pred)\\n\\n# Output the results\\nprint(f\"Predicted prices: {y_pred}\")\\nprint(f\"Actual prices: {y_test.values}\")\\nprint(f\"Mean Squared Error: {mse:.2f}\")\\nWhy Split Data?\\n- Training Set: Used to teach the model by showing it examples with known answers.\\n- Test Set: Used to see how well the model performs on data it has never seen before. This helps check if the model is overfitting (performing well on training but poorly on new data) or generalizing well.\\n\\nPredictions are made on the test set, and the Mean Squared Error (MSE) is calculated to measure how well the model is performing. The lower the MSE, the better the model’s predictions.\\n\\nMachine Learning offers powerful tools for making predictions and uncovering insights from data. By understanding the key types (Supervised, Unsupervised, and Reinforcement Learning) and using simple models, you can begin to explore its potential. Supervised learning is often the easiest to start with since you have labeled data, and you can quickly test models with real-world applications like house price prediction or flower classification.\\n\\nRegression vs. Classification\\nIn machine learning, there are two main types of tasks:\\n\\nClassification\\nRegression\\n1. Classification\\nClassification involves predicting a category or class label. The output is discrete, meaning the model tries to classify data into predefined labels or groups.\\n\\nExample:\\n\\nPredicting whether an email is “spam” or “not spam” (two distinct classes).\\nClassifying a flower as one of three species based on its features.\\n2. Regression\\nRegression involves predicting a continuous value or quantity. The output is continuous, meaning the model predicts a value on a numerical scale.\\n\\nExample:\\n\\nPredicting the price of a house based on its features (like size, number of rooms, location).\\nForecasting the temperature tomorrow based on historical data.\\nUnderstanding Linear Regression\\nLinear regression is one of the most basic types of regression analysis. It is used to predict a continuous value by modeling the relationship between an independent variable (input) and a dependent variable (output).\\n\\nLinear Regression Example\\nAbove example of predicting house prices based on the size of the house was an example of linear regression. Linear regression would model this relationship using a straight line:\\n\\nPress enter or click to view image in full size\\n\\nLinear Regression\\nPress enter or click to view image in full size\\n\\nLinear Regression\\nWe plot the original data and the fitted line to visually see how well the model captures the relationship.\\n\\nMultivariable (Multiple) Linear Regression\\nMultiple Linear Regression is an extension of simple linear regression where we use multiple input features (variables) to predict the output.\\n\\nInstead of just predicting house prices based on size, we could also consider the number of rooms, location, or age of the house. The equation would now look like this:\\n\\nPress enter or click to view image in full size\\n\\nMultivariable Regression\\nUnderstanding the Cost Function in Machine Learning\\nA cost function (sometimes called a loss function or error function) is a mathematical function that tells us how far our model’s predictions are from the actual results. It essentially gives a score to a model’s performance: the higher the score, the worse the model; the lower the score, the better the model.\\n\\nThe ultimate goal is to minimize the cost function, which would mean that the model’s predictions are as close to the actual values as possible.\\n\\nIn the case of Gradient Descent, the algorithm adjusts the parameters in such a way that the cost function decreases with each iteration until it reaches a minimum value (the lowest point in the cost function). At this minimum, the model is making the best possible predictions given the training data.\\n\\nTypes of Cost Functions in Machine Learning\\n1. For Regression (e.g., Linear Regression)*:\\nThe most common cost function for linear regression is the Mean Squared Error (MSE), which looks at the difference between the predicted and actual values, squares them (to avoid negative differences), and averages them over the entire dataset.\\n\\nPress enter or click to view image in full size\\n\\nMSE\\nWhere:\\n\\nn is the number of training examples.\\ny(i) is the predicted value for the i-th training example.\\ny(i)^ is the actual value for the i-th training example.\\n\\n2. For Classification (e.g., Logistic Regression):\\nThe cost function is often the Logarithmic Loss (also known as Cross-Entropy Loss), which measures the error in classifying between categories. For binary classification, this function looks like:\\nPress enter or click to view image in full size\\n\\nlog loss\\nComplexity with Multiple Variables (Features)\\nAs we move from simple linear regression (with one feature) to multiple linear regression (with several features), the complexity of the model increases. When we add more features to our dataset, it becomes challenging to compute the optimal values for the parameters (coefficients) using analytical methods like the Gradient descent.\\n\\nLimitations of Linear Regression\\nLinear regression, though a very powerful algorithm, has certain disadvantages\\n\\nGet Rishabh Singh’s stories in your inbox\\nJoin Medium for free to get updates from this writer.\\n\\nEnter your email\\nSubscribe\\n1. Main limitation of Linear Regression is the assumption of linearity between the dependent variable and the independent variables. In the real world, the data is almost never linearly separable. The assumption that there is a straight-line relationship is usually wrong.\\n\\n2. Prone to noise and overfitting: If the number of observations are lesser than the number of features, Linear Regression should not be used, otherwise it may lead to overfit, and the relationship thus formed will be noisy.\\n\\n3. Prone to outliers: Linear regression is very sensitive to outliers. An outlier can be considered as an anomaly. It refers to a datapoint which has no clear relationship with any other data point in the data. So, outliers should be analyzed and removed before applying Linear Regression to the dataset, or the linear relationship formed would be highly skewed.\\n\\n\\nLinear line formed will not correctly predict the results of data points (shown in blue)\\nGradient Descent\\nGradient Descent is an optimization algorithm used to minimize the cost function by iteratively updating the parameters (coefficients). Gradient Descent iteratively moves toward the optimal solution by following the slope of the cost function.\\n\\nPress enter or click to view image in full size\\n\\nIntuition Behind Gradient Descent\\nImagine you are standing on a mountain peak and want to reach the lowest point (valley). You can’t see where the valley is because you’re blindfolded. The only thing you can do is feel the ground near you and step in the direction where the slope decreases.\\n\\nPress enter or click to view image in full size\\n\\nIn this analogy:\\n\\nThe mountain is the cost function.\\nThe goal is to minimize the cost, i.e., find the point with the lowest value (the valley).\\nEach step you take corresponds to updating the parameters (coefficients) of the model.\\nHow Gradient Descent Works:\\nInitialize Parameters: Start by assigning random values to the parameters (coefficients).\\nCompute the Cost Function: The cost function represents the error between the predicted values and actual values. In linear regression, this is often the Mean Squared Error.\\nCalculate the Gradient (Slope): Compute the slope of the cost function with respect to each parameter. This slope tells us the direction to move the parameters to reduce the cost.\\nUpdate Parameters: Adjust the parameters using the gradient and a learning rate (which controls the size of the steps).\\nRepeat: Continue this process until the parameters converge (when further updates make minimal improvements).\\nImportance of Learning Rate\\nThe learning rate (α) is a crucial hyperparameter that controls how large each update step is. Learning rate controls how much the coefficients can change on each iteration. If the learning rate is too large, the algorithm might overshoot the minimum and fail to converge. If it’s too small, the algorithm might take too long to find the minimum.\\n\\nPress enter or click to view image in full size\\n\\nLearning Rate\\nSmall Learning Rate: Slow convergence, but more precise.\\nLarge Learning Rate: Faster, but risks overshooting and not finding the minimum.\\n\\nFeature Scaling in Machine Learning\\nWhen working with machine learning algorithms, the features (input variables) can often have different scales. For instance, if one feature is measured in kilometers and another in meters, the range of values can differ significantly. Feature scaling helps normalize or standardize these features, ensuring that the model treats them equally during training.\\n\\nWhy Feature Scaling is Important?\\nMany machine learning algorithms, especially those based on distance or gradient descent, are sensitive to the scale of the input features. Some examples include:\\n\\nGradient Descent: The convergence of gradient descent is faster when features are on a similar scale. Without scaling, features with larger ranges dominate the optimization process, making it inefficient.\\nTypes of Feature Scaling\\nMin-Max Normalization (Rescaling)\\nStandardization (Z-score scaling)\\nLet’s explore both techniques with simple examples.\\n\\n1. Min-Max Normalization\\nMin-Max normalization scales the data to a fixed range, typically between 0 and 1. Each feature’s minimum value becomes 0, and the maximum value becomes 1.\\n\\nPress enter or click to view image in full size\\n\\nWhere:\\n\\nX is the original feature value,\\nXmin, Xmax\\u200b are the minimum and maximum values of that feature.\\nWhen to Use Min-Max Scaling?\\nUse Min-Max scaling when you know that the distribution of your data does not contain extreme outliers and is relatively uniform.\\nIt’s commonly used in algorithms like KNN, which are based on distances between data points.\\n2. Standardization (Z-score Scaling)\\nStandardization (also called Z-score scaling) transforms the data to have a mean of 0 and a standard deviation of 1. It centers the data by subtracting the mean and then scales by dividing by the standard deviation.\\n\\n\\nWhere:\\n\\nX is the original feature value,\\nμ is the mean of the feature,\\nσ is the standard deviation of the feature.\\nWhen to Use Standardization?\\nStandardization is useful when your data has outliers or when the distribution of features is not uniform.\\nIt’s widely used in algorithms that rely on the Gaussian distribution, such as logistic regression, linear regression, and support vector machines.\\nWhen is Feature Scaling Not Necessary?\\nNot all algorithms are sensitive to the scale of features. For example:\\n\\nTree-based models (like Decision Trees, Random Forests) do not require feature scaling since they are based on splitting points in the data and are not sensitive to the relative scales of the features.\\nNaive Bayes is also insensitive to feature scaling because it relies on probabilities rather than distance or magnitude.\\nUsing Min-Max Scaling with scikit-learn:\\nfrom sklearn.preprocessing import MinMaxScaler\\nimport numpy as np\\n\\n# Original data\\ndata = np.array([[50, 30], [60, 90], [70, 100]])\\n# Initialize MinMaxScaler\\nscaler = MinMaxScaler()\\n# Scale the data\\nscaled_data = scaler.fit_transform(data)\\nprint(\"Scaled Data using Min-Max Scaling:\\n\", scaled_data)\\nUsing Standardization with scikit-learn:\\nfrom sklearn.preprocessing import StandardScaler\\n# Initialize StandardScaler\\nscaler = StandardScaler()\\n# Scale the data\\nscaled_data = scaler.fit_transform(data)\\nprint(\"Scaled Data using Standardization:\\n\", scaled_data)\\nThis is just introduction to ML, we will learn more about ML is detail in future blogs…Stay tuned'),\n",
       " Document(metadata={'source': '../datas/text_files/python.txt'}, page_content='What is Python? Executive Summary\\nPython is an interpreted, object-oriented, high-level programming language with dynamic semantics. Its high-level built in data structures, combined with dynamic typing and dynamic binding, make it very attractive for Rapid Application Development, as well as for use as a scripting or glue language to connect existing components together. Python\\'s simple, easy to learn syntax emphasizes readability and therefore reduces the cost of program maintenance. Python supports modules and packages, which encourages program modularity and code reuse. The Python interpreter and the extensive standard library are available in source or binary form without charge for all major platforms, and can be freely distributed.\\n\\nOften, programmers fall in love with Python because of the increased productivity it provides. Since there is no compilation step, the edit-test-debug cycle is incredibly fast. Debugging Python programs is easy: a bug or bad input will never cause a segmentation fault. Instead, when the interpreter discovers an error, it raises an exception. When the program doesn\\'t catch the exception, the interpreter prints a stack trace. A source level debugger allows inspection of local and global variables, evaluation of arbitrary expressions, setting breakpoints, stepping through the code a line at a time, and so on. The debugger is written in Python itself, testifying to Python\\'s introspective power. On the other hand, often the quickest way to debug a program is to add a few print statements to the source: the fast edit-test-debug cycle makes this simple approach very effective.\\n\\nSee also some comparisons between Python and other languages.\\n\\n1. Introduction\\nPython, renowned for its simplicity and readability, stands as one of the most versatile and widely adopted programming languages in the world today. Created by Guido van Rossum in the late 1980s, Python was designed with a focus on code readability, enabling developers to express concepts in fewer lines of code compared to languages like C++ or Java.\\n\\nThe Significance of Python\\nThe importance of Python transcends traditional coding realms. Its versatility allows it to be employed in a multitude of applications, ranging from web development and scientific computing to data analysis, artificial intelligence, and more. Python\\'s extensive library ecosystem empowers developers with pre-built modules and packages, easing the implementation of complex functionalities. This language\\'s adaptability is showcased by its role in some of the most prominent technological advances of our time.\\n\\nIn web development, frameworks like Django and Flask have propelled Python to the forefront, enabling developers to build robust and scalable applications. In the realm of data science, Python, along with libraries like Pandas, NumPy, and Matplotlib, has become the de facto choice for data manipulation, analysis, and visualization. Additionally, Python\\'s prowess in artificial intelligence and machine learning is exemplified by the popularity of libraries such as TensorFlow and PyTorch.\\n\\nBeyond these domains, Python finds application in automation, scripting, game development, and more. Its straightforward syntax and vast community support make it an ideal choice for both novice programmers and seasoned developers alike.\\n\\nIn this article, we embark on a journey through the foundational aspects of Python programming, equipping you with the skills to leverage this versatile language for your own projects and endeavors.\\n\\n2. Getting Started with Python\\nPython’s Genesis and Guido van Rossum\\nPython, conceived in the late 1980s by Guido van Rossum, was designed with a vision to create a language that emphasized code readability and maintainability. Its name is a nod to the British comedy group Monty Python, highlighting the language\\'s penchant for humor and accessibility.\\n\\nInstalling Python\\nBefore we dive into Python programming, you\\'ll need to set up Python on your system. Follow these steps based on your operating system:\\n\\n- For Windows:\\n1. Visit the official Python website at python.org.\\n2. Navigate to the \"Downloads\" section.\\n3. Select the latest version compatible with your system (usually recommended for most users).\\n4. Check the box that says \"Add Python X.X to PATH\" during installation.\\n5. Click \"Install Now\" and follow the on-screen prompts.\\n\\n- For macOS:\\n1. Also, Visit python.org.\\n2. Navigate to the \"Downloads\" section.\\n3. Select the latest version compatible with your system (usually recommended for most users).\\n4. Run the installer and follow the on-screen instructions.\\n\\n- For Linux:\\n- Python is often pre-installed in many Linux distributions. To check if it’s installed, open a terminal and type `python --version`. If Python is not installed, you can install it via your package manager (e.g., `sudo apt install python3` for Ubuntu).\\n\\nChoosing an IDE or Text Editor\\nOnce Python is installed, you\\'ll need an environment to write and run your code. Here are a few popular options:\\n\\n- PyCharm:\\n- PyCharm is a powerful IDE known for its intelligent code assistance, debugging capabilities, and extensive plugin ecosystem. It’s suitable for both beginners and experienced developers.\\n\\n- Jupyter Notebook:\\n- Jupyter Notebook provides an interactive environment for running code snippets. It’s particularly useful for data analysis, experimentation, and creating interactive documents.\\n\\n- Visual Studio Code (VSCode):\\n- VSCode is a lightweight, open-source code editor that supports Python with extensions. It offers a rich set of features, including debugging, version control, and a thriving community.\\n\\nSelecting the right environment largely depends on your personal preferences and the nature of your projects. Experiment with a few to find the one that best suits your workflow.\\n\\n3. Python Basics\\nWriting and Running a Simple Python Program\\nLet\\'s kickstart your Python journey by writing and running a basic program. Open your chosen Python environment (IDE or text editor), and type the following:\\n\\nprint(\"Hello, Python!\")\\nSave this file with a `.py` extension (e.g., `hello.py`). Then, in your terminal or command prompt, navigate to the directory containing the file and execute it by typing `python hello.py`. You should see the output: `Hello, Python!`.\\n\\nVariables and Data Types\\nIn Python, variables are like containers that hold data. They can store various types of information such as numbers, text, and more. Here are some essential data types:\\n\\n- Integer (int): Represents whole numbers (positive or negative), e.g., `5`, `-10`.\\n- Float (float): Represents decimal numbers, e.g., `3.14`, `-0.001`.\\n- String (str): Represents text, enclosed in either single or double quotes, e.g., `’Hello’`, `\"Python\"`.\\n\\nTo declare a variable, you simply assign a value to it:\\n\\nage = 25\\npi = 3.14\\nname = \\'Alice\\'\\nOperators\\nPython supports a variety of operators for performing operations on variables and values.\\n\\n- Arithmetic Operators (+, -, \\\\*, /, %,**):\\n- Addition, subtraction, multiplication, division, modulus (remainder), exponentiation.\\n\\n- Comparison Operators (==, !=, <, >, <=, >=):\\n- Compare values and return `True` or `False`.\\n\\n- Logical Operators (and, or, not):\\n- Perform logical operations on `True` and `False` values.\\n\\nHere\\'s a quick example illustrating these operators:\\n\\nx = 10\\ny = 5\\n\\n# Arithmetic\\nsum_result = x + y\\ndifference = x - y\\nproduct = x * y\\nquotient = x / y\\n\\n# Comparison\\nis_equal = x == y\\nis_greater = x > y\\n\\n# Logical\\nlogical_and = (x > 0) and (y < 10)\\nlogical_or = (x > 0) or (y > 10)\\nlogical_not = not(x > 0)\\nUnderstanding and using these concepts will serve as a strong foundation for your Python programming journey.\\n\\n4. Control Flow\\nConditional Statements (if-else)\\nConditional statements allow your program to make decisions based on certain conditions. They are pivotal for executing different code blocks depending on the input or circumstances.\\n\\n# Example 1: Simple if-else statement\\nage = 20\\n\\nif age >= 18:\\n    print(\"You are eligible to vote.\")\\nelse:\\n    print(\"You are not eligible to vote yet.\")\\nIn this example, if the condition `age >= 18` evaluates to `True`, the first block of code (indented under `if`) will be executed. Otherwise, the block of code under `else` will be executed.\\n\\n# Example 2: Chained conditions with elif\\nscore = 85\\n\\nif score >= 90:\\n    print(\"You got an A!\")\\nelif score >= 80:\\n    print(\"You got a B.\")\\nelif score >= 70:\\n    print(\"You got a C.\")\\nelse:\\n    print(\"You need to improve.\")\\nHere, the program checks multiple conditions one after another. If the first condition is not met, it moves to the next `elif` statement. If none of the conditions are met, the code under `else` is executed.\\n\\nLoops (for, while)\\nLoops are fundamental for executing a block of code repeatedly.\\n\\nFor Loop Example:\\n\\n# Example 1: Iterating over a list\\nfruits = [\\'apple\\', \\'banana\\', \\'cherry\\']\\n\\nfor fruit in fruits:\\n    print(fruit)\\nThis loop iterates through the list of fruits and prints each one.\\n\\n# Example 2: Using range() for a specified number of iterations\\nfor i in range(5):\\n    print(i)\\nThis loop uses `range(5)` to iterate from `0` to `4`, printing each number.\\n\\nWhile Loop Example:\\n\\n# Example: Countdown using a while loop\\ncount = 5\\n\\nwhile count > 0:\\n    print(count)\\n    count -= 1\\nThis while loop counts down from 5 to 1.\\n\\nChoosing Between for and while Loops\\nWhen deciding between a `for` loop and a `while` loop, consider the following:\\n\\nUse a `for` loop when:\\n\\n- You know the number of iterations in advance.\\n- You\\'re iterating over a sequence, like a list or a range of numbers.\\n- You want to iterate through a collection or perform a specific action a fixed number of times.\\n\\nExample:\\n\\nfor i in range(5):\\n    print(i)\\nUse a `while` loop when:\\n\\n- You don\\'t know the number of iterations in advance or want to loop until a specific condition is met.\\n- The condition for terminating the loop may change during runtime.\\n\\nExample:\\n\\ncount = 5\\nwhile count > 0:\\n    print(count)\\n    count -= 1\\nRemember, `while` loops rely on a condition to stop execution, which means they can potentially run indefinitely if the condition is not met. Always ensure there\\'s a mechanism to break out of a `while` loop.\\n\\nLoops provide a powerful mechanism for automating repetitive tasks in your programs.\\n\\n5. Data Structures in Python\\nLists, Tuples, and Dictionaries\\nPython provides a rich set of data structures to handle different types of data efficiently.\\n\\n- Lists:\\n- Lists are ordered collections of elements that can be of any data type (including mixed types).\\n- They are mutable, meaning you can modify their contents after creation.\\n\\n- Example:\\n\\nnumbers = [1, 2, 3, 4, 5]\\nfruits = [’apple’, \\'banana’, \\'cherry’]\\n- Use Cases: Storing and manipulating sequences of items, such as a list of numbers or names.\\n\\n- Tuples:\\n- Tuples are similar to lists, but they are immutable, meaning their elements cannot be changed after creation.\\n\\n- Example:\\n\\ncoordinates = (3, 5)\\ncolors = (’red’, \\'green’, \\'blue’)\\n- Use Cases: Representing collections of related data, like coordinates or settings.\\n\\n- Dictionaries:\\n- Dictionaries store data in key-value pairs, allowing for fast retrieval based on keys.\\n\\n- They are unordered and mutable.\\n\\n- Example:\\n\\nperson = {’name’: \\'John Doe’, \\'age’: 30, \\'city’: \\'New York’}\\n- Use Cases: Storing and retrieving information based on labels or IDs, like user profiles or configurations.\\n\\nStrings and String Manipulation\\nStrings are sequences of characters, and Python provides powerful tools for working with them.\\n\\n- Concatenation:\\n- Concatenation allows you to combine two or more strings into one.\\n\\n- Example:\\n\\nfirst_name = \\'John\\'\\nlast_name = \\'Doe\\'full_name = first_name + \\' \\' + last_name\\n- Slicing:\\n- Slicing allows you to extract parts of a string.\\n\\n- Example:\\n\\nmessage = \\'Hello, Python!\\'\\nsub_message = message[7:13]  # Output: \\'Python\\'\\n- String Formatting:\\n- String formatting enables you to construct strings dynamically with variables.\\n')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75a9d44",
   "metadata": {},
   "source": [
    "## Load Pdf Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d63e76f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:02<00:00,  1.81it/s]\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyMuPDFLoader, PyPDFLoader\n",
    "directory_loader = DirectoryLoader(\n",
    "    \"../datas\", \n",
    "    glob=\"**/*.pdf\",\n",
    "    loader_cls=PyMuPDFLoader,\n",
    "    # loader_kwargs={'encoding': 'utf-8'},\n",
    "    show_progress=True\n",
    "    )\n",
    "pdf_documents = directory_loader.load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b4550bc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "94"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pdf_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8c47cb8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'source': '../datas/pdf_files/object_detection.pdf', 'file_path': '../datas/pdf_files/object_detection.pdf', 'total_pages': 21, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'trapped': '', 'modDate': 'D:20190417004522Z', 'creationDate': 'D:20190417004522Z', 'page': 0}, page_content='THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION\\n1\\nObject Detection with Deep Learning: A Review\\nZhong-Qiu Zhao, Member, IEEE, Peng Zheng,\\nShou-tao Xu, and Xindong Wu, Fellow, IEEE\\nAbstract—Due to object detection’s close relationship with\\nvideo analysis and image understanding, it has attracted much\\nresearch attention in recent years. Traditional object detection\\nmethods are built on handcrafted features and shallow trainable\\narchitectures. Their performance easily stagnates by constructing\\ncomplex ensembles which combine multiple low-level image\\nfeatures with high-level context from object detectors and scene\\nclassiﬁers. With the rapid development in deep learning, more\\npowerful tools, which are able to learn semantic, high-level,\\ndeeper features, are introduced to address the problems existing\\nin traditional architectures. These models behave differently\\nin network architecture, training strategy and optimization\\nfunction, etc. In this paper, we provide a review on deep\\nlearning based object detection frameworks. Our review begins\\nwith a brief introduction on the history of deep learning and\\nits representative tool, namely Convolutional Neural Network\\n(CNN). Then we focus on typical generic object detection\\narchitectures along with some modiﬁcations and useful tricks\\nto improve detection performance further. As distinct speciﬁc\\ndetection tasks exhibit different characteristics, we also brieﬂy\\nsurvey several speciﬁc tasks, including salient object detection,\\nface detection and pedestrian detection. Experimental analyses\\nare also provided to compare various methods and draw some\\nmeaningful conclusions. Finally, several promising directions and\\ntasks are provided to serve as guidelines for future work in\\nboth object detection and relevant neural network based learning\\nsystems.\\nIndex Terms—deep learning, object detection, neural network\\nI. INTRODUCTION\\nT\\nO gain a complete image understanding, we should not\\nonly concentrate on classifying different images, but\\nalso try to precisely estimate the concepts and locations of\\nobjects contained in each image. This task is referred as object\\ndetection [1][S1], which usually consists of different subtasks\\nsuch as face detection [2][S2], pedestrian detection [3][S2]\\nand skeleton detection [4][S3]. As one of the fundamental\\ncomputer vision problems, object detection is able to provide\\nvaluable information for semantic understanding of images\\nand videos, and is related to many applications, including\\nimage classiﬁcation [5], [6], human behavior analysis [7][S4],\\nface recognition [8][S5] and autonomous driving [9], [10].\\nMeanwhile, Inheriting from neural networks and related learn-\\ning systems, the progress in these ﬁelds will develop neural\\nnetwork algorithms, and will also have great impacts on object\\ndetection techniques which can be considered as learning\\nsystems. [11]–[14][S6]. However, due to large variations in\\nviewpoints, poses, occlusions and lighting conditions, it’s difﬁ-\\ncult to perfectly accomplish object detection with an additional\\nZhong-Qiu Zhao, Peng Zheng and Shou-Tao Xu are with the College of\\nComputer Science and Information Engineering, Hefei University of Technol-\\nogy, China. Xindong Wu is with the School of Computing and Informatics,\\nUniversity of Louisiana at Lafayette, USA.\\nManuscript received August xx, 2017; revised xx xx, 2017.\\nobject localization task. So much attention has been attracted\\nto this ﬁeld in recent years [15]–[18].\\nThe problem deﬁnition of object detection is to determine\\nwhere objects are located in a given image (object localization)\\nand which category each object belongs to (object classiﬁca-\\ntion). So the pipeline of traditional object detection models\\ncan be mainly divided into three stages: informative region\\nselection, feature extraction and classiﬁcation.\\nInformative region selection. As different objects may appear\\nin any positions of the image and have different aspect ratios\\nor sizes, it is a natural choice to scan the whole image with a\\nmulti-scale sliding window. Although this exhaustive strategy\\ncan ﬁnd out all possible positions of the objects, its short-\\ncomings are also obvious. Due to a large number of candidate\\nwindows, it is computationally expensive and produces too\\nmany redundant windows. However, if only a ﬁxed number of\\nsliding window templates are applied, unsatisfactory regions\\nmay be produced.\\nFeature extraction. To recognize different objects, we need\\nto extract visual features which can provide a semantic and\\nrobust representation. SIFT [19], HOG [20] and Haar-like [21]\\nfeatures are the representative ones. This is due to the fact\\nthat these features can produce representations associated with\\ncomplex cells in human brain [19]. However, due to the diver-\\nsity of appearances, illumination conditions and backgrounds,\\nit’s difﬁcult to manually design a robust feature descriptor to\\nperfectly describe all kinds of objects.\\nClassiﬁcation. Besides, a classiﬁer is needed to distinguish\\na target object from all the other categories and to make the\\nrepresentations more hierarchical, semantic and informative\\nfor visual recognition. Usually, the Supported Vector Machine\\n(SVM) [22], AdaBoost [23] and Deformable Part-based Model\\n(DPM) [24] are good choices. Among these classiﬁers, the\\nDPM is a ﬂexible model by combining object parts with\\ndeformation cost to handle severe deformations. In DPM, with\\nthe aid of a graphical model, carefully designed low-level\\nfeatures and kinematically inspired part decompositions are\\ncombined. And discriminative learning of graphical models\\nallows for building high-precision part-based models for a\\nvariety of object classes.\\nBased on these discriminant local feature descriptors and\\nshallow learnable architectures, state of the art results have\\nbeen obtained on PASCAL VOC object detection competition\\n[25] and real-time embedded systems have been obtained with\\na low burden on hardware. However, small gains are obtained\\nduring 2010-2012 by only building ensemble systems and\\nemploying minor variants of successful methods [15]. This fact\\nis due to the following reasons: 1) The generation of candidate\\nbounding boxes with a sliding window strategy is redundant,\\ninefﬁcient and inaccurate. 2) The semantic gap cannot be\\narXiv:1807.05511v2  [cs.CV]  16 Apr 2019'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'source': '../datas/pdf_files/object_detection.pdf', 'file_path': '../datas/pdf_files/object_detection.pdf', 'total_pages': 21, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'trapped': '', 'modDate': 'D:20190417004522Z', 'creationDate': 'D:20190417004522Z', 'page': 1}, page_content='THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION\\n2\\nPedestrian \\ndetection\\nSalient object \\ndetection \\nFace\\ndetection \\nGeneric object \\ndetection\\nObject \\ndetection\\nBounding box \\nregression\\nLocal contrast \\nSegmentation\\nMulti-feature\\nBoosting forest\\nMulti-scale\\nadaption\\nFig. 1. The application domains of object detection.\\nbridged by the combination of manually engineered low-level\\ndescriptors and discriminatively-trained shallow models.\\nThanks to the emergency of Deep Neural Networks (DNNs)\\n[6][S7], a more signiﬁcant gain is obtained with the introduc-\\ntion of Regions with CNN features (R-CNN) [15]. DNNs, or\\nthe most representative CNNs, act in a quite different way from\\ntraditional approaches. They have deeper architectures with the\\ncapacity to learn more complex features than the shallow ones.\\nAlso the expressivity and robust training algorithms allow to\\nlearn informative object representations without the need to\\ndesign features manually [26].\\nSince the proposal of R-CNN, a great deal of improved\\nmodels have been suggested, including Fast R-CNN which\\njointly optimizes classiﬁcation and bounding box regression\\ntasks [16], Faster R-CNN which takes an additional sub-\\nnetwork to generate region proposals [18] and YOLO which\\naccomplishes object detection via a ﬁxed-grid regression [17].\\nAll of them bring different degrees of detection performance\\nimprovements over the primary R-CNN and make real-time\\nand accurate object detection become more achievable.\\nIn this paper, a systematic review is provided to summarise\\nrepresentative models and their different characteristics in\\nseveral application domains, including generic object detec-\\ntion [15], [16], [18], salient object detection [27], [28], face\\ndetection [29]–[31] and pedestrian detection [32], [33]. Their\\nrelationships are depicted in Figure 1. Based on basic CNN ar-\\nchitectures, generic object detection is achieved with bounding\\nbox regression, while salient object detection is accomplished\\nwith local contrast enhancement and pixel-level segmentation.\\nFace detection and pedestrian detection are closely related\\nto generic object detection and mainly accomplished with\\nmulti-scale adaption and multi-feature fusion/boosting forest,\\nrespectively. The dotted lines indicate that the corresponding\\ndomains are associated with each other under certain con-\\nditions. It should be noticed that the covered domains are\\ndiversiﬁed. Pedestrian and face images have regular structures,\\nwhile general objects and scene images have more complex\\nvariations in geometric structures and layouts. Therefore,\\ndifferent deep models are required by various images.\\nThere has been a relevant pioneer effort [34] which mainly\\nfocuses on relevant software tools to implement deep learning\\ntechniques for image classiﬁcation and object detection, but\\npays little attention on detailing speciﬁc algorithms. Different\\nfrom it, our work not only reviews deep learning based object\\ndetection models and algorithms covering different applica-\\ntion domains in detail, but also provides their corresponding\\nexperimental comparisons and meaningful analyses.\\nThe rest of this paper is organized as follows. In Section\\n2, a brief introduction on the history of deep learning and the\\nbasic architecture of CNN is provided. Generic object detec-\\ntion architectures are presented in Section 3. Then reviews\\nof CNN applied in several speciﬁc tasks, including salient\\nobject detection, face detection and pedestrian detection, are\\nexhibited in Section 4-6, respectively. Several promising future\\ndirections are proposed in Section 7. At last, some concluding\\nremarks are presented in Section 8.\\nII. A BRIEF OVERVIEW OF DEEP LEARNING\\nPrior to overview on deep learning based object detection\\napproaches, we provide a review on the history of deep\\nlearning along with an introduction on the basic architecture\\nand advantages of CNN.\\nA. The History: Birth, Decline and Prosperity\\nDeep models can be referred to as neural networks with\\ndeep structures. The history of neural networks can date back\\nto 1940s [35], and the original intention was to simulate the\\nhuman brain system to solve general learning problems in a\\nprincipled way. It was popular in 1980s and 1990s with the\\nproposal of back-propagation algorithm by Hinton et al. [36].\\nHowever, due to the overﬁtting of training, lack of large scale\\ntraining data, limited computation power and insigniﬁcance\\nin performance compared with other machine learning tools,\\nneural networks fell out of fashion in early 2000s.\\nDeep learning has become popular since 2006 [37][S7] with\\na break through in speech recognition [38]. The recovery of\\ndeep learning can be attributed to the following factors.\\n• The emergence of large scale annotated training data, such\\nas ImageNet [39], to fully exhibit its very large learning\\ncapacity;\\n• Fast development of high performance parallel computing\\nsystems, such as GPU clusters;\\n• Signiﬁcant advances in the design of network structures\\nand training strategies. With unsupervised and layerwise\\npre-training guided by Auto-Encoder (AE) [40] or Re-\\nstricted Boltzmann Machine (RBM) [41], a good initializa-\\ntion is provided. With dropout and data augmentation, the\\noverﬁtting problem in training has been relieved [6], [42].\\nWith batch normalization (BN), the training of very deep\\nneural networks becomes quite efﬁcient [43]. Meanwhile,\\nvarious network structures, such as AlexNet [6], Overfeat\\n[44], GoogLeNet [45], VGG [46] and ResNet [47], have\\nbeen extensively studied to improve the performance.\\nWhat prompts deep learning to have a huge impact on the\\nentire academic community? It may owe to the contribution of\\nHinton’s group, whose continuous efforts have demonstrated\\nthat deep learning would bring a revolutionary breakthrough\\non grand challenges rather than just obvious improvements on\\nsmall datasets. Their success results from training a large CNN\\non 1.2 million labeled images together with a few techniques\\n[6] (e.g., ReLU operation [48] and ‘dropout’ regularization).\\nB. Architecture and Advantages of CNN\\nCNN is the most representative model of deep learning [26].\\nA typical CNN architecture, which is referred to as VGG16,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'source': '../datas/pdf_files/object_detection.pdf', 'file_path': '../datas/pdf_files/object_detection.pdf', 'total_pages': 21, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'trapped': '', 'modDate': 'D:20190417004522Z', 'creationDate': 'D:20190417004522Z', 'page': 2}, page_content='THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION\\n3\\ncan be found in Fig. S1. Each layer of CNN is known as a\\nfeature map. The feature map of the input layer is a 3D matrix\\nof pixel intensities for different color channels (e.g. RGB). The\\nfeature map of any internal layer is an induced multi-channel\\nimage, whose ‘pixel’ can be viewed as a speciﬁc feature. Every\\nneuron is connected with a small portion of adjacent neurons\\nfrom the previous layer (receptive ﬁeld). Different types of\\ntransformations [6], [49], [50] can be conducted on feature\\nmaps, such as ﬁltering and pooling. Filtering (convolution)\\noperation convolutes a ﬁlter matrix (learned weights) with\\nthe values of a receptive ﬁeld of neurons and takes a non-\\nlinear function (such as sigmoid [51], ReLU) to obtain ﬁnal\\nresponses. Pooling operation, such as max pooling, average\\npooling, L2-pooling and local contrast normalization [52],\\nsummaries the responses of a receptive ﬁeld into one value\\nto produce more robust feature descriptions.\\nWith an interleave between convolution and pooling, an\\ninitial feature hierarchy is constructed, which can be ﬁne-tuned\\nin a supervised manner by adding several fully connected (FC)\\nlayers to adapt to different visual tasks. According to the tasks\\ninvolved, the ﬁnal layer with different activation functions [6]\\nis added to get a speciﬁc conditional probability for each\\noutput neuron. And the whole network can be optimized on\\nan objective function (e.g. mean squared error or cross-entropy\\nloss) via the stochastic gradient descent (SGD) method. The\\ntypical VGG16 has totally 13 convolutional (conv) layers, 3\\nfully connected layers, 3 max-pooling layers and a softmax\\nclassiﬁcation layer. The conv feature maps are produced by\\nconvoluting 3*3 ﬁlter windows, and feature map resolutions\\nare reduced with 2 stride max-pooling layers. An arbitrary test\\nimage of the same size as training samples can be processed\\nwith the trained network. Re-scaling or cropping operations\\nmay be needed if different sizes are provided [6].\\nThe advantages of CNN against traditional methods can be\\nsummarised as follows.\\n• Hierarchical feature representation, which is the multi-\\nlevel representations from pixel to high-level semantic fea-\\ntures learned by a hierarchical multi-stage structure [15],\\n[53], can be learned from data automatically and hidden\\nfactors of input data can be disentangled through multi-level\\nnonlinear mappings.\\n• Compared with traditional shallow models, a deeper\\narchitecture provides an exponentially increased expressive\\ncapability.\\n• The architecture of CNN provides an opportunity to\\njointly optimize several related tasks together (e.g. Fast R-\\nCNN combines classiﬁcation and bounding box regression\\ninto a multi-task leaning manner).\\n• Beneﬁtting from the large learning capacity of deep\\nCNNs, some classical computer vision challenges can be\\nrecast as high-dimensional data transform problems and\\nsolved from a different viewpoint.\\nDue to these advantages, CNN has been widely applied\\ninto many research ﬁelds, such as image super-resolution\\nreconstruction [54], [55], image classiﬁcation [5], [56], im-\\nage retrieval [57], [58], face recognition [8][S5], pedestrian\\ndetection [59]–[61] and video analysis [62], [63].\\nIII. GENERIC OBJECT DETECTION\\nGeneric object detection aims at locating and classifying\\nexisting objects in any one image, and labeling them with\\nrectangular bounding boxes to show the conﬁdences of exis-\\ntence. The frameworks of generic object detection methods\\ncan mainly be categorized into two types (see Figure 2).\\nOne follows traditional object detection pipeline, generating\\nregion proposals at ﬁrst and then classifying each proposal into\\ndifferent object categories. The other regards object detection\\nas a regression or classiﬁcation problem, adopting a uniﬁed\\nframework to achieve ﬁnal results (categories and locations)\\ndirectly. The region proposal based methods mainly include\\nR-CNN [15], SPP-net [64], Fast R-CNN [16], Faster R-CNN\\n[18], R-FCN [65], FPN [66] and Mask R-CNN [67], some of\\nwhich are correlated with each other (e.g. SPP-net modiﬁes R-\\nCNN with a SPP layer). The regression/classiﬁcation based\\nmethods mainly includes MultiBox [68], AttentionNet [69],\\nG-CNN [70], YOLO [17], SSD [71], YOLOv2 [72], DSSD\\n[73] and DSOD [74]. The correlations between these two\\npipelines are bridged by the anchors introduced in Faster R-\\nCNN. Details of these methods are as follows.\\nA. Region Proposal Based Framework\\nThe region proposal based framework, a two-step process,\\nmatches the attentional mechanism of human brain to some\\nextent, which gives a coarse scan of the whole scenario ﬁrstly\\nand then focuses on regions of interest. Among the pre-related\\nworks [44], [75], [76], the most representative one is Overfeat\\n[44]. This model inserts CNN into sliding window method,\\nwhich predicts bounding boxes directly from locations of\\nthe topmost feature map after obtaining the conﬁdences of\\nunderlying object categories.\\n1) R-CNN: It is of signiﬁcance to improve the quality of\\ncandidate bounding boxes and to take a deep architecture to\\nextract high-level features. To solve these problems, R-CNN\\n[15] was proposed by Ross Girshick in 2014 and obtained a\\nmean average precision (mAP) of 53.3% with more than 30%\\nimprovement over the previous best result (DPM HSC [77]) on\\nPASCAL VOC 2012. Figure 3 shows the ﬂowchart of R-CNN,\\nwhich can be divided into three stages as follows.\\nRegion proposal generation. The R-CNN adopts selective\\nsearch [78] to generate about 2k region proposals for each\\nimage. The selective search method relies on simple bottom-up\\ngrouping and saliency cues to provide more accurate candidate\\nboxes of arbitrary sizes quickly and to reduce the searching\\nspace in object detection [24], [39].\\nCNN based deep feature extraction. In this stage, each\\nregion proposal is warped or cropped into a ﬁxed resolution\\nand the CNN module in [6] is utilized to extract a 4096-\\ndimensional feature as the ﬁnal representation. Due to large\\nlearning capacity, dominant expressive power and hierarchical\\nstructure of CNNs, a high-level, semantic and robust feature\\nrepresentation for each region proposal can be obtained.\\nClassiﬁcation and localization. With pre-trained category-\\nspeciﬁc linear SVMs for multiple classes, different region pro-\\nposals are scored on a set of positive regions and background\\n(negative) regions. The scored regions are then adjusted with'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'source': '../datas/pdf_files/object_detection.pdf', 'file_path': '../datas/pdf_files/object_detection.pdf', 'total_pages': 21, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'trapped': '', 'modDate': 'D:20190417004522Z', 'creationDate': 'D:20190417004522Z', 'page': 3}, page_content='THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION\\n4\\nGeneric object \\ndetection\\nRegion proposal \\nbased\\nRegression/\\nClassification \\nbased \\nR-CNN\\n(2014)\\nSPP-net\\n(2015)\\nFRCN\\n(2015)\\nFaster \\nR-CNN\\n(2015)\\nR-FCN\\n(2016)\\nFPN\\n(2017)\\nMask R-CNN\\n(2017)\\nMultiBox\\n(2014)\\nAttentionNet\\n(2015)\\nG-CNN\\n(2016)\\nYOLO\\n(2016)\\nSSD\\n(2016)\\nYOLOv2\\n(2017)\\nSPP \\nlayer\\nMulti-\\ntask\\nRPN\\nFCN\\nFeature\\npyramid\\nInstance\\nSegmentation\\nRegion\\nproposal\\nUnified\\nloss\\nDirection\\niteration\\nJoint Grid\\nregression\\nRPN\\nBN\\nMulti-scale\\nGrid\\nregression\\nDSSD\\n(2017)\\nDSOD\\n(2017)\\nStem block\\nDense block\\nResNet101 \\nDeconv layers\\nFig. 2. Two types of frameworks: region proposal based and regression/classiﬁcation based. SPP: Spatial Pyramid Pooling [64], FRCN: Faster R-CNN [16],\\nRPN: Region Proposal Network [18], FCN: Fully Convolutional Network [65], BN: Batch Normalization [43], Deconv layers: Deconvolution layers [54]\\n.\\n1. Input \\nimage\\n2. Extract region \\nproposals (~2k)\\n3. Compute \\nCNN features\\naeroplane? no.\\n...\\nperson? yes.\\ntvmonitor? no.\\n4. Classify \\nregions\\nwarped region\\n...\\nCNN\\nR-CNN: Regions with CNN features\\nFig. 3. The ﬂowchart of R-CNN [15], which consists of 3 stages: (1) extracts\\nbottom-up region proposals, (2) computes features for each proposal using a\\nCNN, and then (3) classiﬁes each region with class-speciﬁc linear SVMs.\\nbounding box regression and ﬁltered with a greedy non-\\nmaximum suppression (NMS) to produce ﬁnal bounding boxes\\nfor preserved object locations.\\nWhen there are scarce or insufﬁcient labeled data, pre-\\ntraining is usually conducted. Instead of unsupervised pre-\\ntraining [79], R-CNN ﬁrstly conducts supervised pre-training\\non ILSVRC, a very large auxiliary dataset, and then takes a\\ndomain-speciﬁc ﬁne-tuning. This scheme has been adopted by\\nmost of subsequent approaches [16], [18].\\nIn spite of its improvements over traditional methods and\\nsigniﬁcance in bringing CNN into practical object detection,\\nthere are still some disadvantages.\\n• Due to the existence of FC layers, the CNN requires a\\nﬁxed-size (e.g., 227×227) input image, which directly leads\\nto the re-computation of the whole CNN for each evaluated\\nregion, taking a great deal of time in the testing period.\\n• Training of R-CNN is a multi-stage pipeline. At ﬁrst,\\na convolutional network (ConvNet) on object proposals is\\nﬁne-tuned. Then the softmax classiﬁer learned by ﬁne-\\ntuning is replaced by SVMs to ﬁt in with ConvNet features.\\nFinally, bounding-box regressors are trained.\\n• Training is expensive in space and time. Features are\\nextracted from different region proposals and stored on the\\ndisk. It will take a long time to process a relatively small\\ntraining set with very deep networks, such as VGG16. At the\\nsame time, the storage memory required by these features\\nshould also be a matter of concern.\\n• Although selective search can generate region proposals\\nwith relatively high recalls, the obtained region proposals\\nare still redundant and this procedure is time-consuming\\n(around 2 seconds to extract 2k region proposals).\\nTo solve these problems, many methods have been pro-\\nposed. GOP [80] takes a much faster geodesic based segmen-\\ntation to replace traditional graph cuts. MCG [81] searches\\ndifferent scales of the image for multiple hierarchical segmen-\\ntations and combinatorially groups different regions to produce\\nproposals. Instead of extracting visually distinct segments,\\nthe edge boxes method [82] adopts the idea that objects are\\nmore likely to exist in bounding boxes with fewer contours\\nstraggling their boundaries. Also some researches tried to\\nre-rank or reﬁne pre-extracted region proposals to remove\\nunnecessary ones and obtained a limited number of valuable\\nones, such as DeepBox [83] and SharpMask [84].\\nIn addition, there are some improvements to solve the\\nproblem of inaccurate localization. Zhang et al. [85] utilized\\na bayesian optimization based search algorithm to guide\\nthe regressions of different bounding boxes sequentially, and\\ntrained class-speciﬁc CNN classiﬁers with a structured loss\\nto penalize the localization inaccuracy explicitly. Saurabh\\nGupta et al. improved object detection for RGB-D images\\nwith semantically rich image and depth features [86], and\\nlearned a new geocentric embedding for depth images to\\nencode each pixel. The combination of object detectors and\\nsuperpixel classiﬁcation framework gains a promising result\\non semantic scene segmentation task. Ouyang et al. proposed\\na deformable deep CNN (DeepID-Net) [87] which introduces\\na novel deformation constrained pooling (def-pooling) layer\\nto impose geometric penalty on the deformation of various\\nobject parts and makes an ensemble of models with different\\nsettings. Lenc et al. [88] provided an analysis on the role\\nof proposal generation in CNN-based detectors and tried to\\nreplace this stage with a constant and trivial region generation\\nscheme. The goal is achieved by biasing sampling to match\\nthe statistics of the ground truth bounding boxes with K-means\\nclustering. However, more candidate boxes are required to\\nachieve comparable results to those of R-CNN.\\n2) SPP-net: FC layers must take a ﬁxed-size input. That’s\\nwhy R-CNN chooses to warp or crop each region proposal\\ninto the same size. However, the object may exist partly in\\nthe cropped region and unwanted geometric distortion may be\\nproduced due to the warping operation. These content losses or\\ndistortions will reduce recognition accuracy, especially when\\nthe scales of objects vary.\\nTo solve this problem, He et al. took the theory of spatial\\npyramid matching (SPM) [89], [90] into consideration and\\nproposed a novel CNN architecture named SPP-net [64]. SPM\\ntakes several ﬁner to coarser scales to partition the image into\\na number of divisions and aggregates quantized local features\\ninto mid-level representations.\\nThe architecture of SPP-net for object detection can be\\nfound in Figure 4. Different from R-CNN, SPP-net reuses\\nfeature maps of the 5-th conv layer (conv5) to project region'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'source': '../datas/pdf_files/object_detection.pdf', 'file_path': '../datas/pdf_files/object_detection.pdf', 'total_pages': 21, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'trapped': '', 'modDate': 'D:20190417004522Z', 'creationDate': 'D:20190417004522Z', 'page': 4}, page_content='THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION\\n5\\nspatial pyramid \\npooling layer\\nfeature maps of conv5\\nconvolutional layers\\nfixed-length representation\\ninput image\\nwindow\\n…...\\nfully-connected layers (fc6, fc7)\\nFig. 4. The architecture of SPP-net for object detection [64].\\nDeep\\nConvNet\\nConv\\nfeature map\\nRoI\\nprojection\\nRoI\\npooling\\nlayer\\nFCs\\nRoI feature\\nvector\\nsoftmax\\nbbox\\nregressor\\nOutputs:\\nFC\\nFC\\nFor each RoI\\nFig. 5. The architecture of Fast R-CNN [16].\\nproposals of arbitrary sizes to ﬁxed-length feature vectors. The\\nfeasibility of the reusability of these feature maps is due to\\nthe fact that the feature maps not only involve the strength of\\nlocal responses, but also have relationships with their spatial\\npositions [64]. The layer after the ﬁnal conv layer is referred\\nto as spatial pyramid pooling layer (SPP layer). If the number\\nof feature maps in conv5 is 256, taking a 3-level pyramid,\\nthe ﬁnal feature vector for each region proposal obtained after\\nSPP layer has a dimension of 256 × (12 + 22 + 42) = 5376.\\nSPP-net not only gains better results with correct estimation\\nof different region proposals in their corresponding scales, but\\nalso improves detection efﬁciency in testing period with the\\nsharing of computation cost before SPP layer among different\\nproposals.\\n3) Fast R-CNN: Although SPP-net has achieved impressive\\nimprovements in both accuracy and efﬁciency over R-CNN,\\nit still has some notable drawbacks. SPP-net takes almost\\nthe same multi-stage pipeline as R-CNN, including feature\\nextraction, network ﬁne-tuning, SVM training and bounding-\\nbox regressor ﬁtting. So an additional expense on storage space\\nis still required. Additionally, the conv layers preceding the\\nSPP layer cannot be updated with the ﬁne-tuning algorithm\\nintroduced in [64]. As a result, an accuracy drop of very deep\\nnetworks is unsurprising. To this end, Girshick [16] introduced\\na multi-task loss on classiﬁcation and bounding box regression\\nand proposed a novel CNN architecture named Fast R-CNN.\\nThe architecture of Fast R-CNN is exhibited in Figure 5.\\nSimilar to SPP-net, the whole image is processed with conv\\nlayers to produce feature maps. Then, a ﬁxed-length feature\\nvector is extracted from each region proposal with a region of\\ninterest (RoI) pooling layer. The RoI pooling layer is a special\\ncase of the SPP layer, which has only one pyramid level. Each\\nfeature vector is then fed into a sequence of FC layers before\\nﬁnally branching into two sibling output layers. One output\\nlayer is responsible for producing softmax probabilities for\\nall C + 1 categories (C object classes plus one ‘background’\\nclass) and the other output layer encodes reﬁned bounding-\\nbox positions with four real-valued numbers. All parameters\\nin these procedures (except the generation of region proposals)\\nare optimized via a multi-task loss in an end-to-end way.\\nThe multi-tasks loss L is deﬁned as below to jointly train\\nclassiﬁcation and bounding-box regression,\\nL(p, u, tu, v) = Lcls(p, u) + λ[u ≥1]Lloc(tu, v)\\n(1)\\nwhere Lcls(p, u) = −log pu calculates the log loss for ground\\ntruth class u and pu is driven from the discrete probability\\ndistribution p = (p0, · · · , pC) over the C +1 outputs from the\\nlast FC layer. Lloc(tu, v) is deﬁned over the predicted offsets\\ntu = (tu\\nx, tu\\ny, tu\\nw, tu\\nh) and ground-truth bounding-box regression\\ntargets v = (vx, vy, vw, vh), where x, y, w, h denote the two\\ncoordinates of the box center, width, and height, respectively.\\nEach tu adopts the parameter settings in [15] to specify an\\nobject proposal with a log-space height/width shift and scale-\\ninvariant translation. The Iverson bracket indicator function\\n[u ≥1] is employed to omit all background RoIs. To provide\\nmore robustness against outliers and eliminate the sensitivity\\nin exploding gradients, a smooth L1 loss is adopted to ﬁt\\nbounding-box regressors as below\\nLloc(tu, v) =\\nX\\ni∈x,y,w,h\\nsmoothL1(tu\\ni −vi)\\n(2)\\nwhere\\nsmoothL1(x) =\\n(\\n0.5x2\\nif |x| < 1\\n|x| −0.5\\notherwise\\n(3)\\nTo accelerate the pipeline of Fast R-CNN, another two tricks\\nare of necessity. On one hand, if training samples (i.e. RoIs)\\ncome from different images, back-propagation through the\\nSPP layer becomes highly inefﬁcient. Fast R-CNN samples\\nmini-batches hierarchically, namely N images sampled ran-\\ndomly at ﬁrst and then R/N RoIs sampled in each image,\\nwhere R represents the number of RoIs. Critically, computa-\\ntion and memory are shared by RoIs from the same image in\\nthe forward and backward pass. On the other hand, much time\\nis spent in computing the FC layers during the forward pass\\n[16]. The truncated Singular Value Decomposition (SVD) [91]\\ncan be utilized to compress large FC layers and to accelerate\\nthe testing procedure.\\nIn the Fast R-CNN, regardless of region proposal genera-\\ntion, the training of all network layers can be processed in\\na single-stage with a multi-task loss. It saves the additional\\nexpense on storage space, and improves both accuracy and\\nefﬁciency with more reasonable training schemes.\\n4) Faster R-CNN: Despite the attempt to generate candi-\\ndate boxes with biased sampling [88], state-of-the-art object\\ndetection networks mainly rely on additional methods, such as\\nselective search and Edgebox, to generate a candidate pool of\\nisolated region proposals. Region proposal computation is also\\na bottleneck in improving efﬁciency. To solve this problem,\\nRen et al. introduced an additional Region Proposal Network\\n(RPN) [18], [92], which acts in a nearly cost-free way by\\nsharing full-image conv features with detection network.\\nRPN is achieved with a fully-convolutional network, which\\nhas the ability to predict object bounds and scores at each\\nposition simultaneously. Similar to [78], RPN takes an image\\nof arbitrary size to generate a set of rectangular object propos-\\nals. RPN operates on a speciﬁc conv layer with the preceding\\nlayers shared with object detection network.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'source': '../datas/pdf_files/object_detection.pdf', 'file_path': '../datas/pdf_files/object_detection.pdf', 'total_pages': 21, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'trapped': '', 'modDate': 'D:20190417004522Z', 'creationDate': 'D:20190417004522Z', 'page': 5}, page_content='THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION\\n6\\nconv feature map\\nintermediate layer\\n256-d\\n2k scores\\n4k coordinates\\nsliding window\\nreg layer\\ncls layer\\nk anchor boxes\\nFig. 6.\\nThe RPN in Faster R-CNN [18]. K predeﬁned anchor boxes are\\nconvoluted with each sliding window to produce ﬁxed-length vectors which\\nare taken by cls and reg layer to obtain corresponding outputs.\\nThe architecture of RPN is shown in Figure 6. The network\\nslides over the conv feature map and fully connects to an\\nn × n spatial window. A low dimensional vector (512-d for\\nVGG16) is obtained in each sliding window and fed into two\\nsibling FC layers, namely box-classiﬁcation layer (cls) and\\nbox-regression layer (reg). This architecture is implemented\\nwith an n × n conv layer followed by two sibling 1 × 1 conv\\nlayers. To increase non-linearity, ReLU is applied to the output\\nof the n × n conv layer.\\nThe regressions towards true bounding boxes are achieved\\nby comparing proposals relative to reference boxes (anchors).\\nIn the Faster R-CNN, anchors of 3 scales and 3 aspect ratios\\nare adopted. The loss function is similar to (1).\\nL(pi, ti) =\\n1\\nNcls\\nX\\ni\\nLcls(pi, p∗\\ni ) + λ\\n1\\nNreg\\nX\\ni\\np∗\\ni Lreg(ti, t∗\\ni )\\n(4)\\nwhere pi shows the predicted probability of the i-th anchor\\nbeing an object. The ground truth label p∗\\ni is 1 if the anchor is\\npositive, otherwise 0. ti stores 4 parameterized coordinates of\\nthe predicted bounding box while t∗\\ni is related to the ground-\\ntruth box overlapping with a positive anchor. Lcls is a binary\\nlog loss and Lreg is a smoothed L1 loss similar to (2). These\\ntwo terms are normalized with the mini-batch size (Ncls)\\nand the number of anchor locations (Nreg), respectively. In\\nthe form of fully-convolutional networks, Faster R-CNN can\\nbe trained end-to-end by back-propagation and SGD in an\\nalternate training manner.\\nWith the proposal of Faster R-CNN, region proposal based\\nCNN architectures for object detection can really be trained\\nin an end-to-end way. Also a frame rate of 5 FPS (Frame\\nPer Second) on a GPU is achieved with state-of-the-art object\\ndetection accuracy on PASCAL VOC 2007 and 2012. How-\\never, the alternate training algorithm is very time-consuming\\nand RPN produces object-like regions (including backgrounds)\\ninstead of object instances and is not skilled in dealing with\\nobjects with extreme scales or shapes.\\n5) R-FCN: Divided by the RoI pooling layer, a prevalent\\nfamily [16], [18] of deep networks for object detection are\\ncomposed of two subnetworks: a shared fully convolutional\\nsubnetwork (independent of RoIs) and an unshared RoI-wise\\nsubnetwork. This decomposition originates from pioneering\\nclassiﬁcation architectures (e.g. AlexNet [6] and VGG16 [46])\\nwhich consist of a convolutional subnetwork and several FC\\nlayers separated by a speciﬁc spatial pooling layer.\\nRecent state-of-the-art image classiﬁcation networks, such\\nas Residual Nets (ResNets) [47] and GoogLeNets [45], [93],\\nare fully convolutional. To adapt to these architectures, it’s\\n(a) Featurized image pyramid\\npredict\\npredict\\npredict\\npredict\\n(b) Single feature map\\npredict\\n(d) Feature Pyramid Network\\npredict\\npredict\\npredict\\n(c) Pyramidal feature hierarchy\\npredict\\npredict\\npredict\\nFig. 7. The main concern of FPN [66]. (a) It is slow to use an image pyramid\\nto build a feature pyramid. (b) Only single scale features is adopted for faster\\ndetection. (c) An alternative to the featurized image pyramid is to reuse the\\npyramidal feature hierarchy computed by a ConvNet. (d) FPN integrates both\\n(b) and (c). Blue outlines indicate feature maps and thicker outlines denote\\nsemantically stronger features.\\nnatural to construct a fully convolutional object detection net-\\nwork without RoI-wise subnetwork. However, it turns out to be\\ninferior with such a naive solution [47]. This inconsistence is\\ndue to the dilemma of respecting translation variance in object\\ndetection compared with increasing translation invariance in\\nimage classiﬁcation. In other words, shifting an object inside\\nan image should be indiscriminative in image classiﬁcation\\nwhile any translation of an object in a bounding box may\\nbe meaningful in object detection. A manual insertion of\\nthe RoI pooling layer into convolutions can break down\\ntranslation invariance at the expense of additional unshared\\nregion-wise layers. So Li et al. [65] proposed a region-based\\nfully convolutional networks (R-FCN, Fig. S2).\\nDifferent from Faster R-CNN, for each category, the last\\nconv layer of R-FCN produces a total of k2 position-sensitive\\nscore maps with a ﬁxed grid of k × k ﬁrstly and a position-\\nsensitive RoI pooling layer is then appended to aggregate the\\nresponses from these score maps. Finally, in each RoI, k2\\nposition-sensitive scores are averaged to produce a C + 1-d\\nvector and softmax responses across categories are computed.\\nAnother 4k2-d conv layer is appended to obtain class-agnostic\\nbounding boxes.\\nWith R-FCN, more powerful classiﬁcation networks can be\\nadopted to accomplish object detection in a fully-convolutional\\narchitecture by sharing nearly all the layers, and state-of-the-\\nart results are obtained on both PASCAL VOC and Microsoft\\nCOCO [94] datasets at a test speed of 170ms per image.\\n6) FPN: Feature pyramids built upon image pyramids\\n(featurized image pyramids) have been widely applied in\\nmany object detection systems to improve scale invariance\\n[24], [64] (Figure 7(a)). However, training time and memory\\nconsumption increase rapidly. To this end, some techniques\\ntake only a single input scale to represent high-level semantics\\nand increase the robustness to scale changes (Figure 7(b)),\\nand image pyramids are built at test time which results in\\nan inconsistency between train/test-time inferences [16], [18].\\nThe in-network feature hierarchy in a deep ConvNet produces\\nfeature maps of different spatial resolutions while introduces\\nlarge semantic gaps caused by different depths (Figure 7(c)).\\nTo avoid using low-level features, pioneer works [71], [95]\\nusually build the pyramid starting from middle layers or\\njust sum transformed feature responses, missing the higher-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'source': '../datas/pdf_files/object_detection.pdf', 'file_path': '../datas/pdf_files/object_detection.pdf', 'total_pages': 21, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'trapped': '', 'modDate': 'D:20190417004522Z', 'creationDate': 'D:20190417004522Z', 'page': 6}, page_content='THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION\\n7\\nFig. 8. The Mask R-CNN framework for instance segmentation [67].\\nresolution maps of the feature hierarchy.\\nDifferent from these approaches, FPN [66] holds an ar-\\nchitecture with a bottom-up pathway, a top-down pathway\\nand several lateral connections to combine low-resolution and\\nsemantically strong features with high-resolution and seman-\\ntically weak features (Figure 7(d)). The bottom-up pathway,\\nwhich is the basic forward backbone ConvNet, produces a\\nfeature hierarchy by downsampling the corresponding feature\\nmaps with a stride of 2. The layers owning the same size of\\noutput maps are grouped into the same network stage and the\\noutput of the last layer of each stage is chosen as the reference\\nset of feature maps to build the following top-down pathway.\\nTo build the top-down pathway, feature maps from higher\\nnetwork stages are upsampled at ﬁrst and then enhanced with\\nthose of the same spatial size from the bottom-up pathway\\nvia lateral connections. A 1 × 1 conv layer is appended to\\nthe upsampled map to reduce channel dimensions and the\\nmergence is achieved by element-wise addition. Finally, a 3×3\\nconvolution is also appended to each merged map to reduce\\nthe aliasing effect of upsampling and the ﬁnal feature map is\\ngenerated. This process is iterated until the ﬁnest resolution\\nmap is generated.\\nAs feature pyramid can extract rich semantics from all\\nlevels and be trained end-to-end with all scales, state-of-the-\\nart representation can be obtained without sacriﬁcing speed\\nand memory. Meanwhile, FPN is independent of the backbone\\nCNN architectures and can be applied to different stages of\\nobject detection (e.g. region proposal generation) and to many\\nother computer vision tasks (e.g. instance segmentation).\\n7) Mask R-CNN: Instance segmentation [96] is a challeng-\\ning task which requires detecting all objects in an image and\\nsegmenting each instance (semantic segmentation [97]). These\\ntwo tasks are usually regarded as two independent processes.\\nAnd the multi-task scheme will create spurious edge and\\nexhibit systematic errors on overlapping instances [98]. To\\nsolve this problem, parallel to the existing branches in Faster\\nR-CNN for classiﬁcation and bounding box regression, the\\nMask R-CNN [67] adds a branch to predict segmentation\\nmasks in a pixel-to-pixel manner (Figure 8).\\nDifferent from the other two branches which are inevitably\\ncollapsed into short output vectors by FC layers, the segmen-\\ntation mask branch encodes an m × m mask to maintain the\\nexplicit object spatial layout. This kind of fully convolutional\\nrepresentation requires fewer parameters but is more accurate\\nthan that of [97]. Formally, besides the two losses in (1) for\\nclassiﬁcation and bounding box regression, an additional loss\\nfor segmentation mask branch is deﬁned to reach a multi-task\\nloss. An this loss is only associated with ground-truth class\\nand relies on the classiﬁcation branch to predict the category.\\nBecause RoI pooling, the core operation in Faster R-CNN,\\nperforms a coarse spatial quantization for feature extraction,\\nmisalignment is introduced between the RoI and the features.\\nIt affects classiﬁcation little because of its robustness to small\\ntranslations. However, it has a large negative effect on pixel-\\nto-pixel mask prediction. To solve this problem, Mask R-CNN\\nadopts a simple and quantization-free layer, namely RoIAlign,\\nto preserve the explicit per-pixel spatial correspondence faith-\\nfully. RoIAlign is achieved by replacing the harsh quantization\\nof RoI pooling with bilinear interpolation [99], computing the\\nexact values of the input features at four regularly sampled\\nlocations in each RoI bin. In spite of its simplicity, this\\nseemingly minor change improves mask accuracy greatly,\\nespecially under strict localization metrics.\\nGiven the Faster R-CNN framework, the mask branch only\\nadds a small computational burden and its cooperation with\\nother tasks provides complementary information for object\\ndetection. As a result, Mask R-CNN is simple to implement\\nwith promising instance segmentation and object detection\\nresults. In a word, Mask R-CNN is a ﬂexible and efﬁcient\\nframework for instance-level recognition, which can be easily\\ngeneralized to other tasks (e.g. human pose estimation [7][S4])\\nwith minimal modiﬁcation.\\n8) Multi-task Learning, Multi-scale Representation and\\nContextual Modelling:\\nAlthough the Faster R-CNN gets\\npromising results with several hundred proposals, it still strug-\\ngles in small-size object detection and localization, mainly due\\nto the coarseness of its feature maps and limited information\\nprovided in particular candidate boxes. The phenomenon is\\nmore obvious on the Microsoft COCO dataset which consists\\nof objects at a broad range of scales, less prototypical images,\\nand requires more precise localization. To tackle these prob-\\nlems, it is of necessity to accomplish object detection with\\nmulti-task learning [100], multi-scale representation [95] and\\ncontext modelling [101] to combine complementary informa-\\ntion from multiple sources.\\nMulti-task Learning learns a useful representation for\\nmultiple correlated tasks from the same input [102], [103].\\nBrahmbhatt et al. introduced conv features trained for ob-\\nject segmentation and ‘stuff’ (amorphous categories such as\\nground and water) to guide accurate object detection of small\\nobjects (StuffNet) [100]. Dai et al. [97] presented Multitask\\nNetwork Cascades of three networks, namely class-agnostic\\nregion proposal generation, pixel-level instance segmentation\\nand regional instance classiﬁcation. Li et al. incorporated the\\nweakly-supervised object segmentation cues and region-based\\nobject detection into a multi-stage architecture to fully exploit\\nthe learned segmentation features [104].\\nMulti-scale Representation combines activations from\\nmultiple layers with skip-layer connections to provide seman-\\ntic information of different spatial resolutions [66]. Cai et\\nal. proposed the MS-CNN [105] to ease the inconsistency\\nbetween the sizes of objects and receptive ﬁelds with multiple\\nscale-independent output layers. Yang et al. investigated two\\nstrategies, namely scale-dependent pooling (SDP) and layer-\\nwise cascaded rejection classiﬁers (CRC), to exploit appropri-\\nate scale-dependent conv features [33]. Kong et al. proposed\\nthe HyperNet to calculate the shared features between RPN\\nand object detection network by aggregating and compressing\\nhierarchical feature maps from different resolutions into a'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'source': '../datas/pdf_files/object_detection.pdf', 'file_path': '../datas/pdf_files/object_detection.pdf', 'total_pages': 21, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'trapped': '', 'modDate': 'D:20190417004522Z', 'creationDate': 'D:20190417004522Z', 'page': 7}, page_content='THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION\\n8\\nuniform space [101].\\nContextual Modelling improves detection performance by\\nexploiting features from or around RoIs of different support\\nregions and resolutions to deal with occlusions and local\\nsimilarities [95]. Zhu et al. proposed the SegDeepM to exploit\\nobject segmentation which reduces the dependency on initial\\ncandidate boxes with Markov Random Field [106]. Moysset\\net al. took advantage of 4 directional 2D-LSTMs [107] to\\nconvey global context between different local regions and re-\\nduced trainable parameters with local parameter-sharing [108].\\nZeng et al. proposed a novel GBD-Net by introducing gated\\nfunctions to control message transmission between different\\nsupport regions [109].\\nThe Combination incorporates different components above\\ninto the same model to improve detection performance further.\\nGidaris et al. proposed the Multi-Region CNN (MR-CNN)\\nmodel [110] to capture different aspects of an object, the\\ndistinct appearances of various object parts and semantic\\nsegmentation-aware features. To obtain contextual and multi-\\nscale representations, Bell et al. proposed the Inside-Outside\\nNet (ION) by exploiting information both inside and outside\\nthe RoI [95] with spatial recurrent neural networks [111] and\\nskip pooling [101]. Zagoruyko et al. proposed the MultiPath\\narchitecture by introducing three modiﬁcations to the Fast\\nR-CNN [112], including multi-scale skip connections [95],\\na modiﬁed foveal structure [110] and a novel loss function\\nsumming different IoU losses.\\n9) Thinking in Deep Learning based Object Detection:\\nApart from the above approaches, there are still many impor-\\ntant factors for continued progress.\\nThere is a large imbalance between the number of annotated\\nobjects and background examples. To address this problem,\\nShrivastava et al. proposed an effective online mining algo-\\nrithm (OHEM) [113] for automatic selection of the hard ex-\\namples, which leads to a more effective and efﬁcient training.\\nInstead of concentrating on feature extraction, Ren et al.\\nmade a detailed analysis on object classiﬁers [114], and\\nfound that it is of particular importance for object detection\\nto construct a deep and convolutional per-region classiﬁer\\ncarefully, especially for ResNets [47] and GoogLeNets [45].\\nTraditional CNN framework for object detection is not\\nskilled in handling signiﬁcant scale variation, occlusion or\\ntruncation, especially when only 2D object detection is in-\\nvolved. To address this problem, Xiang et al. proposed a\\nnovel subcategory-aware region proposal network [60], which\\nguides the generation of region proposals with subcategory\\ninformation related to object poses and jointly optimize object\\ndetection and subcategory classiﬁcation.\\nOuyang et al. found that the samples from different classes\\nfollow a longtailed distribution [115], which indicates that dif-\\nferent classes with distinct numbers of samples have different\\ndegrees of impacts on feature learning. To this end, objects are\\nﬁrstly clustered into visually similar class groups, and then a\\nhierarchical feature learning scheme is adopted to learn deep\\nrepresentations for each group separately.\\nIn order to minimize computational cost and achieve the\\nstate-of-the-art performance, with the ‘deep and thin’ design\\nprinciple and following the pipeline of Fast R-CNN, Hong et\\nal. proposed the architecture of PVANET [116], which adopts\\nsome building blocks including concatenated ReLU [117],\\nInception [45], and HyperNet [101] to reduce the expense on\\nmulti-scale feature extraction and trains the network with batch\\nnormalization [43], residual connections [47], and learning\\nrate scheduling based on plateau detection [47]. The PVANET\\nachieves the state-of-the-art performance and can be processed\\nin real time on Titan X GPU (21 FPS).\\nB. Regression/Classiﬁcation Based Framework\\nRegion proposal based frameworks are composed of sev-\\neral correlated stages, including region proposal generation,\\nfeature extraction with CNN, classiﬁcation and bounding box\\nregression, which are usually trained separately. Even in recent\\nend-to-end module Faster R-CNN, an alternative training is\\nstill required to obtain shared convolution parameters between\\nRPN and detection network. As a result, the time spent in\\nhandling different components becomes the bottleneck in real-\\ntime application.\\nOne-step\\nframeworks\\nbased\\non\\nglobal\\nregres-\\nsion/classiﬁcation, mapping straightly from image pixels\\nto bounding box coordinates and class probabilities, can\\nreduce time expense. We ﬁrstly reviews some pioneer CNN\\nmodels, and then focus on two signiﬁcant frameworks,\\nnamely You only look once (YOLO) [17] and Single Shot\\nMultiBox Detector (SSD) [71].\\n1) Pioneer Works: Previous to YOLO and SSD, many\\nresearchers have already tried to model object detection as\\na regression or classiﬁcation task.\\nSzegedy et al. formulated object detection task as a DNN-\\nbased regression [118], generating a binary mask for the\\ntest image and extracting detections with a simple bounding\\nbox inference. However, the model has difﬁculty in handling\\noverlapping objects, and bounding boxes generated by direct\\nupsampling is far from perfect.\\nPinheiro et al. proposed a CNN model with two branches:\\none generates class agnostic segmentation masks and the\\nother predicts the likelihood of a given patch centered on\\nan object [119]. Inference is efﬁcient since class scores and\\nsegmentation can be obtained in a single model with most of\\nthe CNN operations shared.\\nErhan et al. proposed regression based MultiBox to produce\\nscored class-agnostic region proposals [68], [120]. A uniﬁed\\nloss was introduced to bias both localization and conﬁdences\\nof multiple components to predict the coordinates of class-\\nagnostic bounding boxes. However, a large quantity of addi-\\ntional parameters are introduced to the ﬁnal layer.\\nYoo et al. adopted an iterative classiﬁcation approach to\\nhandle object detection and proposed an impressive end-to-\\nend CNN architecture named AttentionNet [69]. Starting from\\nthe top-left (TL) and bottom-right (BR) corner of an image,\\nAttentionNet points to a target object by generating quantized\\nweak directions and converges to an accurate object bound-\\nary box with an ensemble of iterative predictions. However,\\nthe model becomes quite inefﬁcient when handling multiple\\ncategories with a progressive two-step procedure.\\nNajibi et al. proposed a proposal-free iterative grid based\\nobject detector (G-CNN), which models object detection as'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'source': '../datas/pdf_files/object_detection.pdf', 'file_path': '../datas/pdf_files/object_detection.pdf', 'total_pages': 21, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'trapped': '', 'modDate': 'D:20190417004522Z', 'creationDate': 'D:20190417004522Z', 'page': 8}, page_content='THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION\\n9\\nFig. 9. Main idea of YOLO [17].\\nﬁnding a path from a ﬁxed grid to boxes tightly surrounding\\nthe objects [70]. Starting with a ﬁxed multi-scale bounding box\\ngrid, G-CNN trains a regressor to move and scale elements of\\nthe grid towards objects iteratively. However, G-CNN has a\\ndifﬁculty in dealing with small or highly overlapping objects.\\n2) YOLO: Redmon et al. [17] proposed a novel framework\\ncalled YOLO, which makes use of the whole topmost feature\\nmap to predict both conﬁdences for multiple categories and\\nbounding boxes. The basic idea of YOLO is exhibited in\\nFigure 9. YOLO divides the input image into an S × S grid and\\neach grid cell is responsible for predicting the object centered\\nin that grid cell. Each grid cell predicts B bounding boxes\\nand their corresponding conﬁdence scores. Formally, conﬁ-\\ndence scores are deﬁned as Pr(Object) ∗IOU truth\\npred , which\\nindicates how likely there exist objects (Pr(Object) ≥0) and\\nshows conﬁdences of its prediction (IOU truth\\npred ). At the same\\ntime, regardless of the number of boxes, C conditional class\\nprobabilities (Pr(Classi|Object)) should also be predicted in\\neach grid cell. It should be noticed that only the contribution\\nfrom the grid cell containing an object is calculated.\\nAt test time, class-speciﬁc conﬁdence scores for each box\\nare achieved by multiplying the individual box conﬁdence\\npredictions with the conditional class probabilities as follows.\\nPr(Object) ∗IOU truth\\npred ∗Pr(Classi|Object)\\n= Pr(Classi) ∗IOU truth\\npred\\n(5)\\nwhere the existing probability of class-speciﬁc objects in the\\nbox and the ﬁtness between the predicted box and the object\\nare both taken into consideration.\\nDuring training, the following loss function is optimized,\\nλcoord\\nS2\\nX\\ni=0\\nB\\nX\\nj=0\\n1obj\\nij\\n\\x02\\n(xi −ˆxi)2 + (yi −ˆyi)2\\x03\\n+λcoord\\nS2\\nX\\ni=0\\nB\\nX\\nj=0\\n1obj\\nij\\n\"\\x12√wi −\\np\\nˆwi)2 + (\\np\\nhi −\\nq\\nˆhi\\n\\x132#\\n+\\nS2\\nX\\ni=0\\nB\\nX\\nj=0\\n1obj\\nij\\n\\x10\\nCi −ˆCi\\n\\x112\\n+λnoobj\\nS2\\nX\\ni=0\\nB\\nX\\nj=0\\n1noobj\\nij\\n\\x10\\nCi −ˆCi\\n\\x112\\n+\\nS2\\nX\\ni=0\\n1obj\\ni\\nX\\nc∈classes\\n(pi(c) −ˆpi(c))2\\n(6)\\nIn a certain cell i, (xi, yi) denote the center of the box relative\\nto the bounds of the grid cell, (wi, hi) are the normalized width\\nand height relative to the image size, Ci represents conﬁdence\\nscores, 1obj\\ni\\nindicates the existence of objects and 1obj\\nij\\ndenotes\\nthat the prediction is conducted by the jth bounding box\\npredictor. Note that only when an object is present in that grid\\ncell, the loss function penalizes classiﬁcation errors. Similarly,\\nwhen the predictor is ‘responsible’ for the ground truth box\\n(i.e. the highest IoU of any predictor in that grid cell is\\nachieved), bounding box coordinate errors are penalized.\\nThe YOLO consists of 24 conv layers and 2 FC layers,\\nof which some conv layers construct ensembles of inception\\nmodules with 1 × 1 reduction layers followed by 3 × 3 conv\\nlayers. The network can process images in real-time at 45\\nFPS and a simpliﬁed version Fast YOLO can reach 155 FPS\\nwith better results than other real-time detectors. Furthermore,\\nYOLO produces fewer false positives on background, which\\nmakes the cooperation with Fast R-CNN become possible. An\\nimproved version, YOLOv2, was later proposed in [72], which\\nadopts several impressive strategies, such as BN, anchor boxes,\\ndimension cluster and multi-scale training.\\n3) SSD: YOLO has a difﬁculty in dealing with small\\nobjects in groups, which is caused by strong spatial constraints\\nimposed on bounding box predictions [17]. Meanwhile, YOLO\\nstruggles to generalize to objects in new/unusual aspect ratios/\\nconﬁgurations and produces relatively coarse features due to\\nmultiple downsampling operations.\\nAiming at these problems, Liu et al. proposed a Single Shot\\nMultiBox Detector (SSD) [71], which was inspired by the\\nanchors adopted in MultiBox [68], RPN [18] and multi-scale\\nrepresentation [95]. Given a speciﬁc feature map, instead of\\nﬁxed grids adopted in YOLO, the SSD takes advantage of a set\\nof default anchor boxes with different aspect ratios and scales\\nto discretize the output space of bounding boxes. To handle\\nobjects with various sizes, the network fuses predictions from\\nmultiple feature maps with different resolutions .\\nThe architecture of SSD is demonstrated in Figure 10. Given\\nthe VGG16 backbone architecture, SSD adds several feature\\nlayers to the end of the network, which are responsible for\\npredicting the offsets to default boxes with different scales and\\naspect ratios and their associated conﬁdences. The network is\\ntrained with a weighted sum of localization loss (e.g. Smooth\\nL1) and conﬁdence loss (e.g. Softmax), which is similar to\\n(1). Final detection results are obtained by conducting NMS\\non multi-scale reﬁned bounding boxes.\\nIntegrating with hard negative mining, data augmentation\\nand a larger number of carefully chosen default anchors,\\nSSD signiﬁcantly outperforms the Faster R-CNN in terms of\\naccuracy on PASCAL VOC and COCO, while being three\\ntimes faster. The SSD300 (input image size is 300×300) runs\\nat 59 FPS, which is more accurate and efﬁcient than YOLO.\\nHowever, SSD is not skilled at dealing with small objects,\\nwhich can be relieved by adopting better feature extractor\\nbackbone (e.g. ResNet101), adding deconvolution layers with\\nskip connections to introduce additional large-scale context\\n[73] and designing better network structure (e.g. Stem Block\\nand Dense Block) [74].'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'source': '../datas/pdf_files/object_detection.pdf', 'file_path': '../datas/pdf_files/object_detection.pdf', 'total_pages': 21, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'trapped': '', 'modDate': 'D:20190417004522Z', 'creationDate': 'D:20190417004522Z', 'page': 9}, page_content='THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION\\n10\\nFig. 10. The architecture of SSD 300 [71]. SSD adds several feature layers to the end of VGG16 backbone network to predict the offsets to default anchor\\nboxes and their associated conﬁdences. Final detection results are obtained by conducting NMS on multi-scale reﬁned bounding boxes.\\nC. Experimental Evaluation\\nWe compare various object detection methods on three\\nbenchmark datasets, including PASCAL VOC 2007 [25],\\nPASCAL VOC 2012 [121] and Microsoft COCO [94]. The\\nevaluated approaches include R-CNN [15], SPP-net [64], Fast\\nR-CNN [16], NOC [114], Bayes [85], MR-CNN&S-CNN\\n[105], Faster R-CNN [18], HyperNet [101], ION [95], MS-\\nGR [104], StuffNet [100], SSD300 [71], SSD512 [71], OHEM\\n[113], SDP+CRC [33], GCNN [70], SubCNN [60], GBD-Net\\n[109], PVANET [116], YOLO [17], YOLOv2 [72], R-FCN\\n[65], FPN [66], Mask R-CNN [67], DSSD [73] and DSOD\\n[74]. If no speciﬁc instructions for the adopted framework\\nare provided, the utilized model is a VGG16 [46] pretrained\\non 1000-way ImageNet classiﬁcation task [39]. Due to the\\nlimitation of paper length, we only provide an overview, in-\\ncluding proposal, learning method, loss function, programming\\nlanguage and platform, of the prominent architectures in Table\\nI. Detailed experimental settings, which can be found in the\\noriginal papers, are missed. In addition to the comparisons of\\ndetection accuracy, another comparison is provided to evaluate\\ntheir test consumption on PASCAL VOC 2007.\\n1) PASCAL VOC 2007/2012: PASCAL VOC 2007 and\\n2012 datasets consist of 20 categories. The evaluation terms\\nare Average Precision (AP) in each single category and mean\\nAverage Precision (mAP) across all the 20 categories. Com-\\nparative results are exhibited in Table II and III, from which\\nthe following remarks can be obtained.\\n• If incorporated with a proper way, more powerful back-\\nbone CNN models can deﬁnitely improve object detection\\nperformance (the comparison among R-CNN with AlexNet,\\nR-CNN with VGG16 and SPP-net with ZF-Net [122]).\\n• With the introduction of SPP layer (SPP-net), end-to-\\nend multi-task architecture (FRCN) and RPN (Faster R-\\nCNN), object detection performance is improved gradually\\nand apparently.\\n• Due to large quantities of trainable parameters, in order to\\nobtain multi-level robust features, data augmentation is very\\nimportant for deep learning based models (Faster R-CNN\\nwith ‘07’ ,‘07+12’ and ‘07+12+coco’).\\n• Apart from basic models, there are still many other factors\\naffecting object detection performance, such as multi-scale\\nand multi-region feature extraction (e.g. MR-CNN), modi-\\nﬁed classiﬁcation networks (e.g. NOC), additional informa-\\ntion from other correlated tasks (e.g. StuffNet, HyperNet),\\nmulti-scale representation (e.g. ION) and mining of hard\\nnegative samples (e.g. OHEM).\\n• As YOLO is not skilled in producing object localizations\\nof high IoU, it obtains a very poor result on VOC 2012.\\nHowever, with the complementary information from Fast\\nR-CNN (YOLO+FRCN) and the aid of other strategies,\\nsuch as anchor boxes, BN and ﬁne grained features, the\\nlocalization errors are corrected (YOLOv2).\\n• By combining many recent tricks and modelling the whole\\nnetwork as a fully convolutional one, R-FCN achieves a\\nmore obvious improvement of detection performance over\\nother approaches.\\n2) Microsoft COCO: Microsoft COCO is composed of\\n300,000 fully segmented images, in which each image has\\nan average of 7 object instances from a total of 80 categories.\\nAs there are a lot of less iconic objects with a broad range\\nof scales and a stricter requirement on object localization,\\nthis dataset is more challenging than PASCAL 2012. Object\\ndetection performance is evaluated by AP computed under\\ndifferent degrees of IoUs and on different object sizes. The\\nresults are shown in Table IV.\\nBesides similar remarks to those of PASCAL VOC, some\\nother conclusions can be drawn as follows from Table IV.\\n• Multi-scale training and test are beneﬁcial in improv-\\ning object detection performance, which provide additional\\ninformation in different resolutions (R-FCN). FPN and\\nDSSD provide some better ways to build feature pyramids\\nto achieve multi-scale representation. The complementary\\ninformation from other related tasks is also helpful for\\naccurate object localization (Mask R-CNN with instance\\nsegmentation task).\\n•\\nOverall,\\nregion\\nproposal\\nbased\\nmethods,\\nsuch\\nas\\nFaster R-CNN and R-FCN, perform better than regres-\\nsion/classﬁcation based approaches, namely YOLO and\\nSSD, due to the fact that quite a lot of localization errors\\nare produced by regression/classﬁcation based approaches.\\n• Context modelling is helpful to locate small objects,\\nwhich provides additional information by consulting nearby\\nobjects and surroundings (GBD-Net and multi-path).\\n• Due to the existence of a large number of nonstandard\\nsmall objects, the results on this dataset are much worse\\nthan those of VOC 2007/2012. With the introduction of\\nother powerful frameworks (e.g. ResNeXt [123]) and useful\\nstrategies (e.g. multi-task learning [67], [124]), the perfor-\\nmance can be improved.\\n• The success of DSOD in training from scratch stresses the\\nimportance of network design to release the requirements\\nfor perfect pre-trained classiﬁers on relevant tasks and large\\nnumbers of annotated samples.\\n3) Timing Analysis: Timing analysis (Table V) is conducted\\non Intel i7-6700K CPU with a single core and NVIDIA Titan'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'source': '../datas/pdf_files/object_detection.pdf', 'file_path': '../datas/pdf_files/object_detection.pdf', 'total_pages': 21, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'trapped': '', 'modDate': 'D:20190417004522Z', 'creationDate': 'D:20190417004522Z', 'page': 10}, page_content='THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION\\n11\\nTABLE I\\nAN OVERVIEW OF PROMINENT GENERIC OBJECT DETECTION ARCHITECTURES.\\nFramework\\nProposal\\nMulti-scale Input\\nLearning Method\\nLoss Function\\nSoftmax Layer\\nEnd-to-end Train\\nPlatform\\nLanguage\\nR-CNN [15]\\nSelective Search\\n-\\nSGD,BP\\nHinge loss (classiﬁcation),Bounding box regression\\n+\\n-\\nCaffe\\nMatlab\\nSPP-net [64]\\nEdgeBoxes\\n+\\nSGD\\nHinge loss (classiﬁcation),Bounding box regression\\n+\\n-\\nCaffe\\nMatlab\\nFast RCNN [16]\\nSelective Search\\n+\\nSGD\\nClass Log loss+bounding box regression\\n+\\n-\\nCaffe\\nPython\\nFaster R-CNN [18]\\nRPN\\n+\\nSGD\\nClass Log loss+bounding box regression\\n+\\n+\\nCaffe\\nPython/Matlab\\nR-FCN [65]\\nRPN\\n+\\nSGD\\nClass Log loss+bounding box regression\\n-\\n+\\nCaffe\\nMatlab\\nMask R-CNN [67]\\nRPN\\n+\\nSGD\\nClass Log loss+bounding box regression\\n+\\n+\\nTensorFlow/Keras\\nPython\\n+Semantic sigmoid loss\\nFPN [66]\\nRPN\\n+\\nSynchronized SGD\\nClass Log loss+bounding box regression\\n+\\n+\\nTensorFlow\\nPython\\nYOLO [17]\\n-\\n-\\nSGD\\nClass sum-squared error loss+bounding box regression\\n+\\n+\\nDarknet\\nC\\n+object conﬁdence+background conﬁdence\\nSSD [71]\\n-\\n-\\nSGD\\nClass softmax loss+bounding box regression\\n-\\n+\\nCaffe\\nC++\\nYOLOv2 [72]\\n-\\n-\\nSGD\\nClass sum-squared error loss+bounding box regression\\n+\\n+\\nDarknet\\nC\\n+object conﬁdence+background conﬁdence\\n* ‘+’ denotes that corresponding techniques are employed while ‘-’ denotes that this technique is not considered. It should be noticed that R-CNN and SPP-net can not be trained end-to-end with a multi-task loss while the\\nother architectures are based on multi-task joint training. As most of these architectures are re-implemented on different platforms with various programming languages, we only list the information associated with the versions\\nby the referenced authors.\\nTABLE II\\nCOMPARATIVE RESULTS ON VOC 2007 TEST SET (%).\\nMethods\\nTrained on\\nareo\\nbike\\nbird\\nboat\\nbottle\\nbus\\ncar\\ncat\\nchair\\ncow\\ntable\\ndog\\nhorse\\nmbike\\nperson\\nplant\\nsheep\\nsofa\\ntrain\\ntv\\nmAP\\nR-CNN (Alex) [15]\\n07\\n68.1\\n72.8\\n56.8\\n43.0\\n36.8\\n66.3\\n74.2\\n67.6\\n34.4\\n63.5\\n54.5\\n61.2\\n69.1\\n68.6\\n58.7\\n33.4\\n62.9\\n51.1\\n62.5\\n68.6\\n58.5\\nR-CNN(VGG16) [15]\\n07\\n73.4\\n77.0\\n63.4\\n45.4\\n44.6\\n75.1\\n78.1\\n79.8\\n40.5\\n73.7\\n62.2\\n79.4\\n78.1\\n73.1\\n64.2\\n35.6\\n66.8\\n67.2\\n70.4\\n71.1\\n66.0\\nSPP-net(ZF) [64]\\n07\\n68.5\\n71.7\\n58.7\\n41.9\\n42.5\\n67.7\\n72.1\\n73.8\\n34.7\\n67.0\\n63.4\\n66.0\\n72.5\\n71.3\\n58.9\\n32.8\\n60.9\\n56.1\\n67.9\\n68.8\\n60.9\\nGCNN [70]\\n07\\n68.3\\n77.3\\n68.5\\n52.4\\n38.6\\n78.5\\n79.5\\n81.0\\n47.1\\n73.6\\n64.5\\n77.2\\n80.5\\n75.8\\n66.6\\n34.3\\n65.2\\n64.4\\n75.6\\n66.4\\n66.8\\nBayes [85]\\n07\\n74.1\\n83.2\\n67.0\\n50.8\\n51.6\\n76.2\\n81.4\\n77.2\\n48.1\\n78.9\\n65.6\\n77.3\\n78.4\\n75.1\\n70.1\\n41.4\\n69.6\\n60.8\\n70.2\\n73.7\\n68.5\\nFast R-CNN [16]\\n07+12\\n77.0\\n78.1\\n69.3\\n59.4\\n38.3\\n81.6\\n78.6\\n86.7\\n42.8\\n78.8\\n68.9\\n84.7\\n82.0\\n76.6\\n69.9\\n31.8\\n70.1\\n74.8\\n80.4\\n70.4\\n70.0\\nSDP+CRC [33]\\n07\\n76.1\\n79.4\\n68.2\\n52.6\\n46.0\\n78.4\\n78.4\\n81.0\\n46.7\\n73.5\\n65.3\\n78.6\\n81.0\\n76.7\\n77.3\\n39.0\\n65.1\\n67.2\\n77.5\\n70.3\\n68.9\\nSubCNN [60]\\n07\\n70.2\\n80.5\\n69.5\\n60.3\\n47.9\\n79.0\\n78.7\\n84.2\\n48.5\\n73.9\\n63.0\\n82.7\\n80.6\\n76.0\\n70.2\\n38.2\\n62.4\\n67.7\\n77.7\\n60.5\\n68.5\\nStuffNet30 [100]\\n07\\n72.6\\n81.7\\n70.6\\n60.5\\n53.0\\n81.5\\n83.7\\n83.9\\n52.2\\n78.9\\n70.7\\n85.0\\n85.7\\n77.0\\n78.7\\n42.2\\n73.6\\n69.2\\n79.2\\n73.8\\n72.7\\nNOC [114]\\n07+12\\n76.3\\n81.4\\n74.4\\n61.7\\n60.8\\n84.7\\n78.2\\n82.9\\n53.0\\n79.2\\n69.2\\n83.2\\n83.2\\n78.5\\n68.0\\n45.0\\n71.6\\n76.7\\n82.2\\n75.7\\n73.3\\nMR-CNN&S-CNN [110]\\n07+12\\n80.3\\n84.1\\n78.5\\n70.8\\n68.5\\n88.0\\n85.9\\n87.8\\n60.3\\n85.2\\n73.7\\n87.2\\n86.5\\n85.0\\n76.4\\n48.5\\n76.3\\n75.5\\n85.0\\n81.0\\n78.2\\nHyperNet [101]\\n07+12\\n77.4\\n83.3\\n75.0\\n69.1\\n62.4\\n83.1\\n87.4\\n87.4\\n57.1\\n79.8\\n71.4\\n85.1\\n85.1\\n80.0\\n79.1\\n51.2\\n79.1\\n75.7\\n80.9\\n76.5\\n76.3\\nMS-GR [104]\\n07+12\\n80.0\\n81.0\\n77.4\\n72.1\\n64.3\\n88.2\\n88.1\\n88.4\\n64.4\\n85.4\\n73.1\\n87.3\\n87.4\\n85.1\\n79.6\\n50.1\\n78.4\\n79.5\\n86.9\\n75.5\\n78.6\\nOHEM+Fast R-CNN [113]\\n07+12\\n80.6\\n85.7\\n79.8\\n69.9\\n60.8\\n88.3\\n87.9\\n89.6\\n59.7\\n85.1\\n76.5\\n87.1\\n87.3\\n82.4\\n78.8\\n53.7\\n80.5\\n78.7\\n84.5\\n80.7\\n78.9\\nION [95]\\n07+12+S\\n80.2\\n85.2\\n78.8\\n70.9\\n62.6\\n86.6\\n86.9\\n89.8\\n61.7\\n86.9\\n76.5\\n88.4\\n87.5\\n83.4\\n80.5\\n52.4\\n78.1\\n77.2\\n86.9\\n83.5\\n79.2\\nFaster R-CNN [18]\\n07\\n70.0\\n80.6\\n70.1\\n57.3\\n49.9\\n78.2\\n80.4\\n82.0\\n52.2\\n75.3\\n67.2\\n80.3\\n79.8\\n75.0\\n76.3\\n39.1\\n68.3\\n67.3\\n81.1\\n67.6\\n69.9\\nFaster R-CNN [18]\\n07+12\\n76.5\\n79.0\\n70.9\\n65.5\\n52.1\\n83.1\\n84.7\\n86.4\\n52.0\\n81.9\\n65.7\\n84.8\\n84.6\\n77.5\\n76.7\\n38.8\\n73.6\\n73.9\\n83.0\\n72.6\\n73.2\\nFaster R-CNN [18]\\n07+12+COCO\\n84.3\\n82.0\\n77.7\\n68.9\\n65.7\\n88.1\\n88.4\\n88.9\\n63.6\\n86.3\\n70.8\\n85.9\\n87.6\\n80.1\\n82.3\\n53.6\\n80.4\\n75.8\\n86.6\\n78.9\\n78.8\\nSSD300 [71]\\n07+12+COCO\\n80.9\\n86.3\\n79.0\\n76.2\\n57.6\\n87.3\\n88.2\\n88.6\\n60.5\\n85.4\\n76.7\\n87.5\\n89.2\\n84.5\\n81.4\\n55.0\\n81.9\\n81.5\\n85.9\\n78.9\\n79.6\\nSSD512 [71]\\n07+12+COCO\\n86.6\\n88.3\\n82.4\\n76.0\\n66.3\\n88.6\\n88.9\\n89.1\\n65.1\\n88.4\\n73.6\\n86.5\\n88.9\\n85.3\\n84.6\\n59.1\\n85.0\\n80.4\\n87.4\\n81.2\\n81.6\\n* ‘07’: VOC2007 trainval, ‘07+12’: union of VOC2007 and VOC2012 trainval, ‘07+12+COCO’: trained on COCO trainval35k at ﬁrst and then ﬁne-tuned on 07+12. The S in ION ‘07+12+S’ denotes SBD segmentation labels.\\nTABLE III\\nCOMPARATIVE RESULTS ON VOC 2012 TEST SET (%).\\nMethods\\nTrained on\\nareo\\nbike\\nbird\\nboat\\nbottle\\nbus\\ncar\\ncat\\nchair\\ncow\\ntable\\ndog\\nhorse\\nmbike\\nperson\\nplant\\nsheep\\nsofa\\ntrain\\ntv\\nmAP\\nR-CNN(Alex) [15]\\n12\\n71.8\\n65.8\\n52.0\\n34.1\\n32.6\\n59.6\\n60.0\\n69.8\\n27.6\\n52.0\\n41.7\\n69.6\\n61.3\\n68.3\\n57.8\\n29.6\\n57.8\\n40.9\\n59.3\\n54.1\\n53.3\\nR-CNN(VGG16) [15]\\n12\\n79.6\\n72.7\\n61.9\\n41.2\\n41.9\\n65.9\\n66.4\\n84.6\\n38.5\\n67.2\\n46.7\\n82.0\\n74.8\\n76.0\\n65.2\\n35.6\\n65.4\\n54.2\\n67.4\\n60.3\\n62.4\\nBayes [85]\\n12\\n82.9\\n76.1\\n64.1\\n44.6\\n49.4\\n70.3\\n71.2\\n84.6\\n42.7\\n68.6\\n55.8\\n82.7\\n77.1\\n79.9\\n68.7\\n41.4\\n69.0\\n60.0\\n72.0\\n66.2\\n66.4\\nFast R-CNN [16]\\n07++12\\n82.3\\n78.4\\n70.8\\n52.3\\n38.7\\n77.8\\n71.6\\n89.3\\n44.2\\n73.0\\n55.0\\n87.5\\n80.5\\n80.8\\n72.0\\n35.1\\n68.3\\n65.7\\n80.4\\n64.2\\n68.4\\nSutffNet30 [100]\\n12\\n83.0\\n76.9\\n71.2\\n51.6\\n50.1\\n76.4\\n75.7\\n87.8\\n48.3\\n74.8\\n55.7\\n85.7\\n81.2\\n80.3\\n79.5\\n44.2\\n71.8\\n61.0\\n78.5\\n65.4\\n70.0\\nNOC [114]\\n07+12\\n82.8\\n79.0\\n71.6\\n52.3\\n53.7\\n74.1\\n69.0\\n84.9\\n46.9\\n74.3\\n53.1\\n85.0\\n81.3\\n79.5\\n72.2\\n38.9\\n72.4\\n59.5\\n76.7\\n68.1\\n68.8\\nMR-CNN&S-CNN [110]\\n07++12\\n85.5\\n82.9\\n76.6\\n57.8\\n62.7\\n79.4\\n77.2\\n86.6\\n55.0\\n79.1\\n62.2\\n87.0\\n83.4\\n84.7\\n78.9\\n45.3\\n73.4\\n65.8\\n80.3\\n74.0\\n73.9\\nHyperNet [101]\\n07++12\\n84.2\\n78.5\\n73.6\\n55.6\\n53.7\\n78.7\\n79.8\\n87.7\\n49.6\\n74.9\\n52.1\\n86.0\\n81.7\\n83.3\\n81.8\\n48.6\\n73.5\\n59.4\\n79.9\\n65.7\\n71.4\\nOHEM+Fast R-CNN [113]\\n07++12+coco\\n90.1\\n87.4\\n79.9\\n65.8\\n66.3\\n86.1\\n85.0\\n92.9\\n62.4\\n83.4\\n69.5\\n90.6\\n88.9\\n88.9\\n83.6\\n59.0\\n82.0\\n74.7\\n88.2\\n77.3\\n80.1\\nION [95]\\n07+12+S\\n87.5\\n84.7\\n76.8\\n63.8\\n58.3\\n82.6\\n79.0\\n90.9\\n57.8\\n82.0\\n64.7\\n88.9\\n86.5\\n84.7\\n82.3\\n51.4\\n78.2\\n69.2\\n85.2\\n73.5\\n76.4\\nFaster R-CNN [18]\\n07++12\\n84.9\\n79.8\\n74.3\\n53.9\\n49.8\\n77.5\\n75.9\\n88.5\\n45.6\\n77.1\\n55.3\\n86.9\\n81.7\\n80.9\\n79.6\\n40.1\\n72.6\\n60.9\\n81.2\\n61.5\\n70.4\\nFaster R-CNN [18]\\n07++12+coco\\n87.4\\n83.6\\n76.8\\n62.9\\n59.6\\n81.9\\n82.0\\n91.3\\n54.9\\n82.6\\n59.0\\n89.0\\n85.5\\n84.7\\n84.1\\n52.2\\n78.9\\n65.5\\n85.4\\n70.2\\n75.9\\nYOLO [17]\\n07++12\\n77.0\\n67.2\\n57.7\\n38.3\\n22.7\\n68.3\\n55.9\\n81.4\\n36.2\\n60.8\\n48.5\\n77.2\\n72.3\\n71.3\\n63.5\\n28.9\\n52.2\\n54.8\\n73.9\\n50.8\\n57.9\\nYOLO+Fast R-CNN [17]\\n07++12\\n83.4\\n78.5\\n73.5\\n55.8\\n43.4\\n79.1\\n73.1\\n89.4\\n49.4\\n75.5\\n57.0\\n87.5\\n80.9\\n81.0\\n74.7\\n41.8\\n71.5\\n68.5\\n82.1\\n67.2\\n70.7\\nYOLOv2 [72]\\n07++12+coco\\n88.8\\n87.0\\n77.8\\n64.9\\n51.8\\n85.2\\n79.3\\n93.1\\n64.4\\n81.4\\n70.2\\n91.3\\n88.1\\n87.2\\n81.0\\n57.7\\n78.1\\n71.0\\n88.5\\n76.8\\n78.2\\nSSD300 [71]\\n07++12+coco\\n91.0\\n86.0\\n78.1\\n65.0\\n55.4\\n84.9\\n84.0\\n93.4\\n62.1\\n83.6\\n67.3\\n91.3\\n88.9\\n88.6\\n85.6\\n54.7\\n83.8\\n77.3\\n88.3\\n76.5\\n79.3\\nSSD512 [71]\\n07++12+coco\\n91.4\\n88.6\\n82.6\\n71.4\\n63.1\\n87.4\\n88.1\\n93.9\\n66.9\\n86.6\\n66.3\\n92.0\\n91.7\\n90.8\\n88.5\\n60.9\\n87.0\\n75.4\\n90.2\\n80.4\\n82.2\\nR-FCN (ResNet101) [16]\\n07++12+coco\\n92.3\\n89.9\\n86.7\\n74.7\\n75.2\\n86.7\\n89.0\\n95.8\\n70.2\\n90.4\\n66.5\\n95.0\\n93.2\\n92.1\\n91.1\\n71.0\\n89.7\\n76.0\\n92.0\\n83.4\\n85.0\\n* ‘07++12’: union of VOC2007 trainval and test and VOC2012 trainval. ‘07++12+COCO’: trained on COCO trainval35k at ﬁrst then ﬁne-tuned on 07++12.\\nTABLE IV\\nCOMPARATIVE RESULTS ON MICROSOFT COCO TEST DEV SET (%).\\nMethods\\nTrained on 0.5:0.95 0.5 0.75\\nS\\nM\\nL\\n1\\n10\\n100\\nS\\nM\\nL\\nFast R-CNN [16]\\ntrain\\n20.5\\n39.9 19.4 4.1 20.0 35.8 21.3 29.4 30.1 7.3 32.1 52.0\\nION [95]\\ntrain\\n23.6\\n43.2 23.6 6.4 24.1 38.3 23.2 32.7 33.5 10.1 37.7 53.6\\nNOC+FRCN(VGG16) [114]\\ntrain\\n21.2\\n41.5 19.7\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\nNOC+FRCN(Google) [114]\\ntrain\\n24.8\\n44.4 25.2\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\nNOC+FRCN (ResNet101) [114]\\ntrain\\n27.2\\n48.4 27.6\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\nGBD-Net [109]\\ntrain\\n27.0\\n45.8\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\nOHEM+FRCN [113]\\ntrain\\n22.6\\n42.5 22.2 5.0 23.7 34.6\\n-\\n-\\n-\\n-\\n-\\n-\\nOHEM+FRCN* [113]\\ntrain\\n24.4\\n44.4 24.8 7.1 26.4 37.9\\n-\\n-\\n-\\n-\\n-\\n-\\nOHEM+FRCN* [113]\\ntrainval\\n25.5\\n45.9 26.1 7.4 27.7 38.5\\n-\\n-\\n-\\n-\\n-\\n-\\nFaster R-CNN [18]\\ntrainval\\n24.2\\n45.3 23.5 7.7 26.4 37.1 23.8 34.0 34.6 12.0 38.5 54.4\\nYOLOv2 [72]\\ntrainval35k\\n21.6\\n44.0 19.2 5.0 22.4 35.5 20.7 31.6 33.3 9.8 36.5 54.4\\nSSD300 [71]\\ntrainval35k\\n23.2\\n41.2 23.4 5.3 23.2 39.6 22.5 33.2 35.3 9.6 37.6 56.5\\nSSD512 [71]\\ntrainval35k\\n26.8\\n46.5 27.8 9.0 28.9 41.9 24.8 37.5 39.8 14.0 43.5 59.0\\nR-FCN (ResNet101) [65]\\ntrainval\\n29.2\\n51.5\\n-\\n10.8 32.8 45.0\\n-\\n-\\n-\\n-\\n-\\n-\\nR-FCN*(ResNet101) [65]\\ntrainval\\n29.9\\n51.9\\n-\\n10.4 32.4 43.3\\n-\\n-\\n-\\n-\\n-\\n-\\nR-FCN**(ResNet101) [65]\\ntrainval\\n31.5\\n53.2\\n-\\n14.3 35.5 44.2\\n-\\n-\\n-\\n-\\n-\\n-\\nMulti-path [112]\\ntrainval\\n33.2\\n51.9 36.3 13.6 37.2 47.8 29.9 46.0 48.3 23.4 56.0 66.4\\nFPN (ResNet101) [66]\\ntrainval35k\\n36.2\\n59.1 39.0 18.2 39.0 48.2\\n-\\n-\\n-\\n-\\n-\\n-\\nMask (ResNet101+FPN) [67]\\ntrainval35k\\n38.2\\n60.3 41.7 20.1 41.1 50.2\\n-\\n-\\n-\\n-\\n-\\n-\\nMask (ResNeXt101+FPN) [67] trainval35k\\n39.8\\n62.3 43.4 22.1 43.2 51.2\\n-\\n-\\n-\\n-\\n-\\n-\\nDSSD513 (ResNet101) [73]\\ntrainval35k\\n33.2\\n53.3 35.2 13.0 35.4 51.1 28.9 43.5 46.2 21.8 49.1 66.4\\nDSOD300 [74]\\ntrainval\\n29.3\\n47.3 30.6 9.4 31.5 47.0 27.3 40.7 43.0 16.7 47.1 65.0\\n* FRCN*: Fast R-CNN with multi-scale training, R-FCN*: R-FCN with multi-scale training, R-FCN**: R-FCN\\nwith multi-scale training and testing, Mask: Mask R-CNN.\\nX GPU. Except for ‘SS’ which is processed with CPU, the\\nother procedures related to CNN are all evaluated on GPU.\\nFrom Table V, we can draw some conclusions as follows.\\n• By computing CNN features on shared feature maps\\n(SPP-net), test consumption is reduced largely. Test time is\\nfurther reduced with the uniﬁed multi-task learning (FRCN)\\nand removal of additional region proposal generation stage\\n(Faster R-CNN). It’s also helpful to compress the parameters\\nof FC layers with SVD [91] (PAVNET and FRCN).\\nTABLE V\\nCOMPARISON OF TESTING CONSUMPTION ON VOC 07 TEST SET.\\nMethods\\nTrained on\\nmAP(%)\\nTest time(sec/img)\\nRate(FPS)\\nSS+R-CNN [15]\\n07\\n66.0\\n32.84\\n0.03\\nSS+SPP-net [64]\\n07\\n63.1\\n2.3\\n0.44\\nSS+FRCN [16]\\n07+12\\n66.9\\n1.72\\n0.6\\nSDP+CRC [33]\\n07\\n68.9\\n0.47\\n2.1\\nSS+HyperNet* [101]\\n07+12\\n76.3\\n0.20\\n5\\nMR-CNN&S-CNN [110]\\n07+12\\n78.2\\n30\\n0.03\\nION [95]\\n07+12+S\\n79.2\\n1.92\\n0.5\\nFaster R-CNN(VGG16) [18]\\n07+12\\n73.2\\n0.11\\n9.1\\nFaster R-CNN(ResNet101) [18]\\n07+12\\n83.8\\n2.24\\n0.4\\nYOLO [17]\\n07+12\\n63.4\\n0.02\\n45\\nSSD300 [71]\\n07+12\\n74.3\\n0.02\\n46\\nSSD512 [71]\\n07+12\\n76.8\\n0.05\\n19\\nR-FCN(ResNet101) [65]\\n07+12+coco\\n83.6\\n0.17\\n5.9\\nYOLOv2(544*544) [72]\\n07+12\\n78.6\\n0.03\\n40\\nDSSD321(ResNet101) [73]\\n07+12\\n78.6\\n0.07\\n13.6\\nDSOD300 [74]\\n07+12+coco\\n81.7\\n0.06\\n17.4\\nPVANET+ [116]\\n07+12+coco\\n83.8\\n0.05\\n21.7\\nPVANET+(compress) [116]\\n07+12+coco\\n82.9\\n0.03\\n31.3\\n* SS: Selective Search [15], SS*: ‘fast mode’ Selective Search [16], HyperNet*: the speed up version of\\nHyperNet and PAVNET+ (compresss): PAVNET with additional bounding box voting and compressed fully\\nconvolutional layers.\\n• It takes additional test time to extract multi-scale fea-\\ntures and contextual information (ION and MR-RCNN&S-\\nRCNN).\\n• It takes more time to train a more complex and deeper\\nnetwork (ResNet101 against VGG16) and this time con-\\nsumption can be reduced by adding as many layers into\\nshared fully convolutional layers as possible (FRCN).\\n• Regression based models can usually be processed in real-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'source': '../datas/pdf_files/object_detection.pdf', 'file_path': '../datas/pdf_files/object_detection.pdf', 'total_pages': 21, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'trapped': '', 'modDate': 'D:20190417004522Z', 'creationDate': 'D:20190417004522Z', 'page': 11}, page_content='THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION\\n12\\ntime at the cost of a drop in accuracy compared with region\\nproposal based models. Also, region proposal based models\\ncan be modiﬁed into real-time systems with the introduction\\nof other tricks [116] (PVANET), such as BN [43], residual\\nconnections [123].\\nIV. SALIENT OBJECT DETECTION\\nVisual saliency detection, one of the most important and\\nchallenging tasks in computer vision, aims to highlight the\\nmost dominant object regions in an image. Numerous ap-\\nplications incorporate the visual saliency to improve their\\nperformance, such as image cropping [125] and segmentation\\n[126], image retrieval [57] and object detection [66].\\nBroadly, there are two branches of approaches in salient\\nobject detection, namely bottom-up (BU) [127] and top-down\\n(TD) [128]. Local feature contrast plays the central role in BU\\nsalient object detection, regardless of the semantic contents of\\nthe scene. To learn local feature contrast, various local and\\nglobal features are extracted from pixels, e.g. edges [129],\\nspatial information [130]. However, high-level and multi-scale\\nsemantic information cannot be explored with these low-level\\nfeatures. As a result, low contrast salient maps instead of\\nsalient objects are obtained. TD salient object detection is task-\\noriented and takes prior knowledge about object categories\\nto guide the generation of salient maps. Taking semantic\\nsegmentation as an example, a saliency map is generated in the\\nsegmentation to assign pixels to particular object categories via\\na TD approach [131]. In a word, TD saliency can be viewed\\nas a focus-of-attention mechanism, which prunes BU salient\\npoints that are unlikely to be parts of the object [132].\\nA. Deep learning in Salient Object Detection\\nDue to the signiﬁcance for providing high-level and multi-\\nscale feature representation and the successful applications\\nin many correlated computer vision tasks, such as semantic\\nsegmentation [131], edge detection [133] and generic object\\ndetection [16], it is feasible and necessary to extend CNN to\\nsalient object detection.\\nThe early work by Eleonora Vig et al. [28] follows a\\ncompletely automatic data-driven approach to perform a large-\\nscale search for optimal features, namely an ensemble of deep\\nnetworks with different layers and parameters. To address the\\nproblem of limited training data, Kummerer et al. proposed the\\nDeep Gaze [134] by transferring from the AlexNet to generate\\na high dimensional feature space and create a saliency map. A\\nsimilar architecture was proposed by Huang et al. to integrate\\nsaliency prediction into pre-trained object recognition DNNs\\n[135]. The transfer is accomplished by ﬁne-tuning DNNs’\\nweights with an objective function based on the saliency\\nevaluation metrics, such as Similarity, KL-Divergence and\\nNormalized Scanpath Saliency.\\nSome works combined local and global visual clues to\\nimprove salient object detection performance. Wang et al.\\ntrained two independent deep CNNs (DNN-L and DNN-G)\\nto capture local information and global contrast and predicted\\nsaliency maps by integrating both local estimation and global\\nsearch [136]. Cholakkal et al. proposed a weakly supervised\\nsaliency detection framework to combine visual saliency from\\nbottom-up and top-down saliency maps, and reﬁned the results\\nwith a multi-scale superpixel-averaging [137]. Zhao et al.\\nproposed a multi-context deep learning framework, which\\nutilizes a uniﬁed learning framework to model global and\\nlocal context jointly with the aid of superpixel segmentation\\n[138]. To predict saliency in videos, Bak et al. fused two\\nstatic saliency models, namely spatial stream net and tem-\\nporal stream net, into a two-stream framework with a novel\\nempirically grounded data augmentation technique [139].\\nComplementary information from semantic segmentation\\nand context modeling is beneﬁcial. To learn internal represen-\\ntations of saliency efﬁciently, He et al. proposed a novel su-\\nperpixelwise CNN approach called SuperCNN [140], in which\\nsalient object detection is formulated as a binary labeling\\nproblem. Based on a fully convolutional neural network, Li\\net al. proposed a multi-task deep saliency model, in which\\nintrinsic correlations between saliency detection and semantic\\nsegmentation are set up [141]. However, due to the conv layers\\nwith large receptive ﬁelds and pooling layers, blurry object\\nboundaries and coarse saliency maps are produced. Tang et\\nal. proposed a novel saliency detection framework (CRPSD)\\n[142], which combines region-level saliency estimation and\\npixel-level saliency prediction together with three closely\\nrelated CNNs. Li et al. proposed a deep contrast network\\nto combine segment-wise spatial pooling and pixel-level fully\\nconvolutional streams [143].\\nThe proper integration of multi-scale feature maps is also\\nof signiﬁcance for improving detection performance. Based\\non Fast R-CNN, Wang et al. proposed the RegionNet by\\nperforming salient object detection with end-to-end edge pre-\\nserving and multi-scale contextual modelling [144]. Liu et al.\\n[27] proposed a multi-resolution convolutional neural network\\n(Mr-CNN) to predict eye ﬁxations, which is achieved by\\nlearning both bottom-up visual saliency and top-down visual\\nfactors from raw image data simultaneously. Cornia et al.\\nproposed an architecture which combines features extracted at\\ndifferent levels of the CNN [145]. Li et al. proposed a multi-\\nscale deep CNN framework to extract three scales of deep\\ncontrast features [146], namely the mean-subtracted region,\\nthe bounding box of its immediate neighboring regions and\\nthe masked entire image, from each candidate region.\\nIt is efﬁcient and accurate to train a direct pixel-wise\\nCNN architecture to predict salient objects with the aids of\\nRNNs and deconvolution networks. Pan et al. formulated\\nsaliency prediction as a minimization optimization on the\\nEuclidean distance between the predicted saliency map and\\nthe ground truth and proposed two kinds of architectures\\n[147]: a shallow one trained from scratch and a deeper one\\nadapted from deconvoluted VGG network. As convolutional-\\ndeconvolution networks are not expert in recognizing objects\\nof multiple scales, Kuen et al. proposed a recurrent attentional\\nconvolutional-deconvolution network (RACDNN) with several\\nspatial transformer and recurrent network units to conquer\\nthis problem [148]. To fuse local, global and contextual\\ninformation of salient objects, Tang et al. developed a deeply-\\nsupervised recurrent convolutional neural network (DSRCNN)\\nto perform a full image-to-image saliency detection [149].'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'source': '../datas/pdf_files/object_detection.pdf', 'file_path': '../datas/pdf_files/object_detection.pdf', 'total_pages': 21, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'trapped': '', 'modDate': 'D:20190417004522Z', 'creationDate': 'D:20190417004522Z', 'page': 12}, page_content='THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION\\n13\\nB. Experimental Evaluation\\nFour representative datasets, including ECSSD [156], HKU-\\nIS [146], PASCALS [157], and SOD [158], are used to\\nevaluate several state-of-the-art methods. ECSSD consists of\\n1000 structurally complex but semantically meaningful natural\\nimages. HKU-IS is a large-scale dataset containing over 4000\\nchallenging images. Most of these images have more than\\none salient object and own low contrast. PASCALS is a\\nsubset chosen from the validation set of PASCAL VOC 2010\\nsegmentation dataset and is composed of 850 natural images.\\nThe SOD dataset possesses 300 images containing multiple\\nsalient objects. The training and validation sets for different\\ndatasets are kept the same as those in [152].\\nTwo standard metrics, namely F-measure and the mean\\nabsolute error (MAE), are utilized to evaluate the quality of a\\nsaliency map. Given precision and recall values pre-computed\\non the union of generated binary mask B and ground truth Z,\\nF-measure is deﬁned as below\\nFβ = (1 + β2)Presion × Recall\\nβ2Presion + Recall\\n(7)\\nwhere β2 is set to 0.3 in order to stress the importance of the\\nprecision value.\\nThe MAE score is computed with the following equation\\nMAE =\\n1\\nH × W\\nH\\nX\\ni=1\\nW\\nX\\nj=1\\n\\x0c\\x0c\\x0c ˆS(i, j) = ˆZ(i, j)\\n\\x0c\\x0c\\x0c\\n(8)\\nwhere ˆZ and ˆS represent the ground truth and the continuous\\nsaliency map, respectively. W and H are the width and\\nheight of the salient area, respectively. This score stresses\\nthe importance of successfully detected salient objects over\\ndetected non-salient pixels [159].\\nThe following approaches are evaluated: CHM [150], RC\\n[151], DRFI [152], MC [138], MDF [146], LEGS [136], DSR\\n[149], MTDNN [141], CRPSD [142], DCL [143], ELD [153],\\nNLDF [154] and DSSC [155]. Among these methods, CHM,\\nRC and DRFI are classical ones with the best performance\\n[159], while the other methods are all associated with CNN.\\nF-measure and MAE scores are shown in Table VI.\\nFrom Table VI, we can ﬁnd that CNN based methods\\nperform better than classic methods. MC and MDF combine\\nthe information from local and global context to reach a\\nmore accurate saliency. ELD refers to low-level handcrafted\\nfeatures for complementary information. LEGS adopts generic\\nregion proposals to provide initial salient regions, which may\\nbe insufﬁcient for salient detection. DSR and MT act in\\ndifferent ways by introducing recurrent network and semantic\\nsegmentation, which provide insights for future improvements.\\nCPRSD, DCL, NLDF and DSSC are all based on multi-scale\\nrepresentations and superpixel segmentation, which provide\\nrobust salient regions and smooth boundaries. DCL, NLDF\\nand DSSC perform the best on these four datasets. DSSC\\nearns the best performance by modelling scale-to-scale short-\\nconnections.\\nOverall, as CNN mainly provides salient information in\\nlocal regions, most of CNN based methods need to model\\nvisual saliency along region boundaries with the aid of su-\\nperpixel segmentation. Meanwhile, the extraction of multi-\\nscale deep CNN features is of signiﬁcance for measuring local\\nconspicuity. Finally, it’s necessary to strengthen local con-\\nnections between different CNN layers and as well to utilize\\ncomplementary information from local and global context.\\nV. FACE DETECTION\\nFace detection is essential to many face applications and acts\\nas an important pre-processing procedure to face recognition\\n[160]–[162], face synthesis [163], [164] and facial expression\\nanalysis [165]. Different from generic object detection, this\\ntask is to recognize and locate face regions covering a very\\nlarge range of scales (30-300 pts vs. 10-1000 pts). At the same\\ntime, faces have their unique object structural conﬁgurations\\n(e.g. the distribution of different face parts) and characteristics\\n(e.g. skin color). All these differences lead to special attention\\nto this task. However, large visual variations of faces, such as\\nocclusions, pose variations and illumination changes, impose\\ngreat challenges for this task in real applications.\\nThe most famous face detector proposed by Viola and\\nJones [166] trains cascaded classiﬁers with Haar-Like features\\nand AdaBoost, achieving good performance with real-time\\nefﬁciency. However, this detector may degrade signiﬁcantly\\nin real-world applications due to larger visual variations of\\nhuman faces. Different from this cascade structure, Felzen-\\nszwalb et al. proposed a deformable part model (DPM) for face\\ndetection [24]. However, for these traditional face detection\\nmethods, high computational expenses and large quantities\\nof annotations are required to achieve a reasonable result.\\nBesides, their performance is greatly restricted by manually\\ndesigned features and shallow architecture.\\nA. Deep learning in Face Detection\\nRecently, some CNN based face detection approaches have\\nbeen proposed [167]–[169].As less accurate localization re-\\nsults from independent regressions of object coordinates, Yu\\net al. [167] proposed a novel IoU loss function for predicting\\nthe four bounds of box jointly. Farfade et al. [168] proposed a\\nDeep Dense Face Detector (DDFD) to conduct multi-view face\\ndetection, which is able to detect faces in a wide range of ori-\\nentations without requirement of pose/landmark annotations.\\nYang et al. proposed a novel deep learning based face detection\\nframework [169], which collects the responses from local fa-\\ncial parts (e.g. eyes, nose and mouths) to address face detection\\nunder severe occlusions and unconstrained pose variations.\\nYang et al. [170] proposed a scale-friendly detection network\\nnamed ScaleFace, which splits a large range of target scales\\ninto smaller sub-ranges. Different specialized sub-networks are\\nconstructed on these sub-scales and combined into a single\\none to conduct end-to-end optimization. Hao et al. designed an\\nefﬁcient CNN to predict the scale distribution histogram of the\\nfaces and took this histogram to guide the zoom-in and zoom-\\nout of the image [171]. Since the faces are approximately\\nin uniform scale after zoom, compared with other state-of-\\nthe-art baselines, better performance is achieved with less\\ncomputation cost. Besides, some generic detection frameworks'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'source': '../datas/pdf_files/object_detection.pdf', 'file_path': '../datas/pdf_files/object_detection.pdf', 'total_pages': 21, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'trapped': '', 'modDate': 'D:20190417004522Z', 'creationDate': 'D:20190417004522Z', 'page': 13}, page_content='THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION\\n14\\nTABLE VI\\nCOMPARISON BETWEEN STATE OF THE ART METHODS.\\nDataset\\nMetrics\\nCHM [150]\\nRC [151]\\nDRFI [152]\\nMC [138]\\nMDF [146]\\nLEGS [136]\\nDSR [149]\\nMTDNN [141]\\nCRPSD [142]\\nDCL [143]\\nELD [153]\\nNLDF [154]\\nDSSC [155]\\nPASCAL-S\\nwFβ\\n0.631\\n0.640\\n0.679\\n0.721\\n0.764\\n0.756\\n0.697\\n0.818\\n0.776\\n0.822\\n0.767\\n0.831\\n0.830\\nMAE\\n0.222\\n0.225\\n0.221\\n0.147\\n0.145\\n0.157\\n0.128\\n0.170\\n0.063\\n0.108\\n0.121\\n0.099\\n0.080\\nECSSD\\nwFβ\\n0.722\\n0.741\\n0.787\\n0.822\\n0.833\\n0.827\\n0.872\\n0.810\\n0.849\\n0.898\\n0.865\\n0.905\\n0.915\\nMAE\\n0.195\\n0.187\\n0.166\\n0.107\\n0.108\\n0.118\\n0.037\\n0.160\\n0.046\\n0.071\\n0.098\\n0.063\\n0.052\\nHKU-IS\\nwFβ\\n0.728\\n0.726\\n0.783\\n0.781\\n0.860\\n0.770\\n0.833\\n-\\n0.821\\n0.907\\n0.844\\n0.902\\n0.913\\nMAE\\n0.158\\n0.165\\n0.143\\n0.098\\n0.129\\n0.118\\n0.040\\n-\\n0.043\\n0.048\\n0.071\\n0.048\\n0.039\\nSOD\\nwFβ\\n0.655\\n0.657\\n0.712\\n0.708\\n0.785\\n0.707\\n-\\n0.781\\n-\\n0.832\\n0.760\\n0.810\\n0.842\\nMAE\\n0.249\\n0.242\\n0.215\\n0.184\\n0.155\\n0.205\\n-\\n0.150\\n-\\n0.126\\n0.154\\n0.143\\n0.118\\n* The bigger wFβ is or the smaller MAE is, the better the performance is.\\nare extended to face detection with different modiﬁcations, e.g.\\nFaster R-CNN [29], [172], [173].\\nSome authors trained CNNs with other complementary\\ntasks, such as 3D modelling and face landmarks, in a multi-\\ntask learning manner. Huang et al. proposed a uniﬁed end-\\nto-end FCN framework called DenseBox to jointly conduct\\nface detection and landmark localization [174]. Li et al.\\n[175] proposed a multi-task discriminative learning framework\\nwhich integrates a ConvNet with a ﬁxed 3D mean face model\\nin an end-to-end manner. In the framework, two issues are\\naddressed to transfer from generic object detection to face\\ndetection, namely eliminating predeﬁned anchor boxes by a\\n3D mean face model and replacing RoI pooling layer with\\na conﬁguration pooling layer. Zhang et al. [176] proposed a\\ndeep cascaded multi-task framework named MTCNN which\\nexploits the inherent correlations between face detection and\\nalignment in unconstrained environment to boost up detection\\nperformance in a coarse-to-ﬁne manner.\\nReducing computational expenses is of necessity in real ap-\\nplications. To achieve real-time detection on mobile platform,\\nKalinovskii and Spitsyn proposed a new solution of frontal\\nface detection based on compact CNN cascades [177]. This\\nmethod takes a cascade of three simple CNNs to generate,\\nclassify and reﬁne candidate object positions progressively.\\nTo reduce the effects of large pose variations, Chen et al.\\nproposed a cascaded CNN denoted by Supervised Transformer\\nNetwork [31]. This network takes a multi-task RPN to predict\\ncandidate face regions along with associated facial landmarks\\nsimultaneously, and adopts a generic R-CNN to verify the\\nexistence of valid faces. Yang et al. proposed a three-stage\\ncascade structure based on FCNs [8], while in each stage, a\\nmulti-scale FCN is utilized to reﬁne the positions of possible\\nfaces. Qin et al. proposed a uniﬁed framework which achieves\\nbetter results with the complementary information from dif-\\nferent jointly trained CNNs [178].\\nB. Experimental Evaluation\\nThe FDDB [179] dataset has a total of 2,845 pictures in\\nwhich 5,171 faces are annotated with elliptical shape. Two\\ntypes of evaluations are used: the discrete score and continuous\\nscore. By varying the threshold of the decision rule, the ROC\\ncurve for the discrete scores can reﬂect the dependence of\\nthe detected face fractions on the number of false alarms.\\nCompared with annotations, any detection with an IoU ratio\\nexceeding 0.5 is treated as positive. Each annotation is only\\nassociated with one detection. The ROC curve for the contin-\\nuous scores is the reﬂection of face localization quality.\\nThe evaluated models cover DDFD [168], CascadeCNN\\n[180], ACF-multiscale [181], Pico [182], HeadHunter [183],\\n 0\\n 0.1\\n 0.2\\n 0.3\\n 0.4\\n 0.5\\n 0.6\\n 0.7\\n 0.8\\n 0.9\\n 1\\n 0\\n 500\\n 1000\\n 1500\\n 2000\\nTrue positive rate\\nFalse positive\\nDDFD\\nCascadeCNN\\nACF-multiscale\\nPico\\nHeadHunter\\nJoint Cascade\\nSURF-multiview\\nViola-Jones\\nNPDFace\\nFaceness\\nCCF\\nMTCNN\\nConv3D\\nHyperface\\nUnitBox\\nLDCF+\\nDeepIR\\nHR-ER\\nFace-R-CNN\\nScaleFace\\n(a) Discrete ROC curves\\n 0\\n 0.1\\n 0.2\\n 0.3\\n 0.4\\n 0.5\\n 0.6\\n 0.7\\n 0.8\\n 0.9\\n 1\\n 0\\n 500\\n 1000\\n 1500\\n 2000\\nTrue positive rate\\nFalse positive\\nDDFD\\nCascadeCNN\\nACF-multiscale\\nPico\\nHeadHunter\\nJoint Cascade\\nSURF-multiview\\nViola-Jones\\nNPDFace\\nFaceness\\nCCF\\nMTCNN\\nConv3D\\nHyperface\\nUnitBox\\nLDCF+\\nDeepIR\\nHR-ER\\nFace-R-CNN\\nScaleFace\\n(b) Continuous ROC curves\\nFig. 11. The ROC curves of state-of-the-art methods on FDDB.\\nJoint Cascade [30], SURF-multiview [184], Viola-Jones [166],\\nNPDFace [185], Faceness [169], CCF [186], MTCNN [176],\\nConv3D [175], Hyperface [187], UnitBox [167], LDCF+ [S2],\\nDeepIR [173], HR-ER [188], Face-R-CNN [172] and Scale-\\nFace [170]. ACF-multiscale, Pico, HeadHunter, Joint Cascade,\\nSURF-multiview, Viola-Jones, NPDFace and LDCF+ are built\\non classic hand-crafted features while the rest methods are\\nbased on deep CNN features. The ROC curves are shown in\\nFigure 11.\\nFrom Figure 11(a), in spite of relatively competitive results\\nproduced by LDCF+, it can be observed that most of classic\\nmethods perform with similar results and are outperformed\\nby CNN based methods by a signiﬁcant margin. From Figure\\n11(b), it can be observed that most of CNN based methods\\nearn similar true positive rates between 60% and 70% while\\nDeepIR and HR-ER perform much better than them. Among\\nclassic methods, Joint Cascade is still competitive. As earlier\\nworks, DDFD and CCF directly make use of generated feature\\nmaps and obtain relatively poor results. CascadeCNN builds\\ncascaded CNNs to locate face regions, which is efﬁcient but in-\\naccurate. Faceness combines the decisions from different part\\ndetectors, resulting in precise face localizations while being\\ntime-consuming. The outstanding performance of MTCNN,\\nConv3D and Hyperface proves the effectiveness of multi-task\\nlearning. HR-ER and ScaleFace adaptively detect faces of\\ndifferent scales, and make a balance between accuracy and\\nefﬁciency. DeepIR and Face-R-CNN are two extensions of the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'source': '../datas/pdf_files/object_detection.pdf', 'file_path': '../datas/pdf_files/object_detection.pdf', 'total_pages': 21, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'trapped': '', 'modDate': 'D:20190417004522Z', 'creationDate': 'D:20190417004522Z', 'page': 14}, page_content='THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION\\n15\\nFaster R-CNN architecture to face detection, which validate\\nthe signiﬁcance and effectiveness of Faster R-CNN. Unitbox\\nprovides an alternative choice for performance improvements\\nby carefully designing optimization loss.\\nFrom these results, we can draw the conclusion that\\nCNN based methods are in the leading position. The perfor-\\nmance can be improved by the following strategies: designing\\nnovel optimization loss, modifying generic detection pipelines,\\nbuilding meaningful network cascades, adapting scale-aware\\ndetection and learning multi-task shared CNN features.\\nVI. PEDESTRIAN DETECTION\\nRecently, pedestrian detection has been intensively studied,\\nwhich has a close relationship to pedestrian tracking [189],\\n[190], person re-identiﬁcation [191], [192] and robot naviga-\\ntion [193], [194]. Prior to the recent progress in DCNN based\\nmethods [195], [196], some researchers combined boosted\\ndecision forests with hand-crafted features to obtain pedestrian\\ndetectors [197]–[199]. At the same time, to explicitly model\\nthe deformation and occlusion, part-based models [200] and\\nexplicit occlusion handling [201], [202] are of concern.\\nAs there are many pedestrian instances of small sizes\\nin typical scenarios of pedestrian detection (e.g. automatic\\ndriving and intelligent surveillance), the application of RoI\\npooling layer in generic object detection pipeline may result\\nin ‘plain’ features due to collapsing bins. In the meantime, the\\nmain source of false predictions in pedestrian detection is the\\nconfusion of hard background instances, which is in contrast\\nto the interference from multiple categories in generic object\\ndetection. As a result, different conﬁgurations and components\\nare required to accomplish accurate pedestrian detection.\\nA. Deep learning in Pedestrian Detection\\nAlthough DCNNs have obtained excellent performance on\\ngeneric object detection [16], [72], none of these approaches\\nhave achieved better results than the best hand-crafted feature\\nbased method [198] for a long time, even when part-based\\ninformation and occlusion handling are incorporated [202].\\nThereby, some researches have been conducted to analyze the\\nreasons. Zhang et al. attempted to adapt generic Faster R-CNN\\n[18] to pedestrian detection [203]. They modiﬁed the down-\\nstream classiﬁer by adding boosted forests to shared, high-\\nresolution conv feature maps and taking a RPN to handle small\\ninstances and hard negative examples. To deal with complex\\nocclusions existing in pedestrian images, inspired by DPM\\n[24], Tian et al. proposed a deep learning framework called\\nDeepParts [204], which makes decisions based an ensemble of\\nextensive part detectors. DeepParts has advantages in dealing\\nwith weakly labeled data, low IoU positive proposals and\\npartial occlusion.\\nOther researchers also tried to combine complementary in-\\nformation from multiple data sources. CompACT-Deep adopts\\na complexity-aware cascade to combine hand-crafted features\\nand ﬁne-tuned DCNNs [195]. Based on Faster R-CNN, Liu et\\nal. proposed multi-spectral deep neural networks for pedestrian\\ndetection to combine complementary information from color\\nand thermal images [205]. Tian et al. [206] proposed a task-\\nassistant CNN (TA-CNN) to jointly learn multiple tasks with\\nTABLE VII\\nDETAILED BREAKDOWN PERFORMANCE COMPARISONS OF\\nSTATE-OF-THE-ART MODELS ON CALTECH PEDESTRIAN DATASET. ALL\\nNUMBERS ARE REPORTED IN L-AMR.\\nMethod\\nReasonable\\nAll\\nFar\\nMedium\\nNear\\nnone\\npartial\\nheavy\\nCheckerboards+ [198]\\n17.1\\n68.4\\n100\\n58.3\\n5.1\\n15.6\\n31.4\\n78.4\\nLDCF++[S2]\\n15.2\\n67.1\\n100\\n58.4\\n5.4\\n13.3\\n33.3\\n76.2\\nSCF+AlexNet [210]\\n23.3\\n70.3\\n100\\n62.3\\n10.2\\n20.0\\n48.5\\n74.7\\nSA-FastRCNN [211]\\n9.7\\n62.6\\n100\\n51.8\\n0\\n7.7\\n24.8\\n64.3\\nMS-CNN [105]\\n10.0\\n61.0\\n97.2\\n49.1\\n2.6\\n8.2\\n19.2\\n60.0\\nDeepParts [204]\\n11.9\\n64.8\\n100\\n56.4\\n4.8\\n10.6\\n19.9\\n60.4\\nCompACT-Deep [195]\\n11.8\\n64.4\\n100\\n53.2\\n4.0\\n9.6\\n25.1\\n65.8\\nRPN+BF [203]\\n9.6\\n64.7\\n100\\n53.9\\n2.3\\n7.7\\n24.2\\n74.2\\nF-DNN+SS [207]\\n8.2\\n50.3\\n77.5\\n33.2\\n2.8\\n6.7\\n15.1\\n53.4\\nmultiple data sources and to combine pedestrian attributes\\nwith semantic scene attributes together. Du et al. proposed\\na deep neural network fusion architecture for fast and robust\\npedestrian detection [207]. Based on the candidate bounding\\nboxes generated with SSD detectors [71], multiple binary\\nclassiﬁers are processed parallelly to conduct soft-rejection\\nbased network fusion (SNF) by consulting their aggregated\\ndegree of conﬁdences.\\nHowever, most of these approaches are much more sophisti-\\ncated than the standard R-CNN framework. CompACT-Deep\\nconsists of a variety of hand-crafted features, a small CNN\\nmodel and a large VGG16 model [195]. DeepParts contains\\n45 ﬁne-tuned DCNN models, and a set of strategies, including\\nbounding box shifting handling and part selection, are required\\nto arrive at the reported results [204]. So the modiﬁcation and\\nsimpliﬁcation is of signiﬁcance to reduce the burden on both\\nsoftware and hardware to satisfy real-time detection demand.\\nTome et al. proposed a novel solution to adapt generic object\\ndetection pipeline to pedestrian detection by optimizing most\\nof its stages [59]. Hu et al. [208] trained an ensemble of\\nboosted decision models by reusing the conv feature maps, and\\na further improvement was gained with simple pixel labelling\\nand additional complementary hand-crafted features. Tome\\net al. [209] proposed a reduced memory region based deep\\nCNN architecture, which fuses regional responses from both\\nACF detectors and SVM classiﬁers into R-CNN. Ribeiro et\\nal. addressed the problem of Human-Aware Navigation [32]\\nand proposed a vision-based person tracking system guided\\nby multiple camera sensors.\\nB. Experimental Evaluation\\nThe evaluation is conducted on the most popular Caltech\\nPedestrian dataset [3]. The dataset was collected from the\\nvideos of a vehicle driving through an urban environment\\nand consists of 250,000 frames with about 2300 unique\\npedestrians and 350,000 annotated bounding boxes (BBs).\\nThree kinds of labels, namely ‘Person (clear identiﬁcations)’,\\n‘Person? (unclear identiﬁcations)’ and ‘People (large group of\\nindividuals)’, are assigned to different BBs. The performance\\nis measured with the log-average miss rate (L-AMR) which\\nis computed evenly spaced in log-space in the range 10−2 to\\n1 by averaging miss rate at the rate of nine false positives\\nper image (FPPI) [3]. According to the differences in the\\nheight and visible part of the BBs, a total of 9 popular settings\\nare adopted to evaluate different properties of these models.\\nDetails of these settings are as [3].\\nEvaluated methods include Checkerboards+ [198], LDCF++\\n[S2], SCF+AlexNet [210], SA-FastRCNN [211], MS-CNN'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'source': '../datas/pdf_files/object_detection.pdf', 'file_path': '../datas/pdf_files/object_detection.pdf', 'total_pages': 21, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'trapped': '', 'modDate': 'D:20190417004522Z', 'creationDate': 'D:20190417004522Z', 'page': 15}, page_content='THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION\\n16\\n[105], DeepParts [204], CompACT-Deep [195], RPN+BF\\n[203] and F-DNN+SS [207]. The ﬁrst two methods are based\\non hand-crafted features while the rest ones rely on deep CNN\\nfeatures. All results are exhibited in Table VII. From this table,\\nwe observe that different from other tasks, classic handcrafted\\nfeatures can still earn competitive results with boosted decision\\nforests [203], ACF [197] and HOG+LUV channels [S2]. As\\nan early attempt to adapt CNN to pedestrian detection, the\\nfeatures generated by SCF+AlexNet are not so discriminant\\nand produce relatively poor results. Based on multiple CNNs,\\nDeepParts and CompACT-Deep accomplish detection tasks via\\ndifferent strategies, namely local part integration and cascade\\nnetwork. The responses from different local part detectors\\nmake DeepParts robust to partial occlusions. However, due to\\ncomplexity, it is too time-consuming to achieve real-time de-\\ntection. The multi-scale representation of MS-CNN improves\\naccuracy of pedestrian locations. SA-FastRCNN extends Fast\\nR-CNN to automatically detecting pedestrians according to\\ntheir different scales, which has trouble when there are partial\\nocclusions. RPN+BF combines the detectors produced by\\nFaster R-CNN with boosting decision forest to accurately\\nlocate different pedestrians. F-DNN+SS, which is composed\\nof multiple parallel classiﬁers with soft rejections, performs\\nthe best followed by RPN+BF, SA-FastRCNN and MS-CNN.\\nIn short, CNN based methods can provide more accurate\\ncandidate boxes and multi-level semantic information for\\nidentifying and locating pedestrians. Meanwhile, handcrafted\\nfeatures are complementary and can be combined with CNN\\nto achieve better results. The improvements over existing CNN\\nmethods can be obtained by carefully designing the framework\\nand classiﬁers, extracting multi-scale and part based semantic\\ninformation and searching for complementary information\\nfrom other related tasks, such as segmentation.\\nVII. PROMISING FUTURE DIRECTIONS AND TASKS\\nIn spite of rapid development and achieved promising\\nprogress of object detection, there are still many open issues\\nfor future work.\\nThe ﬁrst one is small object detection such as occurring\\nin COCO dataset and in face detection task. To improve\\nlocalization accuracy on small objects under partial occlusions,\\nit is necessary to modify network architectures from the\\nfollowing aspects.\\n• Multi-task joint optimization and multi-modal infor-\\nmation fusion. Due to the correlations between different\\ntasks within and outside object detection, multi-task joint\\noptimization has already been studied by many researchers\\n[16] [18]. However, apart from the tasks mentioned in\\nSubs. III-A8, it is desirable to think over the characteristics\\nof different sub-tasks of object detection (e.g. superpixel\\nsemantic segmentation in salient object detection) and ex-\\ntend multi-task optimization to other applications such as\\ninstance segmentation [66], multi-object tracking [202] and\\nmulti-person pose estimation [S4]. Besides, given a speciﬁc\\napplication, the information from different modalities, such\\nas text [212], thermal data [205] and images [65], can be\\nfused together to achieve a more discriminant network.\\n• Scale adaption. Objects usually exist in different scales,\\nwhich is more apparent in face detection and pedestrian\\ndetection. To increase the robustness to scale changes, it\\nis demanded to train scale-invariant, multi-scale or scale-\\nadaptive detectors. For scale-invariant detectors, more pow-\\nerful backbone architectures (e.g. ResNext [123]), negative\\nsample mining [113], reverse connection [213] and sub-\\ncategory modelling [60] are all beneﬁcial. For multi-scale\\ndetectors, both the FPN [66] which produces multi-scale\\nfeature maps and Generative Adversarial Network [214]\\nwhich narrows representation differences between small ob-\\njects and the large ones with a low-cost architecture provide\\ninsights into generating meaningful feature pyramid. For\\nscale-adaptive detectors, it is useful to combine knowledge\\ngraph [215], attentional mechanism [216], cascade network\\n[180] and scale distribution estimation [171] to detect ob-\\njects adaptively.\\n• Spatial correlations and contextual modelling. Spatial\\ndistribution plays an important role in object detection. So\\nregion proposal generation and grid regression are taken\\nto obtain probable object locations. However, the corre-\\nlations between multiple proposals and object categories\\nare ignored. Besides, the global structure information is\\nabandoned by the position-sensitive score maps in R-FCN.\\nTo solve these problems, we can refer to diverse subset\\nselection [217] and sequential reasoning tasks [218] for\\npossible solutions. It is also meaningful to mask salient parts\\nand couple them with the global structure in a joint-learning\\nmanner [219].\\nThe second one is to release the burden on manual labor and\\naccomplish real-time object detection, with the emergence of\\nlarge-scale image and video data. The following three aspects\\ncan be taken into account.\\n• Cascade network. In a cascade network, a cascade of\\ndetectors are built in different stages or layers [180], [220].\\nAnd easily distinguishable examples are rejected at shallow\\nlayers so that features and classiﬁers at latter stages can\\nhandle more difﬁcult samples with the aid of the decisions\\nfrom previous stages. However, current cascades are built in\\na greedy manner, where previous stages in cascade are ﬁxed\\nwhen training a new stage. So the optimizations of different\\nCNNs are isolated, which stresses the necessity of end-to-\\nend optimization for CNN cascade. At the same time, it\\nis also a matter of concern to build contextual associated\\ncascade networks with existing layers.\\n• Unsupervised and weakly supervised learning. It’s\\nvery time consuming to manually draw large quantities\\nof bounding boxes. To release this burden, semantic prior\\n[55], unsupervised object discovery [221], multiple instance\\nlearning [222] and deep neural network prediction [47] can\\nbe integrated to make best use of image-level supervision to\\nassign object category tags to corresponding object regions\\nand reﬁne object boundaries. Furthermore, weakly annota-\\ntions (e.g. center-click annotations [223]) are also helpful\\nfor achieving high-quality detectors with modest annotation\\nefforts, especially aided by the mobile platform.\\n• Network optimization. Given speciﬁc applications and\\nplatforms, it is signiﬁcant to make a balance among speed,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'source': '../datas/pdf_files/object_detection.pdf', 'file_path': '../datas/pdf_files/object_detection.pdf', 'total_pages': 21, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'trapped': '', 'modDate': 'D:20190417004522Z', 'creationDate': 'D:20190417004522Z', 'page': 16}, page_content='THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION\\n17\\nmemory and accuracy by selecting an optimal detection\\narchitecture [116], [224]. However, despite that detection\\naccuracy is reduced, it is more meaningful to learn compact\\nmodels with fewer number of parameters [209]. And this\\nsituation can be relieved by introducing better pre-training\\nschemes [225], knowledge distillation [226] and hint learn-\\ning [227]. DSOD also provides a promising guideline to\\ntrain from scratch to bridge the gap between different image\\nsources and tasks [74].\\nThe third one is to extend typical methods for 2D object de-\\ntection to adapt 3D object detection and video object detection,\\nwith the requirements from autonomous driving, intelligent\\ntransportation and intelligent surveillance.\\n• 3D object detection. With the applications of 3D sensors\\n(e.g. LIDAR and camera), additional depth information can\\nbe utilized to better understand the images in 2D and extend\\nthe image-level knowledge to the real world. However,\\nseldom of these 3D-aware techniques aim to place correct\\n3D bounding boxes around detected objects. To achieve\\nbetter bounding results, multi-view representation [181] and\\n3D proposal network [228] may provide some guidelines to\\nencode depth information with the aid of inertial sensors\\n(accelerometer and gyrometer) [229].\\n• Video object detection. Temporal information across\\ndifferent frames play an important role in understanding\\nthe behaviors of different objects. However, the accuracy\\nsuffers from degenerated object appearances (e.g., motion\\nblur and video defocus) in videos and the network is\\nusually not trained end-to-end. To this end, spatiotemporal\\ntubelets [230], optical ﬂow [199] and LSTM [107] should\\nbe considered to fundamentally model object associations\\nbetween consecutive frames.\\nVIII. CONCLUSION\\nDue to its powerful learning ability and advantages in\\ndealing with occlusion, scale transformation and background\\nswitches, deep learning based object detection has been a\\nresearch hotspot in recent years. This paper provides a detailed\\nreview on deep learning based object detection frameworks\\nwhich handle different sub-problems, such as occlusion, clutter\\nand low resolution, with different degrees of modiﬁcations\\non R-CNN. The review starts on generic object detection\\npipelines which provide base architectures for other related\\ntasks. Then, three other common tasks, namely salient object\\ndetection, face detection and pedestrian detection, are also\\nbrieﬂy reviewed. Finally, we propose several promising future\\ndirections to gain a thorough understanding of the object\\ndetection landscape. This review is also meaningful for the\\ndevelopments in neural networks and related learning systems,\\nwhich provides valuable insights and guidelines for future\\nprogress.\\nACKNOWLEDGMENTS\\nThis research was supported by the National Natural Sci-\\nence Foundation of China (No.61672203 & 61375047 &\\n91746209), the National Key Research and Development Pro-\\ngram of China (2016YFB1000901), and Anhui Natural Sci-\\nence Funds for Distinguished Young Scholar (No.170808J08).\\nREFERENCES\\n[1] P. F. Felzenszwalb, R. B. Girshick, D. Mcallester, and D. Ramanan,\\n“Object detection with discriminatively trained part-based models,”\\nIEEE Trans. Pattern Anal. Mach. Intell., vol. 32, no. 9, p. 1627, 2010.\\n[2] K. K. Sung and T. Poggio, “Example-based learning for view-based\\nhuman face detection,” IEEE Trans. Pattern Anal. Mach. Intell., vol. 20,\\nno. 1, pp. 39–51, 2002.\\n[3] C. Wojek, P. Dollar, B. Schiele, and P. Perona, “Pedestrian detection:\\nAn evaluation of the state of the art,” IEEE Trans. Pattern Anal. Mach.\\nIntell., vol. 34, no. 4, p. 743, 2012.\\n[4] H. Kobatake and Y. Yoshinaga, “Detection of spicules on mammogram\\nbased on skeleton analysis.” IEEE Trans. Med. Imag., vol. 15, no. 3,\\npp. 235–245, 1996.\\n[5] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Girshick,\\nS. Guadarrama, and T. Darrell, “Caffe: Convolutional architecture for\\nfast feature embedding,” in ACM MM, 2014.\\n[6] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classiﬁcation\\nwith deep convolutional neural networks,” in NIPS, 2012.\\n[7] Z. Cao, T. Simon, S.-E. Wei, and Y. Sheikh, “Realtime multi-person\\n2d pose estimation using part afﬁnity ﬁelds,” in CVPR, 2017.\\n[8] Z. Yang and R. Nevatia, “A multi-scale cascade fully convolutional\\nnetwork face detector,” in ICPR, 2016.\\n[9] C. Chen, A. Seff, A. L. Kornhauser, and J. Xiao, “Deepdriving:\\nLearning affordance for direct perception in autonomous driving,” in\\nICCV, 2015.\\n[10] X. Chen, H. Ma, J. Wan, B. Li, and T. Xia, “Multi-view 3d object\\ndetection network for autonomous driving,” in CVPR, 2017.\\n[11] A. Dundar, J. Jin, B. Martini, and E. Culurciello, “Embedded streaming\\ndeep neural networks accelerator with applications,” IEEE Trans.\\nNeural Netw. & Learning Syst., vol. 28, no. 7, pp. 1572–1583, 2017.\\n[12] R. J. Cintra, S. Duffner, C. Garcia, and A. Leite, “Low-complexity\\napproximate convolutional neural networks,” IEEE Trans. Neural Netw.\\n& Learning Syst., vol. PP, no. 99, pp. 1–12, 2018.\\n[13] S. H. Khan, M. Hayat, M. Bennamoun, F. A. Sohel, and R. Togneri,\\n“Cost-sensitive learning of deep feature representations from imbal-\\nanced data.” IEEE Trans. Neural Netw. & Learning Syst., vol. PP,\\nno. 99, pp. 1–15, 2017.\\n[14] A. Stuhlsatz, J. Lippel, and T. Zielke, “Feature extraction with deep\\nneural networks by a generalized discriminant analysis.” IEEE Trans.\\nNeural Netw. & Learning Syst., vol. 23, no. 4, pp. 596–608, 2012.\\n[15] R. Girshick, J. Donahue, T. Darrell, and J. Malik, “Rich feature\\nhierarchies for accurate object detection and semantic segmentation,”\\nin CVPR, 2014.\\n[16] R. Girshick, “Fast r-cnn,” in ICCV, 2015.\\n[17] J. Redmon, S. Divvala, R. Girshick, and A. Farhadi, “You only look\\nonce: Uniﬁed, real-time object detection,” in CVPR, 2016.\\n[18] S. Ren, K. He, R. Girshick, and J. Sun, “Faster r-cnn: Towards real-\\ntime object detection with region proposal networks,” in NIPS, 2015,\\npp. 91–99.\\n[19] D. G. Lowe, “Distinctive image features from scale-invariant key-\\npoints,” Int. J. of Comput. Vision, vol. 60, no. 2, pp. 91–110, 2004.\\n[20] N. Dalal and B. Triggs, “Histograms of oriented gradients for human\\ndetection,” in CVPR, 2005.\\n[21] R. Lienhart and J. Maydt, “An extended set of haar-like features for\\nrapid object detection,” in ICIP, 2002.\\n[22] C. Cortes and V. Vapnik, “Support vector machine,” Machine Learning,\\nvol. 20, no. 3, pp. 273–297, 1995.\\n[23] Y. Freund and R. E. Schapire, “A desicion-theoretic generalization of\\non-line learning and an application to boosting,” J. of Comput. & Sys.\\nSci., vol. 13, no. 5, pp. 663–671, 1997.\\n[24] P. F. Felzenszwalb, R. B. Girshick, D. McAllester, and D. Ramanan,\\n“Object detection with discriminatively trained part-based models,”\\nIEEE Trans. Pattern Anal. Mach. Intell., vol. 32, pp. 1627–1645, 2010.\\n[25] M. Everingham, L. Van Gool, C. K. Williams, J. Winn, and A. Zis-\\nserman, “The pascal visual object classes challenge 2007 (voc 2007)\\nresults (2007),” 2008.\\n[26] Y. LeCun, Y. Bengio, and G. Hinton, “Deep learning,” Nature, vol.\\n521, no. 7553, pp. 436–444, 2015.\\n[27] N. Liu, J. Han, D. Zhang, S. Wen, and T. Liu, “Predicting eye ﬁxations\\nusing convolutional neural networks,” in CVPR, 2015.\\n[28] E. Vig, M. Dorr, and D. Cox, “Large-scale optimization of hierarchical\\nfeatures for saliency prediction in natural images,” in CVPR, 2014.\\n[29] H. Jiang and E. Learned-Miller, “Face detection with the faster r-cnn,”\\nin FG, 2017.\\n[30] D. Chen, S. Ren, Y. Wei, X. Cao, and J. Sun, “Joint cascade face\\ndetection and alignment,” in ECCV, 2014.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'source': '../datas/pdf_files/object_detection.pdf', 'file_path': '../datas/pdf_files/object_detection.pdf', 'total_pages': 21, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'trapped': '', 'modDate': 'D:20190417004522Z', 'creationDate': 'D:20190417004522Z', 'page': 17}, page_content='THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION\\n18\\n[31] D. Chen, G. Hua, F. Wen, and J. Sun, “Supervised transformer network\\nfor efﬁcient face detection,” in ECCV, 2016.\\n[32] D. Ribeiro, A. Mateus, J. C. Nascimento, and P. Miraldo, “A real-time\\npedestrian detector using deep learning for human-aware navigation,”\\narXiv:1607.04441, 2016.\\n[33] F. Yang, W. Choi, and Y. Lin, “Exploit all the layers: Fast and accurate\\ncnn object detector with scale dependent pooling and cascaded rejection\\nclassiﬁers,” in CVPR, 2016.\\n[34] P. Druzhkov and V. Kustikova, “A survey of deep learning methods and\\nsoftware tools for image classiﬁcation and object detection,” Pattern\\nRecognition and Image Anal., vol. 26, no. 1, p. 9, 2016.\\n[35] W. Pitts and W. S. McCulloch, “How we know universals the perception\\nof auditory and visual forms,” The Bulletin of Mathematical Biophysics,\\nvol. 9, no. 3, pp. 127–147, 1947.\\n[36] D. E. Rumelhart, G. E. Hinton, and R. J. Williams, “Learning internal\\nrepresentation by back-propagation of errors,” Nature, vol. 323, no.\\n323, pp. 533–536, 1986.\\n[37] G. E. Hinton and R. R. Salakhutdinov, “Reducing the dimensionality\\nof data with neural networks,” Sci., vol. 313, pp. 504–507, 2006.\\n[38] G. Hinton, L. Deng, D. Yu, G. E. Dahl, A.-r. Mohamed, N. Jaitly,\\nA. Senior, V. Vanhoucke, P. Nguyen, T. N. Sainath et al., “Deep neural\\nnetworks for acoustic modeling in speech recognition: The shared\\nviews of four research groups,” IEEE Signal Process. Mag., vol. 29,\\nno. 6, pp. 82–97, 2012.\\n[39] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, “Imagenet:\\nA large-scale hierarchical image database,” in CVPR, 2009.\\n[40] L. Deng, M. L. Seltzer, D. Yu, A. Acero, A.-r. Mohamed, and\\nG. Hinton, “Binary coding of speech spectrograms using a deep auto-\\nencoder,” in INTERSPEECH, 2010.\\n[41] G. Dahl, A.-r. Mohamed, G. E. Hinton et al., “Phone recognition with\\nthe mean-covariance restricted boltzmann machine,” in NIPS, 2010.\\n[42] G. E. Hinton, N. Srivastava, A. Krizhevsky, I. Sutskever, and\\nR. R. Salakhutdinov, “Improving neural networks by preventing co-\\nadaptation of feature detectors,” arXiv:1207.0580, 2012.\\n[43] S. Ioffe and C. Szegedy, “Batch normalization: Accelerating deep\\nnetwork training by reducing internal covariate shift,” in ICML, 2015.\\n[44] P. Sermanet, D. Eigen, X. Zhang, M. Mathieu, R. Fergus, and Y. LeCun,\\n“Overfeat: Integrated recognition, localization and detection using\\nconvolutional networks,” arXiv:1312.6229, 2013.\\n[45] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov,\\nD. Erhan, V. Vanhoucke, and A. Rabinovich, “Going deeper with\\nconvolutions,” in CVPR, 2015.\\n[46] K. Simonyan and A. Zisserman, “Very deep convolutional networks\\nfor large-scale image recognition,” arXiv:1409.1556, 2014.\\n[47] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image\\nrecognition,” in CVPR, 2016.\\n[48] V. Nair and G. E. Hinton, “Rectiﬁed linear units improve restricted\\nboltzmann machines,” in ICML, 2010.\\n[49] M. Oquab, L. Bottou, I. Laptev, J. Sivic et al., “Weakly supervised\\nobject recognition with convolutional neural networks,” in NIPS, 2014.\\n[50] M. Oquab, L. Bottou, I. Laptev, and J. Sivic, “Learning and transferring\\nmid-level image representations using convolutional neural networks,”\\nin CVPR, 2014.\\n[51] F. M. Wadley, “Probit analysis: a statistical treatment of the sigmoid\\nresponse curve,” Annals of the Entomological Soc. of America, vol. 67,\\nno. 4, pp. 549–553, 1947.\\n[52] K. Kavukcuoglu, R. Fergus, Y. LeCun et al., “Learning invariant\\nfeatures through topographic ﬁlter maps,” in CVPR, 2009.\\n[53] K. Kavukcuoglu, P. Sermanet, Y.-L. Boureau, K. Gregor, M. Mathieu,\\nand Y. LeCun, “Learning convolutional feature hierarchies for visual\\nrecognition,” in NIPS, 2010.\\n[54] M. D. Zeiler, D. Krishnan, G. W. Taylor, and R. Fergus, “Deconvolu-\\ntional networks,” in CVPR, 2010.\\n[55] H. Noh, S. Hong, and B. Han, “Learning deconvolution network for\\nsemantic segmentation,” in ICCV, 2015.\\n[56] Z.-Q. Zhao, B.-J. Xie, Y.-m. Cheung, and X. Wu, “Plant leaf iden-\\ntiﬁcation via a growing convolution neural network with progressive\\nsample learning,” in ACCV, 2014.\\n[57] A. Babenko, A. Slesarev, A. Chigorin, and V. Lempitsky, “Neural codes\\nfor image retrieval,” in ECCV, 2014.\\n[58] J. Wan, D. Wang, S. C. H. Hoi, P. Wu, J. Zhu, Y. Zhang, and J. Li,\\n“Deep learning for content-based image retrieval: A comprehensive\\nstudy,” in ACM MM, 2014.\\n[59] D. Tom`e, F. Monti, L. Barofﬁo, L. Bondi, M. Tagliasacchi, and\\nS. Tubaro, “Deep convolutional neural networks for pedestrian detec-\\ntion,” Signal Process.: Image Commun., vol. 47, pp. 482–489, 2016.\\n[60] Y. Xiang, W. Choi, Y. Lin, and S. Savarese, “Subcategory-aware\\nconvolutional neural networks for object proposals and detection,” in\\nWACV, 2017.\\n[61] Z.-Q. Zhao, H. Bian, D. Hu, W. Cheng, and H. Glotin, “Pedestrian\\ndetection based on fast r-cnn and batch normalization,” in ICIC, 2017.\\n[62] J. Ngiam, A. Khosla, M. Kim, J. Nam, H. Lee, and A. Y. Ng,\\n“Multimodal deep learning,” in ICML, 2011.\\n[63] Z. Wu, X. Wang, Y.-G. Jiang, H. Ye, and X. Xue, “Modeling spatial-\\ntemporal clues in a hybrid deep learning framework for video classiﬁ-\\ncation,” in ACM MM, 2015.\\n[64] K. He, X. Zhang, S. Ren, and J. Sun, “Spatial pyramid pooling in deep\\nconvolutional networks for visual recognition,” IEEE Trans. Pattern\\nAnal. Mach. Intell., vol. 37, no. 9, pp. 1904–1916, 2015.\\n[65] Y. Li, K. He, J. Sun et al., “R-fcn: Object detection via region-based\\nfully convolutional networks,” in NIPS, 2016, pp. 379–387.\\n[66] T.-Y. Lin, P. Doll´ar, R. B. Girshick, K. He, B. Hariharan, and S. J.\\nBelongie, “Feature pyramid networks for object detection,” in CVPR,\\n2017.\\n[67] K. He, G. Gkioxari, P. Doll´ar, and R. B. Girshick, “Mask r-cnn,” in\\nICCV, 2017.\\n[68] D. Erhan, C. Szegedy, A. Toshev, and D. Anguelov, “Scalable object\\ndetection using deep neural networks,” in CVPR, 2014.\\n[69] D. Yoo, S. Park, J.-Y. Lee, A. S. Paek, and I. So Kweon, “Attentionnet:\\nAggregating weak directions for accurate object detection,” in CVPR,\\n2015.\\n[70] M. Najibi, M. Rastegari, and L. S. Davis, “G-cnn: an iterative grid\\nbased object detector,” in CVPR, 2016.\\n[71] W. Liu, D. Anguelov, D. Erhan, C. Szegedy, S. Reed, C.-Y. Fu, and\\nA. C. Berg, “Ssd: Single shot multibox detector,” in ECCV, 2016.\\n[72] J. Redmon and A. Farhadi, “Yolo9000: better, faster, stronger,”\\narXiv:1612.08242, 2016.\\n[73] C. Y. Fu, W. Liu, A. Ranga, A. Tyagi, and A. C. Berg, “Dssd:\\nDeconvolutional single shot detector,” arXiv:1701.06659, 2017.\\n[74] Z. Shen, Z. Liu, J. Li, Y. G. Jiang, Y. Chen, and X. Xue, “Dsod:\\nLearning deeply supervised object detectors from scratch,” in ICCV,\\n2017.\\n[75] G. E. Hinton, A. Krizhevsky, and S. D. Wang, “Transforming auto-\\nencoders,” in ICANN, 2011.\\n[76] G. W. Taylor, I. Spiro, C. Bregler, and R. Fergus, “Learning invariance\\nthrough imitation,” in CVPR, 2011.\\n[77] X. Ren and D. Ramanan, “Histograms of sparse codes for object\\ndetection,” in CVPR, 2013.\\n[78] J. R. Uijlings, K. E. Van De Sande, T. Gevers, and A. W. Smeulders,\\n“Selective search for object recognition,” Int. J. of Comput. Vision, vol.\\n104, no. 2, pp. 154–171, 2013.\\n[79] P. Sermanet, K. Kavukcuoglu, S. Chintala, and Y. LeCun, “Pedestrian\\ndetection with unsupervised multi-stage feature learning,” in CVPR,\\n2013.\\n[80] P. Kr¨ahenb¨uhl and V. Koltun, “Geodesic object proposals,” in ECCV,\\n2014.\\n[81] P. Arbel´aez, J. Pont-Tuset, J. T. Barron, F. Marques, and J. Malik,\\n“Multiscale combinatorial grouping,” in CVPR, 2014.\\n[82] C. L. Zitnick and P. Doll´ar, “Edge boxes: Locating object proposals\\nfrom edges,” in ECCV, 2014.\\n[83] W. Kuo, B. Hariharan, and J. Malik, “Deepbox: Learning objectness\\nwith convolutional networks,” in ICCV, 2015.\\n[84] P. O. Pinheiro, T.-Y. Lin, R. Collobert, and P. Doll´ar, “Learning to\\nreﬁne object segments,” in ECCV, 2016.\\n[85] Y. Zhang, K. Sohn, R. Villegas, G. Pan, and H. Lee, “Improving object\\ndetection with deep convolutional networks via bayesian optimization\\nand structured prediction,” in CVPR, 2015.\\n[86] S. Gupta, R. Girshick, P. Arbel´aez, and J. Malik, “Learning rich features\\nfrom rgb-d images for object detection and segmentation,” in ECCV,\\n2014.\\n[87] W. Ouyang, X. Wang, X. Zeng, S. Qiu, P. Luo, Y. Tian, H. Li, S. Yang,\\nZ. Wang, C.-C. Loy et al., “Deepid-net: Deformable deep convolutional\\nneural networks for object detection,” in CVPR, 2015.\\n[88] K. Lenc and A. Vedaldi, “R-cnn minus r,” arXiv:1506.06981, 2015.\\n[89] S. Lazebnik, C. Schmid, and J. Ponce, “Beyond bags of features:\\nSpatial pyramid matching for recognizing natural scene categories,”\\nin CVPR, 2006.\\n[90] F. Perronnin, J. S´anchez, and T. Mensink, “Improving the ﬁsher kernel\\nfor large-scale image classiﬁcation,” in ECCV, 2010.\\n[91] J. Xue, J. Li, and Y. Gong, “Restructuring of deep neural network\\nacoustic models with singular value decomposition.” in Interspeech,\\n2013.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'source': '../datas/pdf_files/object_detection.pdf', 'file_path': '../datas/pdf_files/object_detection.pdf', 'total_pages': 21, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'trapped': '', 'modDate': 'D:20190417004522Z', 'creationDate': 'D:20190417004522Z', 'page': 18}, page_content='THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION\\n19\\n[92] S. Ren, K. He, R. Girshick, and J. Sun, “Faster r-cnn: Towards real-time\\nobject detection with region proposal networks,” IEEE Trans. Pattern\\nAnal. Mach. Intell., vol. 39, no. 6, pp. 1137–1149, 2017.\\n[93] C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna, “Rethink-\\ning the inception architecture for computer vision,” in CVPR, 2016.\\n[94] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan,\\nP. Doll´ar, and C. L. Zitnick, “Microsoft coco: Common objects in\\ncontext,” in ECCV, 2014.\\n[95] S. Bell, C. Lawrence Zitnick, K. Bala, and R. Girshick, “Inside-outside\\nnet: Detecting objects in context with skip pooling and recurrent neural\\nnetworks,” in CVPR, 2016.\\n[96] A. Arnab and P. H. S. Torr, “Pixelwise instance segmentation with a\\ndynamically instantiated network,” in CVPR, 2017.\\n[97] J. Dai, K. He, and J. Sun, “Instance-aware semantic segmentation via\\nmulti-task network cascades,” in CVPR, 2016.\\n[98] Y. Li, H. Qi, J. Dai, X. Ji, and Y. Wei, “Fully convolutional instance-\\naware semantic segmentation,” in CVPR, 2017.\\n[99] M. Jaderberg, K. Simonyan, A. Zisserman, and K. Kavukcuoglu,\\n“Spatial transformer networks,” in CVPR, 2015.\\n[100] S. Brahmbhatt, H. I. Christensen, and J. Hays, “Stuffnet: Using stuffto\\nimprove object detection,” in WACV, 2017.\\n[101] T. Kong, A. Yao, Y. Chen, and F. Sun, “Hypernet: Towards accurate\\nregion proposal generation and joint object detection,” in CVPR, 2016.\\n[102] A. Pentina, V. Sharmanska, and C. H. Lampert, “Curriculum learning\\nof multiple tasks,” in CVPR, 2015.\\n[103] J. Yim, H. Jung, B. Yoo, C. Choi, D. Park, and J. Kim, “Rotating your\\nface using multi-task deep neural network,” in CVPR, 2015.\\n[104] J. Li, X. Liang, J. Li, T. Xu, J. Feng, and S. Yan, “Multi-stage object\\ndetection with group recursive learning,” arXiv:1608.05159, 2016.\\n[105] Z. Cai, Q. Fan, R. S. Feris, and N. Vasconcelos, “A uniﬁed multi-scale\\ndeep convolutional neural network for fast object detection,” in ECCV,\\n2016.\\n[106] Y. Zhu, R. Urtasun, R. Salakhutdinov, and S. Fidler, “segdeepm:\\nExploiting segmentation and context in deep neural networks for object\\ndetection,” in CVPR, 2015.\\n[107] W. Byeon, T. M. Breuel, F. Raue, and M. Liwicki, “Scene labeling\\nwith lstm recurrent neural networks,” in CVPR, 2015.\\n[108] B. Moysset, C. Kermorvant, and C. Wolf, “Learning to detect and\\nlocalize many objects from few examples,” arXiv:1611.05664, 2016.\\n[109] X. Zeng, W. Ouyang, B. Yang, J. Yan, and X. Wang, “Gated bi-\\ndirectional cnn for object detection,” in ECCV, 2016.\\n[110] S. Gidaris and N. Komodakis, “Object detection via a multi-region and\\nsemantic segmentation-aware cnn model,” in CVPR, 2015.\\n[111] M. Schuster and K. K. Paliwal, “Bidirectional recurrent neural net-\\nworks,” IEEE Trans. Signal Process., vol. 45, pp. 2673–2681, 1997.\\n[112] S. Zagoruyko, A. Lerer, T.-Y. Lin, P. O. Pinheiro, S. Gross, S. Chin-\\ntala, and P. Doll´ar, “A multipath network for object detection,”\\narXiv:1604.02135, 2016.\\n[113] A. Shrivastava, A. Gupta, and R. Girshick, “Training region-based\\nobject detectors with online hard example mining,” in CVPR, 2016.\\n[114] S. Ren, K. He, R. Girshick, X. Zhang, and J. Sun, “Object detection\\nnetworks on convolutional feature maps,” IEEE Trans. Pattern Anal.\\nMach. Intell., vol. 39, no. 7, pp. 1476–1481, 2017.\\n[115] W. Ouyang, X. Wang, C. Zhang, and X. Yang, “Factors in ﬁnetuning\\ndeep model for object detection with long-tail distribution,” in CVPR,\\n2016.\\n[116] S. Hong, B. Roh, K.-H. Kim, Y. Cheon, and M. Park, “Pvanet:\\nLightweight deep neural networks for real-time object detection,”\\narXiv:1611.08588, 2016.\\n[117] W. Shang, K. Sohn, D. Almeida, and H. Lee, “Understanding and\\nimproving convolutional neural networks via concatenated rectiﬁed\\nlinear units,” in ICML, 2016.\\n[118] C. Szegedy, A. Toshev, and D. Erhan, “Deep neural networks for object\\ndetection,” in NIPS, 2013.\\n[119] P. O. Pinheiro, R. Collobert, and P. Doll´ar, “Learning to segment object\\ncandidates,” in NIPS, 2015.\\n[120] C. Szegedy, S. Reed, D. Erhan, D. Anguelov, and S. Ioffe, “Scalable,\\nhigh-quality object detection,” arXiv:1412.1441, 2014.\\n[121] M. Everingham, L. Van Gool, C. Williams, J. Winn, and A. Zisserman,\\n“The pascal visual object classes challenge 2012 (voc2012) results\\n(2012),” in http://www.pascal-network.org/challenges/VOC/voc2011/\\nworkshop/index.html, 2011.\\n[122] M. D. Zeiler and R. Fergus, “Visualizing and understanding convolu-\\ntional networks,” in ECCV, 2014.\\n[123] S. Xie, R. B. Girshick, P. Doll´ar, Z. Tu, and K. He, “Aggregated residual\\ntransformations for deep neural networks,” in CVPR, 2017.\\n[124] J. Dai, H. Qi, Y. Xiong, Y. Li, G. Zhang, H. Hu, and Y. Wei,\\n“Deformable convolutional networks,” arXiv:1703.06211, 2017.\\n[125] C. Rother, L. Bordeaux, Y. Hamadi, and A. Blake, “Autocollage,” ACM\\nTrans. on Graphics, vol. 25, no. 3, pp. 847–852, 2006.\\n[126] C. Jung and C. Kim, “A uniﬁed spectral-domain approach for saliency\\ndetection and its application to automatic object segmentation,” IEEE\\nTrans. Image Process., vol. 21, no. 3, pp. 1272–1283, 2012.\\n[127] W.-C. Tu, S. He, Q. Yang, and S.-Y. Chien, “Real-time salient object\\ndetection with a minimum spanning tree,” in CVPR, 2016.\\n[128] J. Yang and M.-H. Yang, “Top-down visual saliency via joint crf and\\ndictionary learning,” IEEE Trans. Pattern Anal. Mach. Intell., vol. 39,\\nno. 3, pp. 576–588, 2017.\\n[129] P. L. Rosin, “A simple method for detecting salient regions,” Pattern\\nRecognition, vol. 42, no. 11, pp. 2363–2371, 2009.\\n[130] T. Liu, Z. Yuan, J. Sun, J. Wang, N. Zheng, X. Tang, and H.-Y. Shum,\\n“Learning to detect a salient object,” IEEE Trans. Pattern Anal. Mach.\\nIntell., vol. 33, no. 2, pp. 353–367, 2011.\\n[131] J. Long, E. Shelhamer, and T. Darrell, “Fully convolutional networks\\nfor semantic segmentation,” in CVPR, 2015.\\n[132] D. Gao, S. Han, and N. Vasconcelos, “Discriminant saliency, the detec-\\ntion of suspicious coincidences, and applications to visual recognition,”\\nIEEE Trans. Pattern Anal. Mach. Intell., vol. 31, pp. 989–1005, 2009.\\n[133] S. Xie and Z. Tu, “Holistically-nested edge detection,” in ICCV, 2015.\\n[134] M. K¨ummerer, L. Theis, and M. Bethge, “Deep gaze i: Boost-\\ning saliency prediction with feature maps trained on imagenet,”\\narXiv:1411.1045, 2014.\\n[135] X. Huang, C. Shen, X. Boix, and Q. Zhao, “Salicon: Reducing the\\nsemantic gap in saliency prediction by adapting deep neural networks,”\\nin ICCV, 2015.\\n[136] L. Wang, H. Lu, X. Ruan, and M.-H. Yang, “Deep networks for saliency\\ndetection via local estimation and global search,” in CVPR, 2015.\\n[137] H. Cholakkal, J. Johnson, and D. Rajan, “Weakly supervised top-down\\nsalient object detection,” arXiv:1611.05345, 2016.\\n[138] R. Zhao, W. Ouyang, H. Li, and X. Wang, “Saliency detection by\\nmulti-context deep learning,” in CVPR, 2015.\\n[139] C¸ . Bak, A. Erdem, and E. Erdem, “Two-stream convolutional networks\\nfor dynamic saliency prediction,” arXiv:1607.04730, 2016.\\n[140] S. He, R. W. Lau, W. Liu, Z. Huang, and Q. Yang, “Supercnn: A su-\\nperpixelwise convolutional neural network for salient object detection,”\\nInt. J. of Comput. Vision, vol. 115, no. 3, pp. 330–344, 2015.\\n[141] X. Li, L. Zhao, L. Wei, M.-H. Yang, F. Wu, Y. Zhuang, H. Ling, and\\nJ. Wang, “Deepsaliency: Multi-task deep neural network model for\\nsalient object detection,” IEEE Trans. Image Process., vol. 25, no. 8,\\npp. 3919–3930, 2016.\\n[142] Y. Tang and X. Wu, “Saliency detection via combining region-level\\nand pixel-level predictions with cnns,” in ECCV, 2016.\\n[143] G. Li and Y. Yu, “Deep contrast learning for salient object detection,”\\nin CVPR, 2016.\\n[144] X. Wang, H. Ma, S. You, and X. Chen, “Edge preserving and\\nmulti-scale contextual neural network for salient object detection,”\\narXiv:1608.08029, 2016.\\n[145] M. Cornia, L. Baraldi, G. Serra, and R. Cucchiara, “A deep multi-level\\nnetwork for saliency prediction,” in ICPR, 2016.\\n[146] G. Li and Y. Yu, “Visual saliency detection based on multiscale deep\\ncnn features,” IEEE Trans. Image Process., vol. 25, no. 11, pp. 5012–\\n5024, 2016.\\n[147] J. Pan, E. Sayrol, X. Giro-i Nieto, K. McGuinness, and N. E. O’Connor,\\n“Shallow and deep convolutional networks for saliency prediction,” in\\nCVPR, 2016.\\n[148] J. Kuen, Z. Wang, and G. Wang, “Recurrent attentional networks for\\nsaliency detection,” in CVPR, 2016.\\n[149] Y. Tang, X. Wu, and W. Bu, “Deeply-supervised recurrent convolutional\\nneural network for saliency detection,” in ACM MM, 2016.\\n[150] X. Li, Y. Li, C. Shen, A. Dick, and A. Van Den Hengel, “Contextual\\nhypergraph modeling for salient object detection,” in ICCV, 2013.\\n[151] M.-M. Cheng, N. J. Mitra, X. Huang, P. H. Torr, and S.-M. Hu, “Global\\ncontrast based salient region detection,” IEEE Trans. Pattern Anal.\\nMach. Intell., vol. 37, no. 3, pp. 569–582, 2015.\\n[152] H. Jiang, J. Wang, Z. Yuan, Y. Wu, N. Zheng, and S. Li, “Salient object\\ndetection: A discriminative regional feature integration approach,” in\\nCVPR, 2013.\\n[153] G. Lee, Y.-W. Tai, and J. Kim, “Deep saliency with encoded low level\\ndistance map and high level features,” in CVPR, 2016.\\n[154] Z. Luo, A. Mishra, A. Achkar, J. Eichel, S. Li, and P.-M. Jodoin,\\n“Non-local deep features for salient object detection,” in CVPR, 2017.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'source': '../datas/pdf_files/object_detection.pdf', 'file_path': '../datas/pdf_files/object_detection.pdf', 'total_pages': 21, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'trapped': '', 'modDate': 'D:20190417004522Z', 'creationDate': 'D:20190417004522Z', 'page': 19}, page_content='THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION\\n20\\n[155] Q. Hou, M.-M. Cheng, X.-W. Hu, A. Borji, Z. Tu, and P. Torr,\\n“Deeply supervised salient object detection with short connections,”\\narXiv:1611.04849, 2016.\\n[156] Q. Yan, L. Xu, J. Shi, and J. Jia, “Hierarchical saliency detection,” in\\nCVPR, 2013.\\n[157] Y. Li, X. Hou, C. Koch, J. M. Rehg, and A. L. Yuille, “The secrets of\\nsalient object segmentation,” in CVPR, 2014.\\n[158] V. Movahedi and J. H. Elder, “Design and perceptual validation of\\nperformance measures for salient object segmentation,” in CVPRW,\\n2010.\\n[159] A. Borji, M.-M. Cheng, H. Jiang, and J. Li, “Salient object detection:\\nA benchmark,” IEEE Trans. Image Process., vol. 24, no. 12, pp. 5706–\\n5722, 2015.\\n[160] C. Peng, X. Gao, N. Wang, and J. Li, “Graphical representation for\\nheterogeneous face recognition,” IEEE Trans. Pattern Anal. Mach.\\nIntell., vol. 39, no. 2, pp. 301–312, 2015.\\n[161] C. Peng, N. Wang, X. Gao, and J. Li, “Face recognition from multiple\\nstylistic sketches: Scenarios, datasets, and evaluation,” in ECCV, 2016.\\n[162] X. Gao, N. Wang, D. Tao, and X. Li, “Face sketchcphoto synthesis\\nand retrieval using sparse representation,” IEEE Trans. Circuits Syst.\\nVideo Technol., vol. 22, no. 8, pp. 1213–1226, 2012.\\n[163] N. Wang, D. Tao, X. Gao, X. Li, and J. Li, “A comprehensive survey\\nto face hallucination,” Int. J. of Comput. Vision, vol. 106, no. 1, pp.\\n9–30, 2014.\\n[164] C. Peng, X. Gao, N. Wang, D. Tao, X. Li, and J. Li, “Multiple\\nrepresentations-based face sketch-photo synthesis.” IEEE Trans. Neural\\nNetw. & Learning Syst., vol. 27, no. 11, pp. 2201–2215, 2016.\\n[165] A. Majumder, L. Behera, and V. K. Subramanian, “Automatic facial\\nexpression recognition system using deep network-based data fusion,”\\nIEEE Trans. Cybern., vol. 48, pp. 103–114, 2018.\\n[166] P. Viola and M. Jones, “Robust real-time face detection,” Int. J. of\\nComput. Vision, vol. 57, no. 2, pp. 137–154, 2004.\\n[167] J. Yu, Y. Jiang, Z. Wang, Z. Cao, and T. Huang, “Unitbox: An advanced\\nobject detection network,” in ACM MM, 2016.\\n[168] S. S. Farfade, M. J. Saberian, and L.-J. Li, “Multi-view face detection\\nusing deep convolutional neural networks,” in ICMR, 2015.\\n[169] S. Yang, P. Luo, C.-C. Loy, and X. Tang, “From facial parts responses\\nto face detection: A deep learning approach,” in ICCV, 2015.\\n[170] S. Yang, Y. Xiong, C. C. Loy, and X. Tang, “Face detection through\\nscale-friendly deep convolutional networks,” in CVPR, 2017.\\n[171] Z. Hao, Y. Liu, H. Qin, J. Yan, X. Li, and X. Hu, “Scale-aware face\\ndetection,” in CVPR, 2017.\\n[172] H. Wang, Z. Li, X. Ji, and Y. Wang, “Face r-cnn,” arXiv:1706.01061,\\n2017.\\n[173] X. Sun, P. Wu, and S. C. Hoi, “Face detection using deep learning: An\\nimproved faster rcnn approach,” arXiv:1701.08289, 2017.\\n[174] L. Huang, Y. Yang, Y. Deng, and Y. Yu, “Densebox: Unifying landmark\\nlocalization with end to end object detection,” arXiv:1509.04874, 2015.\\n[175] Y. Li, B. Sun, T. Wu, and Y. Wang, “face detection with end-to-end\\nintegration of a convnet and a 3d model,” in ECCV, 2016.\\n[176] K. Zhang, Z. Zhang, Z. Li, and Y. Qiao, “Joint face detection and\\nalignment using multitask cascaded convolutional networks,” IEEE\\nSignal Process. Lett., vol. 23, no. 10, pp. 1499–1503, 2016.\\n[177] I. A. Kalinovsky and V. G. Spitsyn, “Compact convolutional neural\\nnetwork cascadefor face detection,” in CEUR Workshop, 2016.\\n[178] H. Qin, J. Yan, X. Li, and X. Hu, “Joint training of cascaded cnn for\\nface detection,” in CVPR, 2016.\\n[179] V. Jain and E. Learned-Miller, “Fddb: A benchmark for face detection\\nin unconstrained settings,” Tech. Rep., 2010.\\n[180] H. Li, Z. Lin, X. Shen, J. Brandt, and G. Hua, “A convolutional neural\\nnetwork cascade for face detection,” in CVPR, 2015.\\n[181] B. Yang, J. Yan, Z. Lei, and S. Z. Li, “Aggregate channel features for\\nmulti-view face detection,” in IJCB, 2014.\\n[182] N. Markuˇs, M. Frljak, I. S. Pandˇzi´c, J. Ahlberg, and R. Forchheimer,\\n“Object detection with pixel intensity comparisons organized in deci-\\nsion trees,” arXiv:1305.4537, 2013.\\n[183] M. Mathias, R. Benenson, M. Pedersoli, and L. Van Gool, “Face\\ndetection without bells and whistles,” in ECCV, 2014.\\n[184] J. Li and Y. Zhang, “Learning surf cascade for fast and accurate object\\ndetection,” in CVPR, 2013.\\n[185] S. Liao, A. K. Jain, and S. Z. Li, “A fast and accurate unconstrained\\nface detector,” IEEE Trans. Pattern Anal. Mach. Intell., vol. 38, no. 2,\\npp. 211–223, 2016.\\n[186] B. Yang, J. Yan, Z. Lei, and S. Z. Li, “Convolutional channel features,”\\nin ICCV, 2015.\\n[187] R. Ranjan, V. M. Patel, and R. Chellappa, “Hyperface: A deep multi-\\ntask learning framework for face detection, landmark localization, pose\\nestimation, and gender recognition,” arXiv:1603.01249, 2016.\\n[188] P. Hu and D. Ramanan, “Finding tiny faces,” in CVPR, 2017.\\n[189] Z. Jiang and D. Q. Huynh, “Multiple pedestrian tracking from monoc-\\nular videos in an interacting multiple model framework,” IEEE Trans.\\nImage Process., vol. 27, pp. 1361–1375, 2018.\\n[190] D. Gavrila and S. Munder, “Multi-cue pedestrian detection and tracking\\nfrom a moving vehicle,” Int. J. of Comput. Vision, vol. 73, pp. 41–59,\\n2006.\\n[191] S. Xu, Y. Cheng, K. Gu, Y. Yang, S. Chang, and P. Zhou, “Jointly\\nattentive spatial-temporal pooling networks for video-based person re-\\nidentiﬁcation,” in ICCV, 2017.\\n[192] Z. Liu, D. Wang, and H. Lu, “Stepwise metric promotion for unsuper-\\nvised video person re-identiﬁcation,” in ICCV, 2017.\\n[193] A. Khan, B. Rinner, and A. Cavallaro, “Cooperative robots to observe\\nmoving targets: Review,” IEEE Trans. Cybern., vol. 48, pp. 187–198,\\n2018.\\n[194] A. Geiger, P. Lenz, C. Stiller, and R. Urtasun, “Vision meets robotics:\\nThe kitti dataset,” Int. J. of Robotics Res., vol. 32, pp. 1231–1237,\\n2013.\\n[195] Z. Cai, M. Saberian, and N. Vasconcelos, “Learning complexity-aware\\ncascades for deep pedestrian detection,” in ICCV, 2015.\\n[196] Y. Tian, P. Luo, X. Wang, and X. Tang, “Deep learning strong parts\\nfor pedestrian detection,” in CVPR, 2015.\\n[197] P. Doll´ar, R. Appel, S. Belongie, and P. Perona, “Fast feature pyramids\\nfor object detection,” IEEE Trans. Pattern Anal. Mach. Intell., vol. 36,\\nno. 8, pp. 1532–1545, 2014.\\n[198] S. Zhang, R. Benenson, and B. Schiele, “Filtered channel features for\\npedestrian detection,” in CVPR, 2015.\\n[199] S. Paisitkriangkrai, C. Shen, and A. van den Hengel, “Pedestrian detec-\\ntion with spatially pooled features and structured ensemble learning,”\\nIEEE Trans. Pattern Anal. Mach. Intell., vol. 38, pp. 1243–1257, 2016.\\n[200] L. Lin, X. Wang, W. Yang, and J.-H. Lai, “Discriminatively trained\\nand-or graph models for object shape detection,” IEEE Trans. Pattern\\nAnal. Mach. Intell., vol. 37, no. 5, pp. 959–972, 2015.\\n[201] M. Mathias, R. Benenson, R. Timofte, and L. Van Gool, “Handling\\nocclusions with franken-classiﬁers,” in ICCV, 2013.\\n[202] S. Tang, M. Andriluka, and B. Schiele, “Detection and tracking of\\noccluded people,” Int. J. of Comput. Vision, vol. 110, pp. 58–69, 2014.\\n[203] L. Zhang, L. Lin, X. Liang, and K. He, “Is faster r-cnn doing well for\\npedestrian detection?” in ECCV, 2016.\\n[204] Y. Tian, P. Luo, X. Wang, and X. Tang, “Deep learning strong parts\\nfor pedestrian detection,” in ICCV, 2015.\\n[205] J. Liu, S. Zhang, S. Wang, and D. N. Metaxas, “Multispectral deep\\nneural networks for pedestrian detection,” arXiv:1611.02644, 2016.\\n[206] Y. Tian, P. Luo, X. Wang, and X. Tang, “Pedestrian detection aided by\\ndeep learning semantic tasks,” in CVPR, 2015.\\n[207] X. Du, M. El-Khamy, J. Lee, and L. Davis, “Fused dnn: A deep neural\\nnetwork fusion approach to fast and robust pedestrian detection,” in\\nWACV, 2017.\\n[208] Q. Hu, P. Wang, C. Shen, A. van den Hengel, and F. Porikli, “Pushing\\nthe limits of deep cnns for pedestrian detection,” IEEE Trans. Circuits\\nSyst. Video Technol., 2017.\\n[209] D. Tom´e, L. Bondi, L. Barofﬁo, S. Tubaro, E. Plebani, and D. Pau,\\n“Reduced memory region based deep convolutional neural network\\ndetection,” in ICCE-Berlin, 2016.\\n[210] J. Hosang, M. Omran, R. Benenson, and B. Schiele, “Taking a deeper\\nlook at pedestrians,” in CVPR, 2015.\\n[211] J. Li, X. Liang, S. Shen, T. Xu, J. Feng, and S. Yan, “Scale-aware fast\\nr-cnn for pedestrian detection,” arXiv:1510.08160, 2015.\\n[212] Y. Gao, M. Wang, Z.-J. Zha, J. Shen, X. Li, and X. Wu, “Visual-textual\\njoint relevance learning for tag-based social image search,” IEEE Trans.\\nImage Process., vol. 22, no. 1, pp. 363–376, 2013.\\n[213] T. Kong, F. Sun, A. Yao, H. Liu, M. Lv, and Y. Chen, “Ron: Reverse\\nconnection with objectness prior networks for object detection,” in\\nCVPR, 2017.\\n[214] I. J. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley,\\nS. Ozair, A. C. Courville, and Y. Bengio, “Generative adversarial nets,”\\nin NIPS, 2014.\\n[215] Y. Fang, K. Kuan, J. Lin, C. Tan, and V. Chandrasekhar, “Object\\ndetection meets knowledge graphs,” in IJCAI, 2017.\\n[216] S. Welleck, J. Mao, K. Cho, and Z. Zhang, “Saliency-based sequential\\nimage attention with multiset prediction,” in NIPS, 2017.\\n[217] S. Azadi, J. Feng, and T. Darrell, “Learning detection with diverse\\nproposals,” in CVPR, 2017.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'source': '../datas/pdf_files/object_detection.pdf', 'file_path': '../datas/pdf_files/object_detection.pdf', 'total_pages': 21, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'trapped': '', 'modDate': 'D:20190417004522Z', 'creationDate': 'D:20190417004522Z', 'page': 20}, page_content='THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION\\n21\\n[218] S. Sukhbaatar, A. Szlam, J. Weston, and R. Fergus, “End-to-end\\nmemory networks,” in NIPS, 2015.\\n[219] P. Dabkowski and Y. Gal, “Real time image saliency for black box\\nclassiﬁers,” in NIPS, 2017.\\n[220] B. Yang, J. Yan, Z. Lei, and S. Z. Li, “Craft objects from images,” in\\nCVPR, 2016.\\n[221] I. Croitoru, S.-V. Bogolin, and M. Leordeanu, “Unsupervised learning\\nfrom video to detect foreground objects in single images,” in ICCV,\\n2017.\\n[222] C. Wang, W. Ren, K. Huang, and T. Tan, “Weakly supervised object\\nlocalization with latent category learning,” in ECCV, 2014.\\n[223] D. P. Papadopoulos, J. R. R. Uijlings, F. Keller, and V. Ferrari,\\n“Training object class detectors with click supervision,” in CVPR, 2017.\\n[224] J. Huang, V. Rathod, C. Sun, M. Zhu, A. K. Balan, A. Fathi, I. Fischer,\\nZ. Wojna, Y. S. Song, S. Guadarrama, and K. Murphy, “Speed/accuracy\\ntrade-offs for modern convolutional object detectors,” in CVPR, 2017.\\n[225] Q. Li, S. Jin, and J. Yan, “Mimicking very efﬁcient network for object\\ndetection,” in CVPR, 2017.\\n[226] G. Hinton, O. Vinyals, and J. Dean, “Distilling the knowledge in a\\nneural network,” Comput. Sci., vol. 14, no. 7, pp. 38–39, 2015.\\n[227] A. Romero, N. Ballas, S. E. Kahou, A. Chassang, C. Gatta, and\\nY. Bengio, “Fitnets: Hints for thin deep nets,” Comput. Sci., 2014.\\n[228] X. Chen, K. Kundu, Y. Zhu, A. G. Berneshawi, H. Ma, S. Fidler, and\\nR. Urtasun, “3d object proposals for accurate object class detection,”\\nin NIPS, 2015.\\n[229] J. Dong, X. Fei, and S. Soatto, “Visual-inertial-semantic scene repre-\\nsentation for 3d object detection,” in CVPR, 2017.\\n[230] K. Kang, H. Li, T. Xiao, W. Ouyang, J. Yan, X. Liu, and X. Wang,\\n“Object detection in videos with tubelet proposal networks,” in CVPR,\\n2017.\\nZhong-Qiu Zhao is a professor at Hefei Univer-\\nsity of Technology, China. He obtained the Ph.D.\\ndegree in Pattern Recognition & Intelligent System\\nat University of Science and Technology, China, in\\n2007. From April 2008 to November 2009, he held a\\npostdoctoral position in image processing in CNRS\\nUMR6168 Lab Sciences de lInformation et des\\nSyst`emes, France. From January 2013 to December\\n2014, he held a research fellow position in image\\nprocessing at the Department of Computer Science\\nof Hongkong Baptist University, Hongkong, China.\\nHis research is about pattern recognition, image processing, and computer\\nvision.\\nPeng Zheng is a Ph.D. candidate at Hefei Uni-\\nversity of Technology since 2010. He received his\\nBachelor’s degree in 2010 from Hefei University of\\nTechnology. His interests cover pattern recognition,\\nimage processing and computer vision.\\nShou-tao Xu is a Master student at Hefei University\\nof Technology. His research interests cover pattern\\nrecognition, image processing, deep learning and\\ncomputer vision.\\nXindong Wu is an Alfred and Helen Lamson En-\\ndowed Professor in Computer Science, University\\nof Louisiana at Lafayette (USA), and a Fellow of\\nthe IEEE and the AAAS. He received his Ph.D.\\ndegree in Artiﬁcial Intelligence from the University\\nof Edinburgh, Britain. His research interests include\\ndata mining, knowledge-based systems, and Web in-\\nformation exploration. He is the Steering Committee\\nChair of the IEEE International Conference on Data\\nMining (ICDM), the Editor-in-Chief of Knowledge\\nand Information Systems (KAIS, by Springer), and\\na Series Editor of the Springer Book Series on Advanced Information and\\nKnowledge Processing (AI&KP). He was the Editor-in-Chief of the IEEE\\nTransactions on Knowledge and Data Engineering (TKDE, by the IEEE\\nComputer Society) between 2005 and 2008.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'source': '../datas/pdf_files/attention.pdf', 'file_path': '../datas/pdf_files/attention.pdf', 'total_pages': 15, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'trapped': '', 'modDate': 'D:20240410211143Z', 'creationDate': 'D:20240410211143Z', 'page': 0}, page_content='Provided proper attribution is provided, Google hereby grants permission to\\nreproduce the tables and figures in this paper solely for use in journalistic or\\nscholarly works.\\nAttention Is All You Need\\nAshish Vaswani∗\\nGoogle Brain\\navaswani@google.com\\nNoam Shazeer∗\\nGoogle Brain\\nnoam@google.com\\nNiki Parmar∗\\nGoogle Research\\nnikip@google.com\\nJakob Uszkoreit∗\\nGoogle Research\\nusz@google.com\\nLlion Jones∗\\nGoogle Research\\nllion@google.com\\nAidan N. Gomez∗†\\nUniversity of Toronto\\naidan@cs.toronto.edu\\nŁukasz Kaiser∗\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin∗‡\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature. We show that the Transformer generalizes well to\\nother tasks by applying it successfully to English constituency parsing both with\\nlarge and limited training data.\\n∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\n†Work performed while at Google Brain.\\n‡Work performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\\narXiv:1706.03762v7  [cs.CL]  2 Aug 2023'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'source': '../datas/pdf_files/attention.pdf', 'file_path': '../datas/pdf_files/attention.pdf', 'total_pages': 15, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'trapped': '', 'modDate': 'D:20240410211143Z', 'creationDate': 'D:20240410211143Z', 'page': 1}, page_content='1\\nIntroduction\\nRecurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks\\nin particular, have been firmly established as state of the art approaches in sequence modeling and\\ntransduction problems such as language modeling and machine translation [35, 2, 5]. Numerous\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\\narchitectures [38, 24, 15].\\nRecurrent models typically factor computation along the symbol positions of the input and output\\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\\nstates ht, as a function of the previous hidden state ht−1 and the input for position t. This inherently\\nsequential nature precludes parallelization within training examples, which becomes critical at longer\\nsequence lengths, as memory constraints limit batching across examples. Recent work has achieved\\nsignificant improvements in computational efficiency through factorization tricks [21] and conditional\\ncomputation [32], while also improving model performance in case of the latter. The fundamental\\nconstraint of sequential computation, however, remains.\\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-\\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in\\nthe input or output sequences [2, 19]. In all but a few cases [27], however, such attention mechanisms\\nare used in conjunction with a recurrent network.\\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead\\nrelying entirely on an attention mechanism to draw global dependencies between input and output.\\nThe Transformer allows for significantly more parallelization and can reach a new state of the art in\\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.\\n2\\nBackground\\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\\n[16], ByteNet [18] and ConvS2S [9], all of which use convolutional neural networks as basic building\\nblock, computing hidden representations in parallel for all input and output positions. In these models,\\nthe number of operations required to relate signals from two arbitrary input or output positions grows\\nin the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\\nit more difficult to learn dependencies between distant positions [12]. In the Transformer this is\\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\\ndescribed in section 3.2.\\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\\nof a single sequence in order to compute a representation of the sequence. Self-attention has been\\nused successfully in a variety of tasks including reading comprehension, abstractive summarization,\\ntextual entailment and learning task-independent sentence representations [4, 27, 28, 22].\\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\\naligned recurrence and have been shown to perform well on simple-language question answering and\\nlanguage modeling tasks [34].\\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\\nentirely on self-attention to compute representations of its input and output without using sequence-\\naligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\\nself-attention and discuss its advantages over models such as [17, 18] and [9].\\n3\\nModel Architecture\\nMost competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35].\\nHere, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequence\\nof continuous representations z = (z1, ..., zn). Given z, the decoder then generates an output\\nsequence (y1, ..., ym) of symbols one element at a time. At each step the model is auto-regressive\\n[10], consuming the previously generated symbols as additional input when generating the next.\\n2'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'source': '../datas/pdf_files/attention.pdf', 'file_path': '../datas/pdf_files/attention.pdf', 'total_pages': 15, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'trapped': '', 'modDate': 'D:20240410211143Z', 'creationDate': 'D:20240410211143Z', 'page': 2}, page_content='Figure 1: The Transformer - model architecture.\\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\\nrespectively.\\n3.1\\nEncoder and Decoder Stacks\\nEncoder:\\nThe encoder is composed of a stack of N = 6 identical layers. Each layer has two\\nsub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-\\nwise fully connected feed-forward network. We employ a residual connection [11] around each of\\nthe two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is\\nLayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer\\nitself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\\nlayers, produce outputs of dimension dmodel = 512.\\nDecoder:\\nThe decoder is also composed of a stack of N = 6 identical layers. In addition to the two\\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\\nattention over the output of the encoder stack. Similar to the encoder, we employ residual connections\\naround each of the sub-layers, followed by layer normalization. We also modify the self-attention\\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\\nmasking, combined with fact that the output embeddings are offset by one position, ensures that the\\npredictions for position i can depend only on the known outputs at positions less than i.\\n3.2\\nAttention\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\\n3'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'source': '../datas/pdf_files/attention.pdf', 'file_path': '../datas/pdf_files/attention.pdf', 'total_pages': 15, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'trapped': '', 'modDate': 'D:20240410211143Z', 'creationDate': 'D:20240410211143Z', 'page': 3}, page_content='Scaled Dot-Product Attention\\nMulti-Head Attention\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\\nattention layers running in parallel.\\nof the values, where the weight assigned to each value is computed by a compatibility function of the\\nquery with the corresponding key.\\n3.2.1\\nScaled Dot-Product Attention\\nWe call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of\\nqueries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\\nquery with all keys, divide each by √dk, and apply a softmax function to obtain the weights on the\\nvalues.\\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\\ninto a matrix Q. The keys and values are also packed together into matrices K and V . We compute\\nthe matrix of outputs as:\\nAttention(Q, K, V ) = softmax(QKT\\n√dk\\n)V\\n(1)\\nThe two most commonly used attention functions are additive attention [2], and dot-product (multi-\\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\\nof\\n1\\n√dk . Additive attention computes the compatibility function using a feed-forward network with\\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\\nmuch faster and more space-efficient in practice, since it can be implemented using highly optimized\\nmatrix multiplication code.\\nWhile for small values of dk the two mechanisms perform similarly, additive attention outperforms\\ndot product attention without scaling for larger values of dk [3]. We suspect that for large values of\\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\\nextremely small gradients 4. To counteract this effect, we scale the dot products by\\n1\\n√dk .\\n3.2.2\\nMulti-Head Attention\\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\\nwe found it beneficial to linearly project the queries, keys and values h times with different, learned\\nlinear projections to dk, dk and dv dimensions, respectively. On each of these projected versions of\\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\\n4To illustrate why the dot products get large, assume that the components of q and k are independent random\\nvariables with mean 0 and variance 1. Then their dot product, q · k = Pdk\\ni=1 qiki, has mean 0 and variance dk.\\n4'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'source': '../datas/pdf_files/attention.pdf', 'file_path': '../datas/pdf_files/attention.pdf', 'total_pages': 15, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'trapped': '', 'modDate': 'D:20240410211143Z', 'creationDate': 'D:20240410211143Z', 'page': 4}, page_content='output values. These are concatenated and once again projected, resulting in the final values, as\\ndepicted in Figure 2.\\nMulti-head attention allows the model to jointly attend to information from different representation\\nsubspaces at different positions. With a single attention head, averaging inhibits this.\\nMultiHead(Q, K, V ) = Concat(head1, ..., headh)W O\\nwhere headi = Attention(QW Q\\ni , KW K\\ni , V W V\\ni )\\nWhere the projections are parameter matrices W Q\\ni\\n∈Rdmodel×dk, W K\\ni\\n∈Rdmodel×dk, W V\\ni\\n∈Rdmodel×dv\\nand W O ∈Rhdv×dmodel.\\nIn this work we employ h = 8 parallel attention layers, or heads. For each of these we use\\ndk = dv = dmodel/h = 64. Due to the reduced dimension of each head, the total computational cost\\nis similar to that of single-head attention with full dimensionality.\\n3.2.3\\nApplications of Attention in our Model\\nThe Transformer uses multi-head attention in three different ways:\\n• In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\\nand the memory keys and values come from the output of the encoder. This allows every\\nposition in the decoder to attend over all positions in the input sequence. This mimics the\\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\\n[38, 2, 9].\\n• The encoder contains self-attention layers. In a self-attention layer all of the keys, values\\nand queries come from the same place, in this case, the output of the previous layer in the\\nencoder. Each position in the encoder can attend to all positions in the previous layer of the\\nencoder.\\n• Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\\nall positions in the decoder up to and including that position. We need to prevent leftward\\ninformation flow in the decoder to preserve the auto-regressive property. We implement this\\ninside of scaled dot-product attention by masking out (setting to −∞) all values in the input\\nof the softmax which correspond to illegal connections. See Figure 2.\\n3.3\\nPosition-wise Feed-Forward Networks\\nIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\\nconnected feed-forward network, which is applied to each position separately and identically. This\\nconsists of two linear transformations with a ReLU activation in between.\\nFFN(x) = max(0, xW1 + b1)W2 + b2\\n(2)\\nWhile the linear transformations are the same across different positions, they use different parameters\\nfrom layer to layer. Another way of describing this is as two convolutions with kernel size 1.\\nThe dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality\\ndff = 2048.\\n3.4\\nEmbeddings and Softmax\\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input\\ntokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-\\nmation and softmax function to convert the decoder output to predicted next-token probabilities. In\\nour model, we share the same weight matrix between the two embedding layers and the pre-softmax\\nlinear transformation, similar to [30]. In the embedding layers, we multiply those weights by √dmodel.\\n5'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'source': '../datas/pdf_files/attention.pdf', 'file_path': '../datas/pdf_files/attention.pdf', 'total_pages': 15, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'trapped': '', 'modDate': 'D:20240410211143Z', 'creationDate': 'D:20240410211143Z', 'page': 5}, page_content='Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\\nfor different layer types. n is the sequence length, d is the representation dimension, k is the kernel\\nsize of convolutions and r the size of the neighborhood in restricted self-attention.\\nLayer Type\\nComplexity per Layer\\nSequential\\nMaximum Path Length\\nOperations\\nSelf-Attention\\nO(n2 · d)\\nO(1)\\nO(1)\\nRecurrent\\nO(n · d2)\\nO(n)\\nO(n)\\nConvolutional\\nO(k · n · d2)\\nO(1)\\nO(logk(n))\\nSelf-Attention (restricted)\\nO(r · n · d)\\nO(1)\\nO(n/r)\\n3.5\\nPositional Encoding\\nSince our model contains no recurrence and no convolution, in order for the model to make use of the\\norder of the sequence, we must inject some information about the relative or absolute position of the\\ntokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the\\nbottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\\nas the embeddings, so that the two can be summed. There are many choices of positional encodings,\\nlearned and fixed [9].\\nIn this work, we use sine and cosine functions of different frequencies:\\nPE(pos,2i) = sin(pos/100002i/dmodel)\\nPE(pos,2i+1) = cos(pos/100002i/dmodel)\\nwhere pos is the position and i is the dimension. That is, each dimension of the positional encoding\\ncorresponds to a sinusoid. The wavelengths form a geometric progression from 2π to 10000 · 2π. We\\nchose this function because we hypothesized it would allow the model to easily learn to attend by\\nrelative positions, since for any fixed offset k, PEpos+k can be represented as a linear function of\\nPEpos.\\nWe also experimented with using learned positional embeddings [9] instead, and found that the two\\nversions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\\nbecause it may allow the model to extrapolate to sequence lengths longer than the ones encountered\\nduring training.\\n4\\nWhy Self-Attention\\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu-\\ntional layers commonly used for mapping one variable-length sequence of symbol representations\\n(x1, ..., xn) to another sequence of equal length (z1, ..., zn), with xi, zi ∈Rd, such as a hidden\\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\\nconsider three desiderata.\\nOne is the total computational complexity per layer. Another is the amount of computation that can\\nbe parallelized, as measured by the minimum number of sequential operations required.\\nThe third is the path length between long-range dependencies in the network. Learning long-range\\ndependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\\nability to learn such dependencies is the length of the paths forward and backward signals have to\\ntraverse in the network. The shorter these paths between any combination of positions in the input\\nand output sequences, the easier it is to learn long-range dependencies [12]. Hence we also compare\\nthe maximum path length between any two input and output positions in networks composed of the\\ndifferent layer types.\\nAs noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially\\nexecuted operations, whereas a recurrent layer requires O(n) sequential operations. In terms of\\ncomputational complexity, self-attention layers are faster than recurrent layers when the sequence\\n6'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'source': '../datas/pdf_files/attention.pdf', 'file_path': '../datas/pdf_files/attention.pdf', 'total_pages': 15, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'trapped': '', 'modDate': 'D:20240410211143Z', 'creationDate': 'D:20240410211143Z', 'page': 6}, page_content='length n is smaller than the representation dimensionality d, which is most often the case with\\nsentence representations used by state-of-the-art models in machine translations, such as word-piece\\n[38] and byte-pair [31] representations. To improve computational performance for tasks involving\\nvery long sequences, self-attention could be restricted to considering only a neighborhood of size r in\\nthe input sequence centered around the respective output position. This would increase the maximum\\npath length to O(n/r). We plan to investigate this approach further in future work.\\nA single convolutional layer with kernel width k < n does not connect all pairs of input and output\\npositions. Doing so requires a stack of O(n/k) convolutional layers in the case of contiguous kernels,\\nor O(logk(n)) in the case of dilated convolutions [18], increasing the length of the longest paths\\nbetween any two positions in the network. Convolutional layers are generally more expensive than\\nrecurrent layers, by a factor of k. Separable convolutions [6], however, decrease the complexity\\nconsiderably, to O(k · n · d + n · d2). Even with k = n, however, the complexity of a separable\\nconvolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,\\nthe approach we take in our model.\\nAs side benefit, self-attention could yield more interpretable models. We inspect attention distributions\\nfrom our models and present and discuss examples in the appendix. Not only do individual attention\\nheads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic\\nand semantic structure of the sentences.\\n5\\nTraining\\nThis section describes the training regime for our models.\\n5.1\\nTraining Data and Batching\\nWe trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million\\nsentence pairs. Sentences were encoded using byte-pair encoding [3], which has a shared source-\\ntarget vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT\\n2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece\\nvocabulary [38]. Sentence pairs were batched together by approximate sequence length. Each training\\nbatch contained a set of sentence pairs containing approximately 25000 source tokens and 25000\\ntarget tokens.\\n5.2\\nHardware and Schedule\\nWe trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using\\nthe hyperparameters described throughout the paper, each training step took about 0.4 seconds. We\\ntrained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the\\nbottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps\\n(3.5 days).\\n5.3\\nOptimizer\\nWe used the Adam optimizer [20] with β1 = 0.9, β2 = 0.98 and ϵ = 10−9. We varied the learning\\nrate over the course of training, according to the formula:\\nlrate = d−0.5\\nmodel · min(step_num−0.5, step_num · warmup_steps−1.5)\\n(3)\\nThis corresponds to increasing the learning rate linearly for the first warmup_steps training steps,\\nand decreasing it thereafter proportionally to the inverse square root of the step number. We used\\nwarmup_steps = 4000.\\n5.4\\nRegularization\\nWe employ three types of regularization during training:\\n7'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'source': '../datas/pdf_files/attention.pdf', 'file_path': '../datas/pdf_files/attention.pdf', 'total_pages': 15, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'trapped': '', 'modDate': 'D:20240410211143Z', 'creationDate': 'D:20240410211143Z', 'page': 7}, page_content='Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the\\nEnglish-to-German and English-to-French newstest2014 tests at a fraction of the training cost.\\nModel\\nBLEU\\nTraining Cost (FLOPs)\\nEN-DE\\nEN-FR\\nEN-DE\\nEN-FR\\nByteNet [18]\\n23.75\\nDeep-Att + PosUnk [39]\\n39.2\\n1.0 · 1020\\nGNMT + RL [38]\\n24.6\\n39.92\\n2.3 · 1019\\n1.4 · 1020\\nConvS2S [9]\\n25.16\\n40.46\\n9.6 · 1018\\n1.5 · 1020\\nMoE [32]\\n26.03\\n40.56\\n2.0 · 1019\\n1.2 · 1020\\nDeep-Att + PosUnk Ensemble [39]\\n40.4\\n8.0 · 1020\\nGNMT + RL Ensemble [38]\\n26.30\\n41.16\\n1.8 · 1020\\n1.1 · 1021\\nConvS2S Ensemble [9]\\n26.36\\n41.29\\n7.7 · 1019\\n1.2 · 1021\\nTransformer (base model)\\n27.3\\n38.1\\n3.3 · 1018\\nTransformer (big)\\n28.4\\n41.8\\n2.3 · 1019\\nResidual Dropout\\nWe apply dropout [33] to the output of each sub-layer, before it is added to the\\nsub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the\\npositional encodings in both the encoder and decoder stacks. For the base model, we use a rate of\\nPdrop = 0.1.\\nLabel Smoothing\\nDuring training, we employed label smoothing of value ϵls = 0.1 [36]. This\\nhurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\\n6\\nResults\\n6.1\\nMachine Translation\\nOn the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)\\nin Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0\\nBLEU, establishing a new state-of-the-art BLEU score of 28.4. The configuration of this model is\\nlisted in the bottom line of Table 3. Training took 3.5 days on 8 P100 GPUs. Even our base model\\nsurpasses all previously published models and ensembles, at a fraction of the training cost of any of\\nthe competitive models.\\nOn the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0,\\noutperforming all of the previously published single models, at less than 1/4 the training cost of the\\nprevious state-of-the-art model. The Transformer (big) model trained for English-to-French used\\ndropout rate Pdrop = 0.1, instead of 0.3.\\nFor the base models, we used a single model obtained by averaging the last 5 checkpoints, which\\nwere written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We\\nused beam search with a beam size of 4 and length penalty α = 0.6 [38]. These hyperparameters\\nwere chosen after experimentation on the development set. We set the maximum output length during\\ninference to input length + 50, but terminate early when possible [38].\\nTable 2 summarizes our results and compares our translation quality and training costs to other model\\narchitectures from the literature. We estimate the number of floating point operations used to train a\\nmodel by multiplying the training time, the number of GPUs used, and an estimate of the sustained\\nsingle-precision floating-point capacity of each GPU 5.\\n6.2\\nModel Variations\\nTo evaluate the importance of different components of the Transformer, we varied our base model\\nin different ways, measuring the change in performance on English-to-German translation on the\\n5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.\\n8'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'source': '../datas/pdf_files/attention.pdf', 'file_path': '../datas/pdf_files/attention.pdf', 'total_pages': 15, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'trapped': '', 'modDate': 'D:20240410211143Z', 'creationDate': 'D:20240410211143Z', 'page': 8}, page_content='Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\\nper-word perplexities.\\nN\\ndmodel\\ndff\\nh\\ndk\\ndv\\nPdrop\\nϵls\\ntrain\\nPPL\\nBLEU\\nparams\\nsteps\\n(dev)\\n(dev)\\n×106\\nbase\\n6\\n512\\n2048\\n8\\n64\\n64\\n0.1\\n0.1\\n100K\\n4.92\\n25.8\\n65\\n(A)\\n1\\n512\\n512\\n5.29\\n24.9\\n4\\n128\\n128\\n5.00\\n25.5\\n16\\n32\\n32\\n4.91\\n25.8\\n32\\n16\\n16\\n5.01\\n25.4\\n(B)\\n16\\n5.16\\n25.1\\n58\\n32\\n5.01\\n25.4\\n60\\n(C)\\n2\\n6.11\\n23.7\\n36\\n4\\n5.19\\n25.3\\n50\\n8\\n4.88\\n25.5\\n80\\n256\\n32\\n32\\n5.75\\n24.5\\n28\\n1024\\n128\\n128\\n4.66\\n26.0\\n168\\n1024\\n5.12\\n25.4\\n53\\n4096\\n4.75\\n26.2\\n90\\n(D)\\n0.0\\n5.77\\n24.6\\n0.2\\n4.95\\n25.5\\n0.0\\n4.67\\n25.3\\n0.2\\n5.47\\n25.7\\n(E)\\npositional embedding instead of sinusoids\\n4.92\\n25.7\\nbig\\n6\\n1024\\n4096\\n16\\n0.3\\n300K\\n4.33\\n26.4\\n213\\ndevelopment set, newstest2013. We used beam search as described in the previous section, but no\\ncheckpoint averaging. We present these results in Table 3.\\nIn Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,\\nkeeping the amount of computation constant, as described in Section 3.2.2. While single-head\\nattention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.\\nIn Table 3 rows (B), we observe that reducing the attention key size dk hurts model quality. This\\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\\nfunction than dot product may be beneficial. We further observe in rows (C) and (D) that, as expected,\\nbigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our\\nsinusoidal positional encoding with learned positional embeddings [9], and observe nearly identical\\nresults to the base model.\\n6.3\\nEnglish Constituency Parsing\\nTo evaluate if the Transformer can generalize to other tasks we performed experiments on English\\nconstituency parsing. This task presents specific challenges: the output is subject to strong structural\\nconstraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence\\nmodels have not been able to attain state-of-the-art results in small-data regimes [37].\\nWe trained a 4-layer transformer with dmodel = 1024 on the Wall Street Journal (WSJ) portion of the\\nPenn Treebank [25], about 40K training sentences. We also trained it in a semi-supervised setting,\\nusing the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences\\n[37]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens\\nfor the semi-supervised setting.\\nWe performed only a small number of experiments to select the dropout, both attention and residual\\n(section 5.4), learning rates and beam size on the Section 22 development set, all other parameters\\nremained unchanged from the English-to-German base translation model. During inference, we\\n9'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'source': '../datas/pdf_files/attention.pdf', 'file_path': '../datas/pdf_files/attention.pdf', 'total_pages': 15, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'trapped': '', 'modDate': 'D:20240410211143Z', 'creationDate': 'D:20240410211143Z', 'page': 9}, page_content='Table 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23\\nof WSJ)\\nParser\\nTraining\\nWSJ 23 F1\\nVinyals & Kaiser el al. (2014) [37]\\nWSJ only, discriminative\\n88.3\\nPetrov et al. (2006) [29]\\nWSJ only, discriminative\\n90.4\\nZhu et al. (2013) [40]\\nWSJ only, discriminative\\n90.4\\nDyer et al. (2016) [8]\\nWSJ only, discriminative\\n91.7\\nTransformer (4 layers)\\nWSJ only, discriminative\\n91.3\\nZhu et al. (2013) [40]\\nsemi-supervised\\n91.3\\nHuang & Harper (2009) [14]\\nsemi-supervised\\n91.3\\nMcClosky et al. (2006) [26]\\nsemi-supervised\\n92.1\\nVinyals & Kaiser el al. (2014) [37]\\nsemi-supervised\\n92.1\\nTransformer (4 layers)\\nsemi-supervised\\n92.7\\nLuong et al. (2015) [23]\\nmulti-task\\n93.0\\nDyer et al. (2016) [8]\\ngenerative\\n93.3\\nincreased the maximum output length to input length + 300. We used a beam size of 21 and α = 0.3\\nfor both WSJ only and the semi-supervised setting.\\nOur results in Table 4 show that despite the lack of task-specific tuning our model performs sur-\\nprisingly well, yielding better results than all previously reported models with the exception of the\\nRecurrent Neural Network Grammar [8].\\nIn contrast to RNN sequence-to-sequence models [37], the Transformer outperforms the Berkeley-\\nParser [29] even when training only on the WSJ training set of 40K sentences.\\n7\\nConclusion\\nIn this work, we presented the Transformer, the first sequence transduction model based entirely on\\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\\nmulti-headed self-attention.\\nFor translation tasks, the Transformer can be trained significantly faster than architectures based\\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\\nmodel outperforms even all previously reported ensembles.\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\\nplan to extend the Transformer to problems involving input and output modalities other than text and\\nto investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs\\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\\nThe code we used to train and evaluate our models is available at https://github.com/\\ntensorflow/tensor2tensor.\\nAcknowledgements\\nWe are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\\ncomments, corrections and inspiration.\\nReferences\\n[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\\narXiv:1607.06450, 2016.\\n[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\\nlearning to align and translate. CoRR, abs/1409.0473, 2014.\\n[3] Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V. Le. Massive exploration of neural\\nmachine translation architectures. CoRR, abs/1703.03906, 2017.\\n[4] Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\\nreading. arXiv preprint arXiv:1601.06733, 2016.\\n10'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'source': '../datas/pdf_files/attention.pdf', 'file_path': '../datas/pdf_files/attention.pdf', 'total_pages': 15, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'trapped': '', 'modDate': 'D:20240410211143Z', 'creationDate': 'D:20240410211143Z', 'page': 10}, page_content='[5] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\nmachine translation. CoRR, abs/1406.1078, 2014.\\n[6] Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\npreprint arXiv:1610.02357, 2016.\\n[7] Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\nof gated recurrent neural networks on sequence modeling. CoRR, abs/1412.3555, 2014.\\n[8] Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A. Smith. Recurrent neural\\nnetwork grammars. In Proc. of NAACL, 2016.\\n[9] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2, 2017.\\n[10] Alex Graves.\\nGenerating sequences with recurrent neural networks.\\narXiv preprint\\narXiv:1308.0850, 2013.\\n[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition, pages 770–778, 2016.\\n[12] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient flow in\\nrecurrent nets: the difficulty of learning long-term dependencies, 2001.\\n[13] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation,\\n9(8):1735–1780, 1997.\\n[14] Zhongqiang Huang and Mary Harper. Self-training PCFG grammars with latent annotations\\nacross languages. In Proceedings of the 2009 Conference on Empirical Methods in Natural\\nLanguage Processing, pages 832–841. ACL, August 2009.\\n[15] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410, 2016.\\n[16] Łukasz Kaiser and Samy Bengio. Can active memory replace attention? In Advances in Neural\\nInformation Processing Systems, (NIPS), 2016.\\n[17] Łukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\non Learning Representations (ICLR), 2016.\\n[18] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\\nray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2,\\n2017.\\n[19] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\nIn International Conference on Learning Representations, 2017.\\n[20] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.\\n[21] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\narXiv:1703.10722, 2017.\\n[22] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\narXiv:1703.03130, 2017.\\n[23] Minh-Thang Luong, Quoc V. Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser. Multi-task\\nsequence to sequence learning. arXiv preprint arXiv:1511.06114, 2015.\\n[24] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\nbased neural machine translation. arXiv preprint arXiv:1508.04025, 2015.\\n11'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'source': '../datas/pdf_files/attention.pdf', 'file_path': '../datas/pdf_files/attention.pdf', 'total_pages': 15, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'trapped': '', 'modDate': 'D:20240410211143Z', 'creationDate': 'D:20240410211143Z', 'page': 11}, page_content='[25] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated\\ncorpus of english: The penn treebank. Computational linguistics, 19(2):313–330, 1993.\\n[26] David McClosky, Eugene Charniak, and Mark Johnson. Effective self-training for parsing. In\\nProceedings of the Human Language Technology Conference of the NAACL, Main Conference,\\npages 152–159. ACL, June 2006.\\n[27] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\\nmodel. In Empirical Methods in Natural Language Processing, 2016.\\n[28] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\\nsummarization. arXiv preprint arXiv:1705.04304, 2017.\\n[29] Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. Learning accurate, compact,\\nand interpretable tree annotation. In Proceedings of the 21st International Conference on\\nComputational Linguistics and 44th Annual Meeting of the ACL, pages 433–440. ACL, July\\n2006.\\n[30] Ofir Press and Lior Wolf. Using the output embedding to improve language models. arXiv\\npreprint arXiv:1608.05859, 2016.\\n[31] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\\nwith subword units. arXiv preprint arXiv:1508.07909, 2015.\\n[32] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\\nlayer. arXiv preprint arXiv:1701.06538, 2017.\\n[33] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\\nnov. Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine\\nLearning Research, 15(1):1929–1958, 2014.\\n[34] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory\\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\\nAdvances in Neural Information Processing Systems 28, pages 2440–2448. Curran Associates,\\nInc., 2015.\\n[35] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\\nnetworks. In Advances in Neural Information Processing Systems, pages 3104–3112, 2014.\\n[36] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\\nRethinking the inception architecture for computer vision. CoRR, abs/1512.00567, 2015.\\n[37] Vinyals & Kaiser, Koo, Petrov, Sutskever, and Hinton. Grammar as a foreign language. In\\nAdvances in Neural Information Processing Systems, 2015.\\n[38] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine\\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\\narXiv:1609.08144, 2016.\\n[39] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\\nfast-forward connections for neural machine translation. CoRR, abs/1606.04199, 2016.\\n[40] Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, and Jingbo Zhu. Fast and accurate\\nshift-reduce constituent parsing. In Proceedings of the 51st Annual Meeting of the ACL (Volume\\n1: Long Papers), pages 434–443. ACL, August 2013.\\n12'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'source': '../datas/pdf_files/attention.pdf', 'file_path': '../datas/pdf_files/attention.pdf', 'total_pages': 15, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'trapped': '', 'modDate': 'D:20240410211143Z', 'creationDate': 'D:20240410211143Z', 'page': 12}, page_content='Attention Visualizations\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\\nthe verb ‘making’, completing the phrase ‘making...more difficult’. Attentions here shown only for\\nthe word ‘making’. Different colors represent different heads. Best viewed in color.\\n13'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'source': '../datas/pdf_files/attention.pdf', 'file_path': '../datas/pdf_files/attention.pdf', 'total_pages': 15, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'trapped': '', 'modDate': 'D:20240410211143Z', 'creationDate': 'D:20240410211143Z', 'page': 13}, page_content='The\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nFigure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\\nFull attentions for head 5. Bottom: Isolated attentions from just the word ‘its’ for attention heads 5\\nand 6. Note that the attentions are very sharp for this word.\\n14'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'source': '../datas/pdf_files/attention.pdf', 'file_path': '../datas/pdf_files/attention.pdf', 'total_pages': 15, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'trapped': '', 'modDate': 'D:20240410211143Z', 'creationDate': 'D:20240410211143Z', 'page': 14}, page_content='The\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nFigure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the\\nsentence. We give two such examples above, from two different heads from the encoder self-attention\\nat layer 5 of 6. The heads clearly learned to perform different tasks.\\n15'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-06-04T00:19:58+00:00', 'source': '../datas/pdf_files/ViT.pdf', 'file_path': '../datas/pdf_files/ViT.pdf', 'total_pages': 22, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-06-04T00:19:58+00:00', 'trapped': '', 'modDate': 'D:20210604001958Z', 'creationDate': 'D:20210604001958Z', 'page': 0}, page_content='Published as a conference paper at ICLR 2021\\nAN IMAGE IS WORTH 16X16 WORDS:\\nTRANSFORMERS FOR IMAGE RECOGNITION AT SCALE\\nAlexey Dosovitskiy∗,†, Lucas Beyer∗, Alexander Kolesnikov∗, Dirk Weissenborn∗,\\nXiaohua Zhai∗, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer,\\nGeorg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby∗,†\\n∗equal technical contribution, †equal advising\\nGoogle Research, Brain Team\\n{adosovitskiy, neilhoulsby}@google.com\\nABSTRACT\\nWhile the Transformer architecture has become the de-facto standard for natural\\nlanguage processing tasks, its applications to computer vision remain limited. In\\nvision, attention is either applied in conjunction with convolutional networks, or\\nused to replace certain components of convolutional networks while keeping their\\noverall structure in place. We show that this reliance on CNNs is not necessary\\nand a pure transformer applied directly to sequences of image patches can perform\\nvery well on image classiﬁcation tasks. When pre-trained on large amounts of\\ndata and transferred to multiple mid-sized or small image recognition benchmarks\\n(ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent\\nresults compared to state-of-the-art convolutional networks while requiring sub-\\nstantially fewer computational resources to train.1\\n1\\nINTRODUCTION\\nSelf-attention-based architectures, in particular Transformers (Vaswani et al., 2017), have become\\nthe model of choice in natural language processing (NLP). The dominant approach is to pre-train on\\na large text corpus and then ﬁne-tune on a smaller task-speciﬁc dataset (Devlin et al., 2019). Thanks\\nto Transformers’ computational efﬁciency and scalability, it has become possible to train models of\\nunprecedented size, with over 100B parameters (Brown et al., 2020; Lepikhin et al., 2020). With the\\nmodels and datasets growing, there is still no sign of saturating performance.\\nIn computer vision, however, convolutional architectures remain dominant (LeCun et al., 1989;\\nKrizhevsky et al., 2012; He et al., 2016). Inspired by NLP successes, multiple works try combining\\nCNN-like architectures with self-attention (Wang et al., 2018; Carion et al., 2020), some replacing\\nthe convolutions entirely (Ramachandran et al., 2019; Wang et al., 2020a). The latter models, while\\ntheoretically efﬁcient, have not yet been scaled effectively on modern hardware accelerators due to\\nthe use of specialized attention patterns. Therefore, in large-scale image recognition, classic ResNet-\\nlike architectures are still state of the art (Mahajan et al., 2018; Xie et al., 2020; Kolesnikov et al.,\\n2020).\\nInspired by the Transformer scaling successes in NLP, we experiment with applying a standard\\nTransformer directly to images, with the fewest possible modiﬁcations. To do so, we split an image\\ninto patches and provide the sequence of linear embeddings of these patches as an input to a Trans-\\nformer. Image patches are treated the same way as tokens (words) in an NLP application. We train\\nthe model on image classiﬁcation in supervised fashion.\\nWhen trained on mid-sized datasets such as ImageNet without strong regularization, these mod-\\nels yield modest accuracies of a few percentage points below ResNets of comparable size. This\\nseemingly discouraging outcome may be expected: Transformers lack some of the inductive biases\\n1Fine-tuning\\ncode\\nand\\npre-trained\\nmodels\\nare\\navailable\\nat\\nhttps://github.com/\\ngoogle-research/vision_transformer\\n1\\narXiv:2010.11929v2  [cs.CV]  3 Jun 2021'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-06-04T00:19:58+00:00', 'source': '../datas/pdf_files/ViT.pdf', 'file_path': '../datas/pdf_files/ViT.pdf', 'total_pages': 22, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-06-04T00:19:58+00:00', 'trapped': '', 'modDate': 'D:20210604001958Z', 'creationDate': 'D:20210604001958Z', 'page': 1}, page_content='Published as a conference paper at ICLR 2021\\ninherent to CNNs, such as translation equivariance and locality, and therefore do not generalize well\\nwhen trained on insufﬁcient amounts of data.\\nHowever, the picture changes if the models are trained on larger datasets (14M-300M images). We\\nﬁnd that large scale training trumps inductive bias. Our Vision Transformer (ViT) attains excellent\\nresults when pre-trained at sufﬁcient scale and transferred to tasks with fewer datapoints. When\\npre-trained on the public ImageNet-21k dataset or the in-house JFT-300M dataset, ViT approaches\\nor beats state of the art on multiple image recognition benchmarks. In particular, the best model\\nreaches the accuracy of 88.55% on ImageNet, 90.72% on ImageNet-ReaL, 94.55% on CIFAR-100,\\nand 77.63% on the VTAB suite of 19 tasks.\\n2\\nRELATED WORK\\nTransformers were proposed by Vaswani et al. (2017) for machine translation, and have since be-\\ncome the state of the art method in many NLP tasks. Large Transformer-based models are often\\npre-trained on large corpora and then ﬁne-tuned for the task at hand: BERT (Devlin et al., 2019)\\nuses a denoising self-supervised pre-training task, while the GPT line of work uses language mod-\\neling as its pre-training task (Radford et al., 2018; 2019; Brown et al., 2020).\\nNaive application of self-attention to images would require that each pixel attends to every other\\npixel. With quadratic cost in the number of pixels, this does not scale to realistic input sizes. Thus,\\nto apply Transformers in the context of image processing, several approximations have been tried in\\nthe past. Parmar et al. (2018) applied the self-attention only in local neighborhoods for each query\\npixel instead of globally. Such local multi-head dot-product self attention blocks can completely\\nreplace convolutions (Hu et al., 2019; Ramachandran et al., 2019; Zhao et al., 2020). In a different\\nline of work, Sparse Transformers (Child et al., 2019) employ scalable approximations to global self-\\nattention in order to be applicable to images. An alternative way to scale attention is to apply it in\\nblocks of varying sizes (Weissenborn et al., 2019), in the extreme case only along individual axes (Ho\\net al., 2019; Wang et al., 2020a). Many of these specialized attention architectures demonstrate\\npromising results on computer vision tasks, but require complex engineering to be implemented\\nefﬁciently on hardware accelerators.\\nMost related to ours is the model of Cordonnier et al. (2020), which extracts patches of size 2 × 2\\nfrom the input image and applies full self-attention on top. This model is very similar to ViT,\\nbut our work goes further to demonstrate that large scale pre-training makes vanilla transformers\\ncompetitive with (or even better than) state-of-the-art CNNs. Moreover, Cordonnier et al. (2020)\\nuse a small patch size of 2 × 2 pixels, which makes the model applicable only to small-resolution\\nimages, while we handle medium-resolution images as well.\\nThere has also been a lot of interest in combining convolutional neural networks (CNNs) with forms\\nof self-attention, e.g. by augmenting feature maps for image classiﬁcation (Bello et al., 2019) or by\\nfurther processing the output of a CNN using self-attention, e.g. for object detection (Hu et al., 2018;\\nCarion et al., 2020), video processing (Wang et al., 2018; Sun et al., 2019), image classiﬁcation (Wu\\net al., 2020), unsupervised object discovery (Locatello et al., 2020), or uniﬁed text-vision tasks (Chen\\net al., 2020c; Lu et al., 2019; Li et al., 2019).\\nAnother recent related model is image GPT (iGPT) (Chen et al., 2020a), which applies Transformers\\nto image pixels after reducing image resolution and color space. The model is trained in an unsu-\\npervised fashion as a generative model, and the resulting representation can then be ﬁne-tuned or\\nprobed linearly for classiﬁcation performance, achieving a maximal accuracy of 72% on ImageNet.\\nOur work adds to the increasing collection of papers that explore image recognition at larger scales\\nthan the standard ImageNet dataset. The use of additional data sources allows to achieve state-of-\\nthe-art results on standard benchmarks (Mahajan et al., 2018; Touvron et al., 2019; Xie et al., 2020).\\nMoreover, Sun et al. (2017) study how CNN performance scales with dataset size, and Kolesnikov\\net al. (2020); Djolonga et al. (2020) perform an empirical exploration of CNN transfer learning from\\nlarge scale datasets such as ImageNet-21k and JFT-300M. We focus on these two latter datasets as\\nwell, but train Transformers instead of ResNet-based models used in prior works.\\n2'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-06-04T00:19:58+00:00', 'source': '../datas/pdf_files/ViT.pdf', 'file_path': '../datas/pdf_files/ViT.pdf', 'total_pages': 22, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-06-04T00:19:58+00:00', 'trapped': '', 'modDate': 'D:20210604001958Z', 'creationDate': 'D:20210604001958Z', 'page': 2}, page_content='Published as a conference paper at ICLR 2021\\nTransformer Encoder\\nMLP \\nHead\\nVision Transformer (ViT)\\n*\\nLinear Projection of Flattened Patches\\n* Extra learnable\\n     [ cl ass]  embedding\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n0\\nPatch + Position \\nEmbedding\\nClass\\nBird\\nBall\\nCar\\n...\\nEmbedded \\nPatches\\nMulti-Head \\nAttention\\nNorm\\nMLP\\nNorm\\n+\\nL x\\n+\\nTransformer Encoder\\nFigure 1: Model overview. We split an image into ﬁxed-size patches, linearly embed each of them,\\nadd position embeddings, and feed the resulting sequence of vectors to a standard Transformer\\nencoder. In order to perform classiﬁcation, we use the standard approach of adding an extra learnable\\n“classiﬁcation token” to the sequence. The illustration of the Transformer encoder was inspired by\\nVaswani et al. (2017).\\n3\\nMETHOD\\nIn model design we follow the original Transformer (Vaswani et al., 2017) as closely as possible.\\nAn advantage of this intentionally simple setup is that scalable NLP Transformer architectures – and\\ntheir efﬁcient implementations – can be used almost out of the box.\\n3.1\\nVISION TRANSFORMER (VIT)\\nAn overview of the model is depicted in Figure 1. The standard Transformer receives as input a 1D\\nsequence of token embeddings. To handle 2D images, we reshape the image x ∈RH×W ×C into a\\nsequence of ﬂattened 2D patches xp ∈RN×(P 2·C), where (H, W) is the resolution of the original\\nimage, C is the number of channels, (P, P) is the resolution of each image patch, and N = HW/P 2\\nis the resulting number of patches, which also serves as the effective input sequence length for the\\nTransformer. The Transformer uses constant latent vector size D through all of its layers, so we\\nﬂatten the patches and map to D dimensions with a trainable linear projection (Eq. 1). We refer to\\nthe output of this projection as the patch embeddings.\\nSimilar to BERT’s [class] token, we prepend a learnable embedding to the sequence of embed-\\nded patches (z0\\n0 = xclass), whose state at the output of the Transformer encoder (z0\\nL) serves as the\\nimage representation y (Eq. 4). Both during pre-training and ﬁne-tuning, a classiﬁcation head is at-\\ntached to z0\\nL. The classiﬁcation head is implemented by a MLP with one hidden layer at pre-training\\ntime and by a single linear layer at ﬁne-tuning time.\\nPosition embeddings are added to the patch embeddings to retain positional information. We use\\nstandard learnable 1D position embeddings, since we have not observed signiﬁcant performance\\ngains from using more advanced 2D-aware position embeddings (Appendix D.4). The resulting\\nsequence of embedding vectors serves as input to the encoder.\\nThe Transformer encoder (Vaswani et al., 2017) consists of alternating layers of multiheaded self-\\nattention (MSA, see Appendix A) and MLP blocks (Eq. 2, 3). Layernorm (LN) is applied before\\nevery block, and residual connections after every block (Wang et al., 2019; Baevski & Auli, 2019).\\n3'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-06-04T00:19:58+00:00', 'source': '../datas/pdf_files/ViT.pdf', 'file_path': '../datas/pdf_files/ViT.pdf', 'total_pages': 22, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-06-04T00:19:58+00:00', 'trapped': '', 'modDate': 'D:20210604001958Z', 'creationDate': 'D:20210604001958Z', 'page': 3}, page_content='Published as a conference paper at ICLR 2021\\nThe MLP contains two layers with a GELU non-linearity.\\nz0 = [xclass; x1\\npE; x2\\npE; · · · ; xN\\np E] + Epos,\\nE ∈R(P 2·C)×D, Epos ∈R(N+1)×D\\n(1)\\nz′\\nℓ= MSA(LN(zℓ−1)) + zℓ−1,\\nℓ= 1 . . . L\\n(2)\\nzℓ= MLP(LN(z′\\nℓ)) + z′\\nℓ,\\nℓ= 1 . . . L\\n(3)\\ny = LN(z0\\nL)\\n(4)\\nInductive bias.\\nWe note that Vision Transformer has much less image-speciﬁc inductive bias than\\nCNNs. In CNNs, locality, two-dimensional neighborhood structure, and translation equivariance are\\nbaked into each layer throughout the whole model. In ViT, only MLP layers are local and transla-\\ntionally equivariant, while the self-attention layers are global. The two-dimensional neighborhood\\nstructure is used very sparingly: in the beginning of the model by cutting the image into patches and\\nat ﬁne-tuning time for adjusting the position embeddings for images of different resolution (as de-\\nscribed below). Other than that, the position embeddings at initialization time carry no information\\nabout the 2D positions of the patches and all spatial relations between the patches have to be learned\\nfrom scratch.\\nHybrid Architecture.\\nAs an alternative to raw image patches, the input sequence can be formed\\nfrom feature maps of a CNN (LeCun et al., 1989). In this hybrid model, the patch embedding\\nprojection E (Eq. 1) is applied to patches extracted from a CNN feature map. As a special case,\\nthe patches can have spatial size 1x1, which means that the input sequence is obtained by simply\\nﬂattening the spatial dimensions of the feature map and projecting to the Transformer dimension.\\nThe classiﬁcation input embedding and position embeddings are added as described above.\\n3.2\\nFINE-TUNING AND HIGHER RESOLUTION\\nTypically, we pre-train ViT on large datasets, and ﬁne-tune to (smaller) downstream tasks. For\\nthis, we remove the pre-trained prediction head and attach a zero-initialized D × K feedforward\\nlayer, where K is the number of downstream classes. It is often beneﬁcial to ﬁne-tune at higher\\nresolution than pre-training (Touvron et al., 2019; Kolesnikov et al., 2020). When feeding images\\nof higher resolution, we keep the patch size the same, which results in a larger effective sequence\\nlength. The Vision Transformer can handle arbitrary sequence lengths (up to memory constraints),\\nhowever, the pre-trained position embeddings may no longer be meaningful. We therefore perform\\n2D interpolation of the pre-trained position embeddings, according to their location in the original\\nimage. Note that this resolution adjustment and patch extraction are the only points at which an\\ninductive bias about the 2D structure of the images is manually injected into the Vision Transformer.\\n4\\nEXPERIMENTS\\nWe evaluate the representation learning capabilities of ResNet, Vision Transformer (ViT), and the\\nhybrid. To understand the data requirements of each model, we pre-train on datasets of varying size\\nand evaluate many benchmark tasks. When considering the computational cost of pre-training the\\nmodel, ViT performs very favourably, attaining state of the art on most recognition benchmarks at\\na lower pre-training cost. Lastly, we perform a small experiment using self-supervision, and show\\nthat self-supervised ViT holds promise for the future.\\n4.1\\nSETUP\\nDatasets. To explore model scalability, we use the ILSVRC-2012 ImageNet dataset with 1k classes\\nand 1.3M images (we refer to it as ImageNet in what follows), its superset ImageNet-21k with\\n21k classes and 14M images (Deng et al., 2009), and JFT (Sun et al., 2017) with 18k classes and\\n303M high-resolution images. We de-duplicate the pre-training datasets w.r.t. the test sets of the\\ndownstream tasks following Kolesnikov et al. (2020). We transfer the models trained on these\\ndataset to several benchmark tasks: ImageNet on the original validation labels and the cleaned-up\\nReaL labels (Beyer et al., 2020), CIFAR-10/100 (Krizhevsky, 2009), Oxford-IIIT Pets (Parkhi et al.,\\n2012), and Oxford Flowers-102 (Nilsback & Zisserman, 2008). For these datasets, pre-processing\\nfollows Kolesnikov et al. (2020).\\n4'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-06-04T00:19:58+00:00', 'source': '../datas/pdf_files/ViT.pdf', 'file_path': '../datas/pdf_files/ViT.pdf', 'total_pages': 22, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-06-04T00:19:58+00:00', 'trapped': '', 'modDate': 'D:20210604001958Z', 'creationDate': 'D:20210604001958Z', 'page': 4}, page_content='Published as a conference paper at ICLR 2021\\nModel\\nLayers\\nHidden size D\\nMLP size\\nHeads\\nParams\\nViT-Base\\n12\\n768\\n3072\\n12\\n86M\\nViT-Large\\n24\\n1024\\n4096\\n16\\n307M\\nViT-Huge\\n32\\n1280\\n5120\\n16\\n632M\\nTable 1: Details of Vision Transformer model variants.\\nWe also evaluate on the 19-task VTAB classiﬁcation suite (Zhai et al., 2019b). VTAB evaluates\\nlow-data transfer to diverse tasks, using 1 000 training examples per task. The tasks are divided into\\nthree groups: Natural – tasks like the above, Pets, CIFAR, etc. Specialized – medical and satellite\\nimagery, and Structured – tasks that require geometric understanding like localization.\\nModel Variants. We base ViT conﬁgurations on those used for BERT (Devlin et al., 2019), as\\nsummarized in Table 1. The “Base” and “Large” models are directly adopted from BERT and we\\nadd the larger “Huge” model. In what follows we use brief notation to indicate the model size and\\nthe input patch size: for instance, ViT-L/16 means the “Large” variant with 16×16 input patch size.\\nNote that the Transformer’s sequence length is inversely proportional to the square of the patch size,\\nthus models with smaller patch size are computationally more expensive.\\nFor the baseline CNNs, we use ResNet (He et al., 2016), but replace the Batch Normalization lay-\\ners (Ioffe & Szegedy, 2015) with Group Normalization (Wu & He, 2018), and used standardized\\nconvolutions (Qiao et al., 2019). These modiﬁcations improve transfer (Kolesnikov et al., 2020),\\nand we denote the modiﬁed model “ResNet (BiT)”. For the hybrids, we feed the intermediate fea-\\nture maps into ViT with patch size of one “pixel”. To experiment with different sequence lengths,\\nwe either (i) take the output of stage 4 of a regular ResNet50 or (ii) remove stage 4, place the same\\nnumber of layers in stage 3 (keeping the total number of layers), and take the output of this extended\\nstage 3. Option (ii) results in a 4x longer sequence length, and a more expensive ViT model.\\nTraining & Fine-tuning. We train all models, including ResNets, using Adam (Kingma & Ba,\\n2015) with β1 = 0.9, β2 = 0.999, a batch size of 4096 and apply a high weight decay of 0.1, which\\nwe found to be useful for transfer of all models (Appendix D.1 shows that, in contrast to common\\npractices, Adam works slightly better than SGD for ResNets in our setting). We use a linear learning\\nrate warmup and decay, see Appendix B.1 for details. For ﬁne-tuning we use SGD with momentum,\\nbatch size 512, for all models, see Appendix B.1.1. For ImageNet results in Table 2, we ﬁne-tuned at\\nhigher resolution: 512 for ViT-L/16 and 518 for ViT-H/14, and also used Polyak & Juditsky (1992)\\naveraging with a factor of 0.9999 (Ramachandran et al., 2019; Wang et al., 2020b).\\nMetrics. We report results on downstream datasets either through few-shot or ﬁne-tuning accuracy.\\nFine-tuning accuracies capture the performance of each model after ﬁne-tuning it on the respective\\ndataset. Few-shot accuracies are obtained by solving a regularized least-squares regression problem\\nthat maps the (frozen) representation of a subset of training images to {−1, 1}K target vectors. This\\nformulation allows us to recover the exact solution in closed form. Though we mainly focus on\\nﬁne-tuning performance, we sometimes use linear few-shot accuracies for fast on-the-ﬂy evaluation\\nwhere ﬁne-tuning would be too costly.\\n4.2\\nCOMPARISON TO STATE OF THE ART\\nWe ﬁrst compare our largest models – ViT-H/14 and ViT-L/16 – to state-of-the-art CNNs from\\nthe literature. The ﬁrst comparison point is Big Transfer (BiT) (Kolesnikov et al., 2020), which\\nperforms supervised transfer learning with large ResNets. The second is Noisy Student (Xie et al.,\\n2020), which is a large EfﬁcientNet trained using semi-supervised learning on ImageNet and JFT-\\n300M with the labels removed. Currently, Noisy Student is the state of the art on ImageNet and\\nBiT-L on the other datasets reported here. All models were trained on TPUv3 hardware, and we\\nreport the number of TPUv3-core-days taken to pre-train each of them, that is, the number of TPU\\nv3 cores (2 per chip) used for training multiplied by the training time in days.\\nTable 2 shows the results. The smaller ViT-L/16 model pre-trained on JFT-300M outperforms BiT-L\\n(which is pre-trained on the same dataset) on all tasks, while requiring substantially less computa-\\ntional resources to train. The larger model, ViT-H/14, further improves the performance, especially\\non the more challenging datasets – ImageNet, CIFAR-100, and the VTAB suite. Interestingly, this\\n5'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-06-04T00:19:58+00:00', 'source': '../datas/pdf_files/ViT.pdf', 'file_path': '../datas/pdf_files/ViT.pdf', 'total_pages': 22, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-06-04T00:19:58+00:00', 'trapped': '', 'modDate': 'D:20210604001958Z', 'creationDate': 'D:20210604001958Z', 'page': 5}, page_content='Published as a conference paper at ICLR 2021\\nOurs-JFT\\nOurs-JFT\\nOurs-I21k\\nBiT-L\\nNoisy Student\\n(ViT-H/14)\\n(ViT-L/16)\\n(ViT-L/16)\\n(ResNet152x4)\\n(EfﬁcientNet-L2)\\nImageNet\\n88.55 ± 0.04\\n87.76 ± 0.03\\n85.30 ± 0.02\\n87.54 ± 0.02\\n88.4/88.5∗\\nImageNet ReaL\\n90.72 ± 0.05\\n90.54 ± 0.03\\n88.62 ± 0.05\\n90.54\\n90.55\\nCIFAR-10\\n99.50 ± 0.06\\n99.42 ± 0.03\\n99.15 ± 0.03\\n99.37 ± 0.06\\n−\\nCIFAR-100\\n94.55 ± 0.04\\n93.90 ± 0.05\\n93.25 ± 0.05\\n93.51 ± 0.08\\n−\\nOxford-IIIT Pets\\n97.56 ± 0.03\\n97.32 ± 0.11\\n94.67 ± 0.15\\n96.62 ± 0.23\\n−\\nOxford Flowers-102\\n99.68 ± 0.02\\n99.74 ± 0.00\\n99.61 ± 0.02\\n99.63 ± 0.03\\n−\\nVTAB (19 tasks)\\n77.63 ± 0.23\\n76.28 ± 0.46\\n72.72 ± 0.21\\n76.29 ± 1.70\\n−\\nTPUv3-core-days\\n2.5k\\n0.68k\\n0.23k\\n9.9k\\n12.3k\\nTable 2:\\nComparison with state of the art on popular image classiﬁcation benchmarks. We re-\\nport mean and standard deviation of the accuracies, averaged over three ﬁne-tuning runs. Vision\\nTransformer models pre-trained on the JFT-300M dataset outperform ResNet-based baselines on all\\ndatasets, while taking substantially less computational resources to pre-train. ViT pre-trained on the\\nsmaller public ImageNet-21k dataset performs well too. ∗Slightly improved 88.5% result reported\\nin Touvron et al. (2020).\\nVTAB (19 tasks)\\n65\\n70\\n75\\n80\\nAccuracy [%]\\nNatural (7 tasks)\\n70\\n80\\n90\\nSpecialized (4 tasks)\\n80\\n82\\n85\\n88\\n90\\nStructured (8 tasks)\\n50\\n60\\n70\\nViT-H/14\\nBiT-L (R152x4)\\nVIVI-Ex-100% (R50x3)\\nS4L (R50x1)\\nFigure 2: Breakdown of VTAB performance in Natural, Specialized, and Structured task groups.\\nmodel still took substantially less compute to pre-train than prior state of the art. However, we note\\nthat pre-training efﬁciency may be affected not only by the architecture choice, but also other pa-\\nrameters, such as training schedule, optimizer, weight decay, etc. We provide a controlled study of\\nperformance vs. compute for different architectures in Section 4.4. Finally, the ViT-L/16 model\\npre-trained on the public ImageNet-21k dataset performs well on most datasets too, while taking\\nfewer resources to pre-train: it could be trained using a standard cloud TPUv3 with 8 cores in ap-\\nproximately 30 days.\\nFigure 2 decomposes the VTAB tasks into their respective groups, and compares to previous SOTA\\nmethods on this benchmark: BiT, VIVI – a ResNet co-trained on ImageNet and Youtube (Tschannen\\net al., 2020), and S4L – supervised plus semi-supervised learning on ImageNet (Zhai et al., 2019a).\\nViT-H/14 outperforms BiT-R152x4, and other methods, on the Natural and Structured tasks. On the\\nSpecialized the performance of the top two models is similar.\\n4.3\\nPRE-TRAINING DATA REQUIREMENTS\\nThe Vision Transformer performs well when pre-trained on a large JFT-300M dataset. With fewer\\ninductive biases for vision than ResNets, how crucial is the dataset size? We perform two series of\\nexperiments.\\nFirst, we pre-train ViT models on datasets of increasing size: ImageNet, ImageNet-21k, and JFT-\\n300M. To boost the performance on the smaller datasets, we optimize three basic regularization\\nparameters – weight decay, dropout, and label smoothing. Figure 3 shows the results after ﬁne-\\ntuning to ImageNet (results on other datasets are shown in Table 5)2. When pre-trained on the\\nsmallest dataset, ImageNet, ViT-Large models underperform compared to ViT-Base models, despite\\n(moderate) regularization. With ImageNet-21k pre-training, their performances are similar. Only\\nwith JFT-300M, do we see the full beneﬁt of larger models. Figure 3 also shows the performance\\n2Note that the ImageNet pre-trained models are also ﬁne-tuned, but again on ImageNet. This is because the\\nresolution increase during ﬁne-tuning improves the performance.\\n6'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-06-04T00:19:58+00:00', 'source': '../datas/pdf_files/ViT.pdf', 'file_path': '../datas/pdf_files/ViT.pdf', 'total_pages': 22, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-06-04T00:19:58+00:00', 'trapped': '', 'modDate': 'D:20210604001958Z', 'creationDate': 'D:20210604001958Z', 'page': 6}, page_content='Published as a conference paper at ICLR 2021\\nImageNet\\nImageNet-21k\\nJFT-300M\\nPre-training dataset\\n70\\n75\\n80\\n85\\n90\\nImageNet Top1 Accuracy [%]\\nBiT\\nViT-B/32\\nViT-B/16\\nViT-L/32\\nViT-L/16\\nViT-H/14\\nFigure 3:\\nTransfer to ImageNet.\\nWhile\\nlarge ViT models perform worse than BiT\\nResNets (shaded area) when pre-trained on\\nsmall datasets, they shine when pre-trained on\\nlarger datasets. Similarly, larger ViT variants\\novertake smaller ones as the dataset grows.\\n10 M\\n30 M\\n100 M\\n300 M\\nNumber of JFT pre-training samples\\n30\\n40\\n50\\n60\\n70\\nLinear 5-shot ImageNet Top1 [%]\\nViT-L/16\\nViT-L/32\\nViT-B/32\\nViT-b/32\\nResNet50x1 (BiT)\\nResNet152x2 (BiT)\\nFigure 4: Linear few-shot evaluation on Ima-\\ngeNet versus pre-training size. ResNets per-\\nform better with smaller pre-training datasets\\nbut plateau sooner than ViT, which performs\\nbetter with larger pre-training. ViT-b is ViT-B\\nwith all hidden dimensions halved.\\n102\\n103\\n90\\n95\\nTransfer accuracy [%]\\nAverage-5\\nTransformer (ViT)\\nResNet (BiT)\\nHybrid\\n102\\n103\\n75\\n80\\n85\\n90\\nImageNet\\nTransformer (ViT)\\nResNet (BiT)\\nHybrid\\nTotal pre-training compute [exaFLOPs]\\nFigure 5: Performance versus pre-training compute for different architectures: Vision Transformers,\\nResNets, and hybrids. Vision Transformers generally outperform ResNets with the same compu-\\ntational budget. Hybrids improve upon pure Transformers for smaller model sizes, but the gap\\nvanishes for larger models.\\nregion spanned by BiT models of different sizes. The BiT CNNs outperform ViT on ImageNet, but\\nwith the larger datasets, ViT overtakes.\\nSecond, we train our models on random subsets of 9M, 30M, and 90M as well as the full JFT-\\n300M dataset. We do not perform additional regularization on the smaller subsets and use the same\\nhyper-parameters for all settings. This way, we assess the intrinsic model properties, and not the\\neffect of regularization. We do, however, use early-stopping, and report the best validation accuracy\\nachieved during training. To save compute, we report few-shot linear accuracy instead of full ﬁne-\\ntuning accuracy. Figure 4 contains the results. Vision Transformers overﬁt more than ResNets with\\ncomparable computational cost on smaller datasets. For example, ViT-B/32 is slightly faster than\\nResNet50; it performs much worse on the 9M subset, but better on 90M+ subsets. The same is true\\nfor ResNet152x2 and ViT-L/16. This result reinforces the intuition that the convolutional inductive\\nbias is useful for smaller datasets, but for larger ones, learning the relevant patterns directly from\\ndata is sufﬁcient, even beneﬁcial.\\nOverall, the few-shot results on ImageNet (Figure 4), as well as the low-data results on VTAB\\n(Table 2) seem promising for very low-data transfer. Further analysis of few-shot properties of ViT\\nis an exciting direction of future work.\\n7'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-06-04T00:19:58+00:00', 'source': '../datas/pdf_files/ViT.pdf', 'file_path': '../datas/pdf_files/ViT.pdf', 'total_pages': 22, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-06-04T00:19:58+00:00', 'trapped': '', 'modDate': 'D:20210604001958Z', 'creationDate': 'D:20210604001958Z', 'page': 7}, page_content='Published as a conference paper at ICLR 2021\\n4.4\\nSCALING STUDY\\nWe perform a controlled scaling study of different models by evaluating transfer performance from\\nJFT-300M. In this setting data size does not bottleneck the models’ performances, and we assess\\nperformance versus pre-training cost of each model. The model set includes: 7 ResNets, R50x1,\\nR50x2 R101x1, R152x1, R152x2, pre-trained for 7 epochs, plus R152x2 and R200x3 pre-trained\\nfor 14 epochs; 6 Vision Transformers, ViT-B/32, B/16, L/32, L/16, pre-trained for 7 epochs, plus\\nL/16 and H/14 pre-trained for 14 epochs; and 5 hybrids, R50+ViT-B/32, B/16, L/32, L/16 pre-\\ntrained for 7 epochs, plus R50+ViT-L/16 pre-trained for 14 epochs (for hybrids, the number at the\\nend of the model name stands not for the patch size, but for the total dowsampling ratio in the ResNet\\nbackbone).\\nFigure 5 contains the transfer performance versus total pre-training compute (see Appendix D.5\\nfor details on computational costs). Detailed results per model are provided in Table 6 in the Ap-\\npendix. A few patterns can be observed. First, Vision Transformers dominate ResNets on the\\nperformance/compute trade-off. ViT uses approximately 2 −4× less compute to attain the same\\nperformance (average over 5 datasets). Second, hybrids slightly outperform ViT at small compu-\\ntational budgets, but the difference vanishes for larger models. This result is somewhat surprising,\\nsince one might expect convolutional local feature processing to assist ViT at any size. Third, Vision\\nTransformers appear not to saturate within the range tried, motivating future scaling efforts.\\n4.5\\nINSPECTING VISION TRANSFORMER\\nInput\\nAttention\\nFigure 6: Representative ex-\\namples of attention from the\\noutput token to the input\\nspace. See Appendix D.7 for\\ndetails.\\nTo begin to understand how the Vision Transformer processes im-\\nage data, we analyze its internal representations. The ﬁrst layer of\\nthe Vision Transformer linearly projects the ﬂattened patches into a\\nlower-dimensional space (Eq. 1). Figure 7 (left) shows the top prin-\\ncipal components of the the learned embedding ﬁlters. The com-\\nponents resemble plausible basis functions for a low-dimensional\\nrepresentation of the ﬁne structure within each patch.\\nAfter the projection, a learned position embedding is added to the\\npatch representations. Figure 7 (center) shows that the model learns\\nto encode distance within the image in the similarity of position em-\\nbeddings, i.e. closer patches tend to have more similar position em-\\nbeddings. Further, the row-column structure appears; patches in the\\nsame row/column have similar embeddings. Finally, a sinusoidal\\nstructure is sometimes apparent for larger grids (Appendix D). That\\nthe position embeddings learn to represent 2D image topology ex-\\nplains why hand-crafted 2D-aware embedding variants do not yield\\nimprovements (Appendix D.4).\\nSelf-attention allows ViT to integrate information across the entire\\nimage even in the lowest layers. We investigate to what degree\\nthe network makes use of this capability. Speciﬁcally, we compute\\nthe average distance in image space across which information is\\nintegrated, based on the attention weights (Figure 7, right). This\\n“attention distance” is analogous to receptive ﬁeld size in CNNs.\\nWe ﬁnd that some heads attend to most of the image already in the lowest layers, showing that\\nthe ability to integrate information globally is indeed used by the model. Other attention heads\\nhave consistently small attention distances in the low layers. This highly localized attention is\\nless pronounced in hybrid models that apply a ResNet before the Transformer (Figure 7, right),\\nsuggesting that it may serve a similar function as early convolutional layers in CNNs. Further, the\\nattention distance increases with network depth. Globally, we ﬁnd that the model attends to image\\nregions that are semantically relevant for classiﬁcation (Figure 6).\\n4.6\\nSELF-SUPERVISION\\nTransformers show impressive performance on NLP tasks. However, much of their success stems\\nnot only from their excellent scalability but also from large scale self-supervised pre-training (Devlin\\n8'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-06-04T00:19:58+00:00', 'source': '../datas/pdf_files/ViT.pdf', 'file_path': '../datas/pdf_files/ViT.pdf', 'total_pages': 22, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-06-04T00:19:58+00:00', 'trapped': '', 'modDate': 'D:20210604001958Z', 'creationDate': 'D:20210604001958Z', 'page': 8}, page_content='Published as a conference paper at ICLR 2021\\nRGB embedding filters\\n(first 28 principal components)\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\nInput patch column\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\nInput patch row\\nPosition embedding similarity\\n1\\n1\\nCosine similarity\\n0\\n5\\n10\\n15\\n20\\nNetwork depth (layer)\\n0\\n20\\n40\\n60\\n80\\n100\\n120\\nMean attention distance (pixels)\\nViT-L/16\\nHead 1\\nHead 2\\nHead 3\\n...\\nFigure 7: Left: Filters of the initial linear embedding of RGB values of ViT-L/32. Center: Sim-\\nilarity of position embeddings of ViT-L/32. Tiles show the cosine similarity between the position\\nembedding of the patch with the indicated row and column and the position embeddings of all other\\npatches. Right: Size of attended area by head and network depth. Each dot shows the mean attention\\ndistance across images for one of 16 heads at one layer. See Appendix D.7 for details.\\net al., 2019; Radford et al., 2018). We also perform a preliminary exploration on masked patch\\nprediction for self-supervision, mimicking the masked language modeling task used in BERT. With\\nself-supervised pre-training, our smaller ViT-B/16 model achieves 79.9% accuracy on ImageNet, a\\nsigniﬁcant improvement of 2% to training from scratch, but still 4% behind supervised pre-training.\\nAppendix B.1.2 contains further details. We leave exploration of contrastive pre-training (Chen\\net al., 2020b; He et al., 2020; Bachman et al., 2019; H´enaff et al., 2020) to future work.\\n5\\nCONCLUSION\\nWe have explored the direct application of Transformers to image recognition. Unlike prior works\\nusing self-attention in computer vision, we do not introduce image-speciﬁc inductive biases into\\nthe architecture apart from the initial patch extraction step. Instead, we interpret an image as a\\nsequence of patches and process it by a standard Transformer encoder as used in NLP. This simple,\\nyet scalable, strategy works surprisingly well when coupled with pre-training on large datasets.\\nThus, Vision Transformer matches or exceeds the state of the art on many image classiﬁcation\\ndatasets, whilst being relatively cheap to pre-train.\\nWhile these initial results are encouraging, many challenges remain. One is to apply ViT to other\\ncomputer vision tasks, such as detection and segmentation. Our results, coupled with those in Carion\\net al. (2020), indicate the promise of this approach. Another challenge is to continue exploring self-\\nsupervised pre-training methods. Our initial experiments show improvement from self-supervised\\npre-training, but there is still large gap between self-supervised and large-scale supervised pre-\\ntraining. Finally, further scaling of ViT would likely lead to improved performance.\\nACKNOWLEDGEMENTS\\nThe work was performed in Berlin, Z¨urich, and Amsterdam. We thank many colleagues at Google\\nfor their help, in particular Andreas Steiner for crucial help with the infrastructure and the open-\\nsource release of the code; Joan Puigcerver and Maxim Neumann for help with the large-scale\\ntraining infrastructure; Dmitry Lepikhin, Aravindh Mahendran, Daniel Keysers, Mario Luˇci´c, Noam\\nShazeer, Ashish Vaswani, and Colin Raffel for useful discussions.\\nREFERENCES\\nSamira Abnar and Willem Zuidema. Quantifying attention ﬂow in transformers. In ACL, 2020.\\nPhilip Bachman, R Devon Hjelm, and William Buchwalter. Learning representations by maximizing\\nmutual information across views. In NeurIPS, 2019.\\n9'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-06-04T00:19:58+00:00', 'source': '../datas/pdf_files/ViT.pdf', 'file_path': '../datas/pdf_files/ViT.pdf', 'total_pages': 22, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-06-04T00:19:58+00:00', 'trapped': '', 'modDate': 'D:20210604001958Z', 'creationDate': 'D:20210604001958Z', 'page': 9}, page_content='Published as a conference paper at ICLR 2021\\nAlexei Baevski and Michael Auli. Adaptive input representations for neural language modeling. In\\nICLR, 2019.\\nI. Bello, B. Zoph, Q. Le, A. Vaswani, and J. Shlens. Attention augmented convolutional networks.\\nIn ICCV, 2019.\\nLucas Beyer, Olivier J. H´enaff, Alexander Kolesnikov, Xiaohua Zhai, and A¨aron van den Oord. Are\\nwe done with imagenet? arXiv, 2020.\\nTom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal,\\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are\\nfew-shot learners. arXiv, 2020.\\nNicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and\\nSergey Zagoruyko. End-to-end object detection with transformers. In ECCV, 2020.\\nMark Chen, Alec Radford, Rewon Child, Jeff Wu, and Heewoo Jun. Generative pretraining from\\npixels. In ICML, 2020a.\\nTing Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey E. Hinton. A simple framework\\nfor contrastive learning of visual representations. In ICML, 2020b.\\nYen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, and\\nJingjing Liu. UNITER: UNiversal Image-TExt Representation Learning. In ECCV, 2020c.\\nRewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse\\ntransformers. arXiv, 2019.\\nJean-Baptiste Cordonnier, Andreas Loukas, and Martin Jaggi. On the relationship between self-\\nattention and convolutional layers. In ICLR, 2020.\\nJ. Deng, W. Dong, R. Socher, L. Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical\\nimage database. In CVPR, 2009.\\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep\\nbidirectional transformers for language understanding. In NAACL, 2019.\\nJosip Djolonga, Jessica Yung, Michael Tschannen, Rob Romijnders, Lucas Beyer, Alexander\\nKolesnikov, Joan Puigcerver, Matthias Minderer, Alexander D’Amour, Dan Moldovan, Sylvan\\nGelly, Neil Houlsby, Xiaohua Zhai, and Mario Lucic. On robustness and transferability of convo-\\nlutional neural networks. arXiv, 2020.\\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-\\nnition. In CVPR, 2016.\\nKaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick.\\nMomentum contrast for\\nunsupervised visual representation learning. In CVPR, 2020.\\nJonathan Ho, Nal Kalchbrenner, Dirk Weissenborn, and Tim Salimans. Axial attention in multidi-\\nmensional transformers. arXiv, 2019.\\nHan Hu, Jiayuan Gu, Zheng Zhang, Jifeng Dai, and Yichen Wei. Relation networks for object\\ndetection. In CVPR, 2018.\\nHan Hu, Zheng Zhang, Zhenda Xie, and Stephen Lin. Local relation networks for image recognition.\\nIn ICCV, 2019.\\nZilong Huang, Xinggang Wang, Yunchao Wei, Lichao Huang, Humphrey Shi, Wenyu Liu, and\\nThomas S. Huang. Ccnet: Criss-cross attention for semantic segmentation. In ICCV, 2020.\\nOlivier J. H´enaff, Aravind Srinivas, Jeffrey De Fauw, Ali Razavi, Carl Doersch, S. M. Ali Eslami,\\nand Aaron van den Oord. Data-efﬁcient image recognition with contrastive predictive coding. In\\nICML, 2020.\\n10'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-06-04T00:19:58+00:00', 'source': '../datas/pdf_files/ViT.pdf', 'file_path': '../datas/pdf_files/ViT.pdf', 'total_pages': 22, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-06-04T00:19:58+00:00', 'trapped': '', 'modDate': 'D:20210604001958Z', 'creationDate': 'D:20210604001958Z', 'page': 10}, page_content='Published as a conference paper at ICLR 2021\\nSergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by\\nreducing internal covariate shift. 2015.\\nDiederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.\\nAlexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Joan Puigcerver, Jessica Yung, Sylvain Gelly,\\nand Neil Houlsby. Big transfer (BiT): General visual representation learning. In ECCV, 2020.\\nAlex Krizhevsky. Learning multiple layers of features from tiny images. Technical report, 2009.\\nAlex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. Imagenet classiﬁcation with deep convo-\\nlutional neural networks. In NIPS, 2012.\\nY. LeCun, B. Boser, J. Denker, D. Henderson, R. Howard, W. Hubbard, and L. Jackel. Backpropa-\\ngation applied to handwritten zip code recognition. Neural Computation, 1:541–551, 1989.\\nDmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang,\\nMaxim Krikun, Noam Shazeer, and Zhifeng Chen. Gshard: Scaling giant models with conditional\\ncomputation and automatic sharding. arXiv, 2020.\\nLiunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh, and Kai-Wei Chang. VisualBERT: A\\nSimple and Performant Baseline for Vision and Language. In Arxiv, 2019.\\nFrancesco Locatello, Dirk Weissenborn, Thomas Unterthiner, Aravindh Mahendran, Georg Heigold,\\nJakob Uszkoreit, Alexey Dosovitskiy, and Thomas Kipf. Object-centric learning with slot atten-\\ntion. arXiv, 2020.\\nJiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. ViLBERT: Pretraining Task-Agnostic Visi-\\nolinguistic Representations for Vision-and-Language Tasks. In NeurIPS. 2019.\\nDhruv Mahajan, Ross Girshick, Vignesh Ramanathan, Kaiming He, Manohar Paluri, Yixuan Li,\\nAshwin Bharambe, and Laurens van der Maaten.\\nExploring the limits of weakly supervised\\npretraining. In ECCV, 2018.\\nM. Nilsback and A. Zisserman. Automated ﬂower classiﬁcation over a large number of classes. In\\nICVGIP, 2008.\\nOmkar M. Parkhi, Andrea Vedaldi, Andrew Zisserman, and C. V. Jawahar. Cats and dogs. In CVPR,\\n2012.\\nNiki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz Kaiser, Noam Shazeer, Alexander Ku, and\\nDustin Tran. Image transformer. In ICML, 2018.\\nB. T. Polyak and A. B. Juditsky. Acceleration of stochastic approximation by averaging. SIAM\\nJournal on Control and Optimization, 30(4):838–855, 1992.\\ndoi: 10.1137/0330046.\\nURL\\nhttps://doi.org/10.1137/0330046.\\nSiyuan Qiao, Huiyu Wang, Chenxi Liu, Wei Shen, and Alan Yuille. Weight standardization. arXiv\\npreprint arXiv:1903.10520, 2019.\\nAlec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language under-\\nstanding with unsupervised learning. Technical Report, 2018.\\nAlec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language\\nmodels are unsupervised multitask learners. Technical Report, 2019.\\nPrajit Ramachandran, Niki Parmar, Ashish Vaswani, Irwan Bello, Anselm Levskaya, and Jon Shlens.\\nStand-alone self-attention in vision models. In NeurIPS, 2019.\\nChen Sun, Abhinav Shrivastava, Saurabh Singh, and Abhinav Gupta. Revisiting unreasonable ef-\\nfectiveness of data in deep learning era. In ICCV, 2017.\\nChen Sun, Austin Myers, Carl Vondrick, Kevin Murphy, and Cordelia Schmid. Videobert: A joint\\nmodel for video and language representation learning. In ICCV, 2019.\\n11'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-06-04T00:19:58+00:00', 'source': '../datas/pdf_files/ViT.pdf', 'file_path': '../datas/pdf_files/ViT.pdf', 'total_pages': 22, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-06-04T00:19:58+00:00', 'trapped': '', 'modDate': 'D:20210604001958Z', 'creationDate': 'D:20210604001958Z', 'page': 11}, page_content='Published as a conference paper at ICLR 2021\\nHugo Touvron, Andrea Vedaldi, Matthijs Douze, and Herve Jegou. Fixing the train-test resolution\\ndiscrepancy. In NeurIPS. 2019.\\nHugo Touvron, Andrea Vedaldi, Matthijs Douze, and Herve Jegou. Fixing the train-test resolution\\ndiscrepancy: Fixefﬁcientnet. arXiv preprint arXiv:2003.08237, 2020.\\nMichael Tschannen, Josip Djolonga, Marvin Ritter, Aravindh Mahendran, Neil Houlsby, Sylvain\\nGelly, and Mario Lucic. Self-supervised learning of video-induced visual invariances. In Pro-\\nceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June\\n2020.\\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\\nŁukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NIPS, 2017.\\nHuiyu Wang, Yukun Zhu, Bradley Green, Hartwig Adam, Alan Yuille, and Liang-Chieh Chen.\\nAxial-deeplab: Stand-alone axial-attention for panoptic segmentation. In ECCV, 2020a.\\nHuiyu Wang, Yukun Zhu, Bradley Green, Hartwig Adam, Alan Yuille, and Liang-Chieh\\nChen.\\nAxial-deeplab: Stand-alone axial-attention for panoptic segmentation.\\narXiv preprint\\narXiv:2003.07853, 2020b.\\nQiang Wang, Bei Li, Tong Xiao, Jingbo Zhu, Changliang Li, Derek F. Wong, and Lidia S. Chao.\\nLearning deep transformer models for machine translation. In ACL, 2019.\\nXiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaiming He. Non-local neural networks. In\\nCVPR, 2018.\\nDirk Weissenborn, Oscar T¨ackstr¨om, and Jakob Uszkoreit. Scaling autoregressive video models. In\\nICLR, 2019.\\nBichen Wu, Chenfeng Xu, Xiaoliang Dai, Alvin Wan, Peizhao Zhang, Masayoshi Tomizuka, Kurt\\nKeutzer, and Peter Vajda. Visual transformers: Token-based image representation and processing\\nfor computer vision. arxiv, 2020.\\nYuxin Wu and Kaiming He. Group normalization. In ECCV, 2018.\\nQizhe Xie, Minh-Thang Luong, Eduard Hovy, and Quoc V. Le. Self-training with noisy student\\nimproves imagenet classiﬁcation. In CVPR, 2020.\\nXiaohua Zhai, Avital Oliver, Alexander Kolesnikov, and Lucas Beyer. S4L: Self-Supervised Semi-\\nSupervised Learning. In ICCV, 2019a.\\nXiaohua Zhai, Joan Puigcerver, Alexander Kolesnikov, Pierre Ruyssen, Carlos Riquelme, Mario\\nLucic, Josip Djolonga, Andre Susano Pinto, Maxim Neumann, Alexey Dosovitskiy, et al. A\\nlarge-scale study of representation learning with the visual task adaptation benchmark. arXiv\\npreprint arXiv:1910.04867, 2019b.\\nHengshuang Zhao, Jiaya Jia, and Vladlen Koltun. Exploring self-attention for image recognition. In\\nCVPR, 2020.\\n12'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-06-04T00:19:58+00:00', 'source': '../datas/pdf_files/ViT.pdf', 'file_path': '../datas/pdf_files/ViT.pdf', 'total_pages': 22, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-06-04T00:19:58+00:00', 'trapped': '', 'modDate': 'D:20210604001958Z', 'creationDate': 'D:20210604001958Z', 'page': 12}, page_content='Published as a conference paper at ICLR 2021\\nModels\\nDataset\\nEpochs\\nBase LR\\nLR decay\\nWeight decay\\nDropout\\nViT-B/{16,32}\\nJFT-300M\\n7\\n8 · 10−4\\nlinear\\n0.1\\n0.0\\nViT-L/32\\nJFT-300M\\n7\\n6 · 10−4\\nlinear\\n0.1\\n0.0\\nViT-L/16\\nJFT-300M\\n7/14\\n4 · 10−4\\nlinear\\n0.1\\n0.0\\nViT-H/14\\nJFT-300M\\n14\\n3 · 10−4\\nlinear\\n0.1\\n0.0\\nR50x{1,2}\\nJFT-300M\\n7\\n10−3\\nlinear\\n0.1\\n0.0\\nR101x1\\nJFT-300M\\n7\\n8 · 10−4\\nlinear\\n0.1\\n0.0\\nR152x{1,2}\\nJFT-300M\\n7\\n6 · 10−4\\nlinear\\n0.1\\n0.0\\nR50+ViT-B/{16,32}\\nJFT-300M\\n7\\n8 · 10−4\\nlinear\\n0.1\\n0.0\\nR50+ViT-L/32\\nJFT-300M\\n7\\n2 · 10−4\\nlinear\\n0.1\\n0.0\\nR50+ViT-L/16\\nJFT-300M\\n7/14\\n4 · 10−4\\nlinear\\n0.1\\n0.0\\nViT-B/{16,32}\\nImageNet-21k\\n90\\n10−3\\nlinear\\n0.03\\n0.1\\nViT-L/{16,32}\\nImageNet-21k\\n30/90\\n10−3\\nlinear\\n0.03\\n0.1\\nViT-∗\\nImageNet\\n300\\n3 · 10−3\\ncosine\\n0.3\\n0.1\\nTable 3: Hyperparameters for training. All models are trained with a batch size of 4096 and learn-\\ning rate warmup of 10k steps. For ImageNet we found it beneﬁcial to additionally apply gradient\\nclipping at global norm 1. Training resolution is 224.\\nAPPENDIX\\nA\\nMULTIHEAD SELF-ATTENTION\\nStandard qkv self-attention (SA, Vaswani et al. (2017)) is a popular building block for neural archi-\\ntectures. For each element in an input sequence z ∈RN×D, we compute a weighted sum over all\\nvalues v in the sequence. The attention weights Aij are based on the pairwise similarity between\\ntwo elements of the sequence and their respective query qi and key kj representations.\\n[q, k, v] = zUqkv\\nUqkv ∈RD×3Dh,\\n(5)\\nA = softmax\\n\\x10\\nqk⊤/\\np\\nDh\\n\\x11\\nA ∈RN×N,\\n(6)\\nSA(z) = Av .\\n(7)\\nMultihead self-attention (MSA) is an extension of SA in which we run k self-attention operations,\\ncalled “heads”, in parallel, and project their concatenated outputs. To keep compute and number of\\nparameters constant when changing k, Dh (Eq. 5) is typically set to D/k.\\nMSA(z) = [SA1(z); SA2(z); · · · ; SAk(z)] Umsa\\nUmsa ∈Rk·Dh×D\\n(8)\\nB\\nEXPERIMENT DETAILS\\nB.1\\nTRAINING\\nTable 3 summarizes our training setups for our different models. We found strong regularization\\nto be key when training models from scratch on ImageNet. Dropout, when used, is applied after\\nevery dense layer except for the the qkv-projections and directly after adding positional- to patch\\nembeddings. Hybrid models are trained with the exact setup as their ViT counterparts. Finally, all\\ntraining is done on resolution 224.\\nB.1.1\\nFINE-TUNING\\nWe ﬁne-tune all ViT models using SGD with a momentum of 0.9. We run a small grid search over\\nlearning rates, see learning rate ranges in Table 4. To do so, we use small sub-splits from the training\\nset (10% for Pets and Flowers, 2% for CIFAR, 1% ImageNet) as development set and train on the\\nremaining data. For ﬁnal results we train on the entire training set and evaluate on the respective\\ntest data. For ﬁne-tuning ResNets and hybrid models we use the exact same setup, with the only\\nexception of ImageNet where we add another value 0.06 to the learning rate sweep. Additionally,\\n13'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-06-04T00:19:58+00:00', 'source': '../datas/pdf_files/ViT.pdf', 'file_path': '../datas/pdf_files/ViT.pdf', 'total_pages': 22, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-06-04T00:19:58+00:00', 'trapped': '', 'modDate': 'D:20210604001958Z', 'creationDate': 'D:20210604001958Z', 'page': 13}, page_content='Published as a conference paper at ICLR 2021\\nDataset\\nSteps\\nBase LR\\nImageNet\\n20 000\\n{0.003, 0.01, 0.03, 0.06}\\nCIFAR100\\n10 000\\n{0.001, 0.003, 0.01, 0.03}\\nCIFAR10\\n10 000\\n{0.001, 0.003, 0.01, 0.03}\\nOxford-IIIT Pets\\n500\\n{0.001, 0.003, 0.01, 0.03}\\nOxford Flowers-102\\n500\\n{0.001, 0.003, 0.01, 0.03}\\nVTAB (19 tasks)\\n2 500\\n0.01\\nTable 4: Hyperparameters for ﬁne-tuning. All models are ﬁne-tuned with cosine learning rate decay,\\na batch size of 512, no weight decay, and grad clipping at global norm 1. If not mentioned otherwise,\\nﬁne-tuning resolution is 384.\\nfor ResNets we also run the setup of Kolesnikov et al. (2020) and select the best results across\\nthis run and our sweep. Finally, if not mentioned otherwise, all ﬁne-tuning experiments run at 384\\nresolution (running ﬁne-tuning at different resolution than training is common practice (Kolesnikov\\net al., 2020)).\\nWhen transferring ViT models to another dataset, we remove the whole head (two linear layers) and\\nreplace it by a single, zero-initialized linear layer outputting the number of classes required by the\\ntarget dataset. We found this to be a little more robust than simply re-initializing the very last layer.\\nFor VTAB we follow the protocol in Kolesnikov et al. (2020), and use the same hyperparameter\\nsetting for all tasks. We use a learning rate of 0.01 and train for 2500 steps (Tab. 4). We chose this\\nsetting by running a small sweep over two learning rates and two schedules, and selecting the setting\\nwith the highest VTAB score on the 200-example validation sets. We follow the pre-processing used\\nin Kolesnikov et al. (2020), except that we do not use task-speciﬁc input resolutions. Instead we ﬁnd\\nthat Vision Transformer beneﬁts most from a high resolution (384 × 384) for all tasks.\\nB.1.2\\nSELF-SUPERVISION\\nWe employ the masked patch prediction objective for preliminary self-supervision experiments. To\\ndo so we corrupt 50% of patch embeddings by either replacing their embeddings with a learnable\\n[mask] embedding (80%), a random other patch embedding (10%) or just keeping them as is\\n(10%). This setup is very similar to the one used for language by Devlin et al. (2019). Finally, we\\npredict the 3-bit, mean color (i.e., 512 colors in total) of every corrupted patch using their respective\\npatch representations.\\nWe trained our self-supervised model for 1M steps (ca. 14 epochs) with batch size 4096 on JFT. We\\nuse Adam, with a base learning rate of 2·10−4, warmup of 10k steps and cosine learning rate decay.\\nAs prediction targets for pretraining we tried the following settings: 1) predicting only the mean,\\n3bit color (i.e., 1 prediction of 512 colors), 2) predicting a 4 × 4 downsized version of the 16 × 16\\npatch with 3bit colors in parallel (i.e., 16 predictions of 512 colors), 3) regression on the full patch\\nusing L2 (i.e., 256 regressions on the 3 RGB channels). Surprisingly, we found that all worked quite\\nwell, though L2 was slightly worse. We report ﬁnal results only for option 1) because it has shown\\nbest few-shot performance. We also experimented with 15% corruption rate as used by Devlin et al.\\n(2019) but results were also slightly worse on our few-shot metrics.\\nLastly, we would like to remark that our instantiation of masked patch prediction doesn’t require\\nsuch an enormous amount of pretraining nor a large dataset such as JFT in order to lead to sim-\\nilar performance gains on ImageNet classiﬁcation. That is, we observed diminishing returns on\\ndownstream performance after 100k pretraining steps, and see similar gains when pretraining on\\nImageNet.\\nC\\nADDITIONAL RESULTS\\nWe report detailed results corresponding to the ﬁgures presented in the paper. Table 5 corresponds\\nto Figure 3 from the paper and shows transfer performance of different ViT models pre-trained\\non datasets of increasing size: ImageNet, ImageNet-21k, and JFT-300M. Table 6 corresponds to\\n14'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-06-04T00:19:58+00:00', 'source': '../datas/pdf_files/ViT.pdf', 'file_path': '../datas/pdf_files/ViT.pdf', 'total_pages': 22, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-06-04T00:19:58+00:00', 'trapped': '', 'modDate': 'D:20210604001958Z', 'creationDate': 'D:20210604001958Z', 'page': 14}, page_content='Published as a conference paper at ICLR 2021\\nViT-B/16\\nViT-B/32\\nViT-L/16\\nViT-L/32\\nViT-H/14\\nImageNet\\nCIFAR-10\\n98.13\\n97.77\\n97.86\\n97.94\\n-\\nCIFAR-100\\n87.13\\n86.31\\n86.35\\n87.07\\n-\\nImageNet\\n77.91\\n73.38\\n76.53\\n71.16\\n-\\nImageNet ReaL\\n83.57\\n79.56\\n82.19\\n77.83\\n-\\nOxford Flowers-102\\n89.49\\n85.43\\n89.66\\n86.36\\n-\\nOxford-IIIT-Pets\\n93.81\\n92.04\\n93.64\\n91.35\\n-\\nImageNet-21k\\nCIFAR-10\\n98.95\\n98.79\\n99.16\\n99.13\\n99.27\\nCIFAR-100\\n91.67\\n91.97\\n93.44\\n93.04\\n93.82\\nImageNet\\n83.97\\n81.28\\n85.15\\n80.99\\n85.13\\nImageNet ReaL\\n88.35\\n86.63\\n88.40\\n85.65\\n88.70\\nOxford Flowers-102\\n99.38\\n99.11\\n99.61\\n99.19\\n99.51\\nOxford-IIIT-Pets\\n94.43\\n93.02\\n94.73\\n93.09\\n94.82\\nJFT-300M\\nCIFAR-10\\n99.00\\n98.61\\n99.38\\n99.19\\n99.50\\nCIFAR-100\\n91.87\\n90.49\\n94.04\\n92.52\\n94.55\\nImageNet\\n84.15\\n80.73\\n87.12\\n84.37\\n88.04\\nImageNet ReaL\\n88.85\\n86.27\\n89.99\\n88.28\\n90.33\\nOxford Flowers-102\\n99.56\\n99.27\\n99.56\\n99.45\\n99.68\\nOxford-IIIT-Pets\\n95.80\\n93.40\\n97.11\\n95.83\\n97.56\\nTable 5: Top1 accuracy (in %) of Vision Transformer on various datasets when pre-trained on Im-\\nageNet, ImageNet-21k or JFT300M. These values correspond to Figure 3 in the main text. Models\\nare ﬁne-tuned at 384 resolution. Note that the ImageNet results are computed without additional\\ntechniques (Polyak averaging and 512 resolution images) used to achieve results in Table 2.\\nEpochs\\nImageNet\\nImageNet ReaL\\nCIFAR-10\\nCIFAR-100\\nPets\\nFlowers\\nexaFLOPs\\nname\\nViT-B/32\\n7\\n80.73\\n86.27\\n98.61\\n90.49\\n93.40\\n99.27\\n55\\nViT-B/16\\n7\\n84.15\\n88.85\\n99.00\\n91.87\\n95.80\\n99.56\\n224\\nViT-L/32\\n7\\n84.37\\n88.28\\n99.19\\n92.52\\n95.83\\n99.45\\n196\\nViT-L/16\\n7\\n86.30\\n89.43\\n99.38\\n93.46\\n96.81\\n99.66\\n783\\nViT-L/16\\n14\\n87.12\\n89.99\\n99.38\\n94.04\\n97.11\\n99.56\\n1567\\nViT-H/14\\n14\\n88.08\\n90.36\\n99.50\\n94.71\\n97.11\\n99.71\\n4262\\nResNet50x1\\n7\\n77.54\\n84.56\\n97.67\\n86.07\\n91.11\\n94.26\\n50\\nResNet50x2\\n7\\n82.12\\n87.94\\n98.29\\n89.20\\n93.43\\n97.02\\n199\\nResNet101x1\\n7\\n80.67\\n87.07\\n98.48\\n89.17\\n94.08\\n95.95\\n96\\nResNet152x1\\n7\\n81.88\\n87.96\\n98.82\\n90.22\\n94.17\\n96.94\\n141\\nResNet152x2\\n7\\n84.97\\n89.69\\n99.06\\n92.05\\n95.37\\n98.62\\n563\\nResNet152x2\\n14\\n85.56\\n89.89\\n99.24\\n91.92\\n95.75\\n98.75\\n1126\\nResNet200x3\\n14\\n87.22\\n90.15\\n99.34\\n93.53\\n96.32\\n99.04\\n3306\\nR50x1+ViT-B/32\\n7\\n84.90\\n89.15\\n99.01\\n92.24\\n95.75\\n99.46\\n106\\nR50x1+ViT-B/16\\n7\\n85.58\\n89.65\\n99.14\\n92.63\\n96.65\\n99.40\\n274\\nR50x1+ViT-L/32\\n7\\n85.68\\n89.04\\n99.24\\n92.93\\n96.97\\n99.43\\n246\\nR50x1+ViT-L/16\\n7\\n86.60\\n89.72\\n99.18\\n93.64\\n97.03\\n99.40\\n859\\nR50x1+ViT-L/16\\n14\\n87.12\\n89.76\\n99.31\\n93.89\\n97.36\\n99.11\\n1668\\nTable 6: Detailed results of model scaling experiments. These correspond to Figure 5 in the main\\npaper. We show transfer accuracy on several datasets, as well as the pre-training compute (in ex-\\naFLOPs).\\nFigure 5 from the paper and shows the transfer performance of ViT, ResNet, and hybrid models of\\nvarying size, as well as the estimated computational cost of their pre-training.\\nD\\nADDITIONAL ANALYSES\\nD.1\\nSGD VS. ADAM FOR RESNETS\\nResNets are typically trained with SGD and our use of Adam as optimizer is quite unconventional.\\nHere we show the experiments that motivated this choice. Namely, we compare the ﬁne-tuning\\n15'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-06-04T00:19:58+00:00', 'source': '../datas/pdf_files/ViT.pdf', 'file_path': '../datas/pdf_files/ViT.pdf', 'total_pages': 22, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-06-04T00:19:58+00:00', 'trapped': '', 'modDate': 'D:20210604001958Z', 'creationDate': 'D:20210604001958Z', 'page': 15}, page_content='Published as a conference paper at ICLR 2021\\nResNet50\\nResNet152x2\\nDataset\\nAdam\\nSGD\\nAdam\\nSGD\\nImageNet\\n77.54\\n78.24\\n84.97\\n84.37\\nCIFAR10\\n97.67\\n97.46\\n99.06\\n99.07\\nCIFAR100\\n86.07\\n85.17\\n92.05\\n91.06\\nOxford-IIIT Pets\\n91.11\\n91.00\\n95.37\\n94.79\\nOxford Flowers-102\\n94.26\\n92.06\\n98.62\\n99.32\\nAverage\\n89.33\\n88.79\\n94.01\\n93.72\\nTable 7: Fine-tuning ResNet models pre-trained with Adam and SGD.\\n100\\n101\\nRelative Compute\\n0.2\\n0.3\\n0.4\\n0.5\\n0.6\\nImageNet 5shot\\nModels\\nAll\\nDepth\\nPatch size\\nWidth MLP\\nWidth\\n100\\n101\\nRelative Compute\\n0.4\\n0.5\\n0.6\\n0.7\\n0.8\\nAverage 5shot\\nModels\\nAll\\nDepth\\nPatch size\\nWidth MLP\\nWidth\\nFigure 8: Scaling different model dimensions of the Vision Transformer.\\nperformance of two ResNets – 50x1 and 152x2 – pre-trained on JFT with SGD and Adam. For\\nSGD, we use the hyperparameters recommended by Kolesnikov et al. (2020). Results are presented\\nin Table 7. Adam pre-training outperforms SGD pre-training on most datasets and on average.\\nThis justiﬁes the choice of Adam as the optimizer used to pre-train ResNets on JFT. Note that the\\nabsolute numbers are lower than those reported by Kolesnikov et al. (2020), since we pre-train only\\nfor 7 epochs, not 30.\\nD.2\\nTRANSFORMER SHAPE\\nWe ran ablations on scaling different dimensions of the Transformer architecture to ﬁnd out which\\nare best suited for scaling to very large models. Figure 8 shows 5-shot performance on ImageNet\\nfor different conﬁgurations. All conﬁgurations are based on a ViT model with 8 layers, D = 1024,\\nDMLP = 2048 and a patch size of 32, the intersection of all lines. We can see that scaling the\\ndepth results in the biggest improvements which are clearly visible up until 64 layers. However,\\ndiminishing returns are already visible after 16 layers. Interestingly, scaling the width of the net-\\nwork seems to result in the smallest changes. Decreasing the patch size and thus increasing the\\neffective sequence length shows surprisingly robust improvements without introducing parameters.\\nThese ﬁndings suggest that compute might be a better predictor of performance than the number of\\nparameters, and that scaling should emphasize depth over width if any. Overall, we ﬁnd that scaling\\nall dimensions proportionally results in robust improvements.\\nD.3\\nHEAD TYPE AND CLASS TOKEN\\nIn order to stay as close as possible to the original Transformer model, we made use of an additional\\n[class] token, which is taken as image representation. The output of this token is then trans-\\nformed into a class prediction via a small multi-layer perceptron (MLP) with tanh as non-linearity\\nin the single hidden layer.\\nThis design is inherited from the Transformer model for text, and we use it throughout the main\\npaper. An initial attempt at using only image-patch embeddings, globally average-pooling (GAP)\\nthem, followed by a linear classiﬁer—just like ResNet’s ﬁnal feature map—performed very poorly.\\nHowever, we found that this is neither due to the extra token, nor to the GAP operation. Instead,\\n16'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-06-04T00:19:58+00:00', 'source': '../datas/pdf_files/ViT.pdf', 'file_path': '../datas/pdf_files/ViT.pdf', 'total_pages': 22, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-06-04T00:19:58+00:00', 'trapped': '', 'modDate': 'D:20210604001958Z', 'creationDate': 'D:20210604001958Z', 'page': 16}, page_content='Published as a conference paper at ICLR 2021\\n0\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\nEpochs of training\\n25\\n30\\n35\\n40\\n45\\n50\\n55\\n60\\nImageNet linear 5-shot accuracy [%]\\nCLS-Token, lr=8e-4\\nGAP, lr=8e-4\\nGAP, lr=3e-4\\nFigure 9: Comparison of class-token and global average pooling classiﬁers. Both work similarly\\nwell, but require different learning-rates.\\nPos. Emb.\\nDefault/Stem\\nEvery Layer\\nEvery Layer-Shared\\nNo Pos. Emb.\\n0.61382\\nN/A\\nN/A\\n1-D Pos. Emb.\\n0.64206\\n0.63964\\n0.64292\\n2-D Pos. Emb.\\n0.64001\\n0.64046\\n0.64022\\nRel. Pos. Emb.\\n0.64032\\nN/A\\nN/A\\nTable 8: Results of the ablation study on positional embeddings with ViT-B/16 model evaluated on\\nImageNet 5-shot linear.\\nthe difference in performance is fully explained by the requirement for a different learning-rate, see\\nFigure 9.\\nD.4\\nPOSITIONAL EMBEDDING\\nWe ran ablations on different ways of encoding spatial information using positional embedding. We\\ntried the following cases:\\n• Providing no positional information: Considering the inputs as a bag of patches.\\n• 1-dimensional positional embedding: Considering the inputs as a sequence of patches in\\nthe raster order (default across all other experiments in this paper).\\n• 2-dimensional positional embedding: Considering the inputs as a grid of patches in two\\ndimensions. In this case, two sets of embeddings are learned, each for one of the axes,\\nX-embedding, and Y -embedding, each with size D/2. Then, based on the coordinate on\\nthe path in the input, we concatenate the X and Y embedding to get the ﬁnal positional\\nembedding for that patch.\\n• Relative positional embeddings: Considering the relative distance between patches to en-\\ncode the spatial information as instead of their absolute position. To do so, we use 1-\\ndimensional Relative Attention, in which we deﬁne the relative distance all possible pairs\\nof patches. Thus, for every given pair (one as query, and the other as key/value in the at-\\ntention mechanism), we have an offset pq −pk, where each offset is associated with an\\nembedding. Then, we simply run extra attention, where we use the original query (the\\ncontent of query), but use relative positional embeddings as keys. We then use the log-\\nits from the relative attention as a bias term and add it to the logits of the main attention\\n(content-based attention) before applying the softmax.\\nIn addition to different ways of encoding spatial information, we also tried different ways of in-\\ncorporating this information in our model. For the 1-dimensional and 2-dimensional positional\\nembeddings, we tried three different cases: (1) add positional embeddings to the inputs right after\\n17'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-06-04T00:19:58+00:00', 'source': '../datas/pdf_files/ViT.pdf', 'file_path': '../datas/pdf_files/ViT.pdf', 'total_pages': 22, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-06-04T00:19:58+00:00', 'trapped': '', 'modDate': 'D:20210604001958Z', 'creationDate': 'D:20210604001958Z', 'page': 17}, page_content='Published as a conference paper at ICLR 2021\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n10 11 12 13 14\\nInput patch column\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n10\\n11\\n12\\n13\\n14\\nInput patch row\\nViT-L16\\n7 epochs, LR=0.0002, WD=0.01\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n10 11 12 13 14\\nInput patch column\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n10\\n11\\n12\\n13\\n14\\nInput patch row\\nViT-L16\\n7 epochs, LR=0.0004, WD=0.1\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n10 11 12 13 14\\nInput patch column\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n10\\n11\\n12\\n13\\n14\\nInput patch row\\nViT-L16\\n14 epochs, LR=0.0004, WD=0.1\\n1\\n1\\nCosine similarity\\nFigure 10: Position embeddings of models trained with different hyperparameters.\\nthe stem of them model and before feeding the inputs to the Transformer encoder (default across\\nall other experiments in this paper); (2) learn and add positional embeddings to the inputs at the\\nbeginning of each layer; (3) add a learned positional embeddings to the inputs at the beginning of\\neach layer (shared between layers).\\nTable 8 summarizes the results from this ablation study on a ViT-B/16 model. As we can see, while\\nthere is a large gap between the performances of the model with no positional embedding and mod-\\nels with positional embedding, there is little to no difference between different ways of encoding\\npositional information. We speculate that since our Transformer encoder operates on patch-level\\ninputs, as opposed to pixel-level, the differences in how to encode spatial information is less impor-\\ntant. More precisely, in patch-level inputs, the spatial dimensions are much smaller than the original\\npixel-level inputs, e.g., 14 × 14 as opposed to 224 × 224, and learning to represent the spatial re-\\nlations in this resolution is equally easy for these different positional encoding strategies. Even so,\\nthe speciﬁc pattern of position embedding similarity learned by the network depends on the training\\nhyperparameters (Figure 10).\\n0\\n5\\n10\\n15\\n20\\nNetwork depth (layer)\\n0\\n20\\n40\\n60\\n80\\n100\\n120\\nMean attention distance (pixels)\\nViT-L/16\\nHead 1\\nHead 2\\nHead 3\\n...\\n0\\n5\\n10\\n15\\n20\\nNetwork depth (layer)\\n0\\n20\\n40\\n60\\n80\\n100\\n120\\nR50x1 + ViT-L/16\\nHead 1\\nHead 2\\nHead 3\\n...\\nFigure 11: Size of attended area by head and network depth. Attention distance was computed for\\n128 example images by averaging the distance between the query pixel and all other pixels, weighted\\nby the attention weight. Each dot shows the mean attention distance across images for one of 16\\nheads at one layer. Image width is 224 pixels.\\nD.5\\nEMPIRICAL COMPUTATIONAL COSTS\\nWe are also interested in real-world speed of the architectures on our hardware, which is not always\\nwell predicted by theoretical FLOPs due to details like lane widths and cache sizes. For this purpose,\\n18'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-06-04T00:19:58+00:00', 'source': '../datas/pdf_files/ViT.pdf', 'file_path': '../datas/pdf_files/ViT.pdf', 'total_pages': 22, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-06-04T00:19:58+00:00', 'trapped': '', 'modDate': 'D:20210604001958Z', 'creationDate': 'D:20210604001958Z', 'page': 18}, page_content='Published as a conference paper at ICLR 2021\\nwe perform timing of inference speed for the main models of interest, on a TPUv3 accelerator; the\\ndifference between inference and backprop speed is a constant model-independent factor.\\nFigure 12 (left) shows how many images one core can handle per second, across various input sizes.\\nEvery single point refers to the peak performance measured across a wide range of batch-sizes. As\\ncan be seen, the theoretical bi-quadratic scaling of ViT with image size only barely starts happening\\nfor the largest models at the largest resolutions.\\nAnother quantity of interest is the largest batch-size each model can ﬁt onto a core, larger being\\nbetter for scaling to large datasets. Figure 12 (right) shows this quantity for the same set of models.\\nThis shows that large ViT models have a clear advantage in terms of memory-efﬁciency over ResNet\\nmodels.\\n64\\n128\\n224\\n384\\n512\\nInput size [px]\\n102\\n103\\n104\\nPeak inference speed [img/sec/core]\\n64\\n128\\n224\\n384\\n512\\nInput size [px]\\n102\\n103\\nLargest per-core batch-size\\nR50x1\\nR50x2\\nViT-B/32\\nViT-L/32\\nViT-B/16\\nViT-L/16\\nViT-H/14\\nR152x4\\nFigure 12: Left: Real wall-clock timings of various architectures across input sizes. ViT models\\nhave speed comparable to similar ResNets. Right: Largest per-core batch-size ﬁtting on device with\\nvarious architectures across input sizes. ViT models are clearly more memory-efﬁcient.\\nD.6\\nAXIAL ATTENTION\\nAxial Attention (Huang et al., 2020; Ho et al., 2019) is a simple, yet effective technique to run self-\\nattention on large inputs that are organized as multidimensional tensors. The general idea of axial\\nattention is to perform multiple attention operations, each along a single axis of the input tensor,\\ninstead of applying 1-dimensional attention to the ﬂattened version of the input. In axial attention,\\neach attention mixes information along a particular axis, while keeping information along the other\\naxes independent. Along this line, Wang et al. (2020b) proposed the AxialResNet model in which\\nall the convolutions with kernel size 3 × 3 in a ResNet50 are replaced by axial self-attention, i.e.\\na row and column attention, augmented by relative positional encoding. We have implemented\\nAxialResNet as a baseline model.3.\\nMoreover, we have modiﬁed ViT to process inputs in the 2-dimensional shape, instead of a 1-\\ndimensional sequence of patches, and incorporate Axial Transformer blocks, in which instead of\\na self-attention followed by an MLP, we have a a row-self-attention plus an MLP followed by a\\ncolumn-self-attention plus an MLP.\\nFigure 13, present the performance of Axial ResNet, Axial-ViT-B/32 and Axial-ViT-B/16 on Ima-\\ngeNet 5shot linear, when pretrained on JFT dataset, verses the pretraining compute, both in terms of\\nnumber of FLOPs and inference time (example per seconds). As we can see, both Axial-ViT-B/32\\nand Axial-ViT-B/16 do better than their ViT-B counterpart in terms of performance, but it comes at\\n3Our implementation is based on the open-sourced PyTorch implementation in https://github.com/\\ncsrhddlam/axial-deeplab. In our experiments, we reproduced the scores reported in (Wang et al.,\\n2020b) in terms of accuracy, however, our implementation, similar to the open-source implementation, is very\\nslow on TPUs. Therefore, we were not able to use it for extensive large-scale experiments. These may be\\nunlocked by a carefully optimized implementation.\\n19'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-06-04T00:19:58+00:00', 'source': '../datas/pdf_files/ViT.pdf', 'file_path': '../datas/pdf_files/ViT.pdf', 'total_pages': 22, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-06-04T00:19:58+00:00', 'trapped': '', 'modDate': 'D:20210604001958Z', 'creationDate': 'D:20210604001958Z', 'page': 19}, page_content='Published as a conference paper at ICLR 2021\\n102\\nTotal compute [exaFLOPs]\\n0.500\\n0.525\\n0.550\\n0.575\\n0.600\\n0.625\\n0.650\\nImageNet 5-shot linear top-1 accuracy\\nAxialViT-B/16\\nAxialViT-B/32\\nViT-B/16\\nViT-B/32\\nResNet50\\nAxialResNet50\\n102\\n103\\nPeak inference speed [img/sec/core]\\n0.500\\n0.525\\n0.550\\n0.575\\n0.600\\n0.625\\n0.650\\nImageNet 5-shot linear top-1 accuracy\\nAxialViT-B/16\\nAxialViT-B/32\\nViT-B/16\\nViT-B/32\\nResNet50\\nAxialResNet50\\nFigure 13: Performance of Axial-Attention based models, in terms of top-1 accuracy on ImageNet\\n5-shot linear, versus their speed in terms of number of FLOPs (left) and inference time (left).\\nthe cost of more compute. This is because in Axial-ViT models, each Transformer block with global\\nself-attention is replaced by two Axial Transformer blocks, one with row and one with column self-\\nattention and although the sequence length that self-attention operates on is smaller in axial case,\\nthere is a extra MLP per Axial-ViT block. For the AxialResNet, although it looks reasonable in\\nterms of accuracy/compute trade-off (Figure 13, left), the naive implementation is extremely slow\\non TPUs (Figure 13, right).\\nD.7\\nATTENTION DISTANCE\\nTo understand how ViT uses self-attention to integrate information across the image, we analyzed\\nthe average distance spanned by attention weights at different layers (Figure 11). This “attention\\ndistance” is analogous to receptive ﬁeld size in CNNs. Average attention distance is highly variable\\nacross heads in lower layers, with some heads attending to much of the image, while others attend\\nto small regions at or near the query location. As depth increases, attention distance increases for all\\nheads. In the second half of the network, most heads attend widely across tokens.\\nD.8\\nATTENTION MAPS\\nTo compute maps of the attention from the output token to the input space (Figures 6 and 14), we\\nused Attention Rollout (Abnar & Zuidema, 2020). Brieﬂy, we averaged attention weights of ViT-\\nL/16 across all heads and then recursively multiplied the weight matrices of all layers. This accounts\\nfor the mixing of attention across tokens through all layers.\\nD.9\\nOBJECTNET RESULTS\\nWe also evaluate our ﬂagship ViT-H/14 model on the ObjectNet benchmark following the evaluation\\nsetup in Kolesnikov et al. (2020), resulting in 82.1% top-5 accuracy and 61.7% top-1 accuracy.\\nD.10\\nVTAB BREAKDOWN\\nTable 9 shows the scores attained on each of the VTAB-1k tasks.\\n20'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-06-04T00:19:58+00:00', 'source': '../datas/pdf_files/ViT.pdf', 'file_path': '../datas/pdf_files/ViT.pdf', 'total_pages': 22, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-06-04T00:19:58+00:00', 'trapped': '', 'modDate': 'D:20210604001958Z', 'creationDate': 'D:20210604001958Z', 'page': 20}, page_content='Published as a conference paper at ICLR 2021\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n10\\n11\\n12\\n13\\n14\\n15\\n16\\n17\\n18\\n19\\n20\\n21\\n22\\n23\\n24\\n25\\n26\\n27\\n28\\n29\\n30\\n31\\n32\\n33\\n34\\n35\\n36\\n37\\n38\\n39\\n40\\n41\\n42\\n43\\n44\\n45\\n46\\n47\\n48\\n49\\n50\\n51\\n52\\n53\\n54\\n55\\n56\\n57\\n58\\n59\\n60\\n61\\n62\\n63\\n64\\n65\\n66\\n67\\n68\\n69\\n70\\n71\\n72\\n73\\n74\\n75\\n76\\n77\\n78\\n79\\n80\\n81\\n82\\n83\\n84\\n85\\n86\\n87\\n88\\n89\\n90\\n91\\n92\\n93\\n94\\n95\\n96\\n97\\n98\\n99\\n100\\n101\\n102\\n103\\n104\\n105\\n106\\n107\\n108\\n109\\n110\\n111\\n112\\n113\\n114\\n115\\n116\\n117\\n118\\n119\\n120\\n121\\n122\\n123\\n124\\n125\\n126\\n127\\n128\\nFigure 14: Further example attention maps as in Figure 6 (random selection).\\n21'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-06-04T00:19:58+00:00', 'source': '../datas/pdf_files/ViT.pdf', 'file_path': '../datas/pdf_files/ViT.pdf', 'total_pages': 22, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-06-04T00:19:58+00:00', 'trapped': '', 'modDate': 'D:20210604001958Z', 'creationDate': 'D:20210604001958Z', 'page': 21}, page_content='Published as a conference paper at ICLR 2021\\nTable 9: Breakdown of VTAB-1k performance across tasks.\\nCaltech101\\nCIFAR-100\\nDTD\\nFlowers102\\nPets\\nSun397\\nSVHN\\nCamelyon\\nEuroSAT\\nResisc45\\nRetinopathy\\nClevr-Count\\nClevr-Dist\\nDMLab\\ndSpr-Loc\\ndSpr-Ori\\nKITTI-Dist\\nsNORB-Azim\\nsNORB-Elev\\nMean\\nViT-H/14 (JFT) 95.3 85.5 75.2 99.7 97.2 65.0 88.9 83.3 96.7 91.4 76.6 91.7 63.8 53.1 79.4 63.3 84.5 33.2 51.2 77.6\\nViT-L/16 (JFT) 95.4 81.9 74.3 99.7 96.7 63.5 87.4 83.6 96.5 89.7 77.1 86.4 63.1 49.7 74.5 60.5 82.2 36.2 51.1 76.3\\nViT-L/16 (I21k) 90.8 84.1 74.1 99.3 92.7 61.0 80.9 82.5 95.6 85.2 75.3 70.3 56.1 41.9 74.7 64.9 79.9 30.5 41.7 72.7\\n22'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-22T01:56:19+00:00', 'source': '../datas/pdf_files/RAG.pdf', 'file_path': '../datas/pdf_files/RAG.pdf', 'total_pages': 36, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-10-22T01:56:19+00:00', 'trapped': '', 'modDate': 'D:20241022015619Z', 'creationDate': 'D:20241022015619Z', 'page': 0}, page_content='Developing Retrieval Augmented Generation\\n(RAG) based LLM Systems from PDFs: An\\nExperience Report\\nAyman Asad Khan\\nTampere University\\nayman.khan@tuni.fi\\nMd Toufique Hasan\\nTampere University\\nmdtoufique.hasan@tuni.fi\\nKai Kristian Kemell\\nTampere University\\nkai-kristian.kemell@tuni.fi\\nJussi Rasku\\nTampere University\\njussi.rasku@tuni.fi\\nPekka Abrahamsson\\nTampere University\\npekka.abrahamsson@tuni.fi\\nAbstract. This paper presents an experience report on the develop-\\nment of Retrieval Augmented Generation (RAG) systems using PDF\\ndocuments as the primary data source. The RAG architecture combines\\ngenerative capabilities of Large Language Models (LLMs) with the preci-\\nsion of information retrieval. This approach has the potential to redefine\\nhow we interact with and augment both structured and unstructured\\nknowledge in generative models to enhance transparency, accuracy and\\ncontextuality of responses. The paper details the end-to-end pipeline,\\nfrom data collection, preprocessing, to retrieval indexing and response\\ngeneration, highlighting technical challenges and practical solutions. We\\naim to offer insights to researchers and practitioners developing similar\\nsystems using two distinct approaches:\\nOpenAI’s Assistant API with\\nGPT Series and Llama’s open-source models. The practical implications\\nof this research lie in enhancing the reliability of generative AI systems\\nin various sectors where domain specific knowledge and real time infor-\\nmation retrieval is important. The Python code used in this work is also\\navailable at: GitHub.\\nKeywords: Retrieval Augmented Generation (RAG), Large Language Models\\n(LLMs), Generative AI in Software Development, Transparent AI.\\n1\\nIntroduction\\nLarge language models (LLMs) excel at generating human like responses, but\\nbase AI models can’t keep up with the constantly evolving information within\\ndynamic sectors. They rely on static training data, leading to outdated or incom-\\nplete answers. Thus they often lack transparency and accuracy in high stakes\\narXiv:2410.15944v1  [cs.SE]  21 Oct 2024'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-22T01:56:19+00:00', 'source': '../datas/pdf_files/RAG.pdf', 'file_path': '../datas/pdf_files/RAG.pdf', 'total_pages': 36, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-10-22T01:56:19+00:00', 'trapped': '', 'modDate': 'D:20241022015619Z', 'creationDate': 'D:20241022015619Z', 'page': 1}, page_content='decision making. Retrieval Augmented Generation (RAG) presents a powerful\\nsolution to this problem. RAG systems pull in information from external data\\nsources, like PDFs, databases, or websites, grounding the generated content in\\naccurate and current data making it ideal for knowledge intensive tasks.\\nIn this report, we document our experience as a step-by-step guide to build\\nRAG systems that integrates PDF documents as the primary knowledge base.\\nWe discuss the design choice, development of system, and evaluation of the guide,\\nproviding insights into the technical challenges encountered and the practical so-\\nlutions applied. We detail our experience using both proprietary tools (OpenAI)\\nand open-source alternatives (Llama) with data security, offering guidance on\\nchoosing the right strategy. Our insights are designed to help practitioners and\\nresearchers optimize RAG models for precision, accuracy and transparency that\\nbest suites their use case.\\n2\\nBackground\\nThis section presents the theoretical background of this study. Traditional gen-\\nerative models, such as GPT, BERT, or T5 are trained on massive datasets but\\nhave a fixed internal knowledge cut off based on their training data. They can\\nonly generate black box answers based on what they know, and this limitation\\nis notable in fields where information changes rapidly and better explainabil-\\nity and traceability of responses is required, such as healthcare, legal analysis,\\ncustomer service, or technical support.\\n2.1\\nWhat is RAG?\\nThe concept of Retrieval Augmented Generation (RAG) models is built on in-\\ntegrating two core components of NLP: Information Retrieval (IR) and Natural\\nLanguage Generation (NLG). The RAG framework, first introduced by Lewis\\net al.[5] combines dense retrieval methods with large scale generative models to\\nproduce responses that are both contextually relevant and factually accurate.\\nBy explicitly retrieving relevant passages from a large corpus and augmenting\\nthis information in the generation process, RAG models enhance the factual\\ngrounding of their outputs from the up-to-date knowledge.\\nA generic workflow of Retrieval Augmented Generation (RAG) system, show-\\ncasing how it fundamentally enhances the capabilities of Large Language Models\\n(LLMs) by grounding their outputs in real-time, relevant information is illus-\\ntrated in the Fig[1]. Unlike static models which generate responses based only\\non closed-world knowledge, the RAG process is structured into the following key\\nsteps:\\n1. Data Collection:\\nThe workflow begins with the acquisition of relevant, domain specific textual\\ndata from various external sources, such as PDFs, structured documents, or\\ntext files. These documents represent raw data important for building a tai-\\nlored knowledge base that the system will query during the retrieval process\\n2'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-22T01:56:19+00:00', 'source': '../datas/pdf_files/RAG.pdf', 'file_path': '../datas/pdf_files/RAG.pdf', 'total_pages': 36, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-10-22T01:56:19+00:00', 'trapped': '', 'modDate': 'D:20241022015619Z', 'creationDate': 'D:20241022015619Z', 'page': 2}, page_content='enhancing the model’s ability to respond.\\nFig. 1: Architecture of Retrieval Augmented Generation(RAG) system.\\n2. Data Preprocessing:\\nThe collected data is then preprocessed to create manageable and meaning-\\nful chunks. Preprocessing involves cleaning the text (e.g., removing noise,\\nformatting), normalizing it, and segmenting it into smaller units, such as to-\\nkens (e.g., words or group of words), that can be easily indexed and retrieved\\nlater. This segmentation is necessary to ensure that the retrieval process is\\naccurate and efficient.\\n3. Creating Vector Embeddings:\\nAfter preprocessing, the chunks of data are transformed into vector repre-\\nsentations using embedding models (e.g., BERT, Sentence Transformers).\\nThese vector embeddings capture the semantic meaning of the text, allow-\\ning the system to perform similarity searches. The vector representations are\\nstored in a Vector Store, an indexed database optimized for fast retrieval\\nbased on similarity measures.\\n4. Retrieval of Relevant Content:\\nWhen a Query is input into the system, it is first transformed into a vec-\\ntor embedding, similar to the documents in the vector store. The Retriever\\ncomponent then performs a search within the vector store to identify and\\nretrieve the most relevant chunks of information related to the query. This\\nretrieval process ensures that the system uses the most pertinent and up-to-\\n3'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-22T01:56:19+00:00', 'source': '../datas/pdf_files/RAG.pdf', 'file_path': '../datas/pdf_files/RAG.pdf', 'total_pages': 36, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-10-22T01:56:19+00:00', 'trapped': '', 'modDate': 'D:20241022015619Z', 'creationDate': 'D:20241022015619Z', 'page': 3}, page_content='date information to respond to the query.\\n5. Augmentation of Context:\\nBy merging two knowledge streams - the fixed, general knowledge embed-\\nded in the LLM and the flexible, domain-specific information augmented on\\ndemand as an additional layer of context, aligns the Large Language Model\\n(LLM) with both established and emerging information.\\n6. Generation of Response by LLM:\\nThe context-infused prompt, consisting of the original user query combined\\nwith the retrieved relevant content is provided to a Large Language Model\\n(LLM) like GPT, T5 or Llama. The LLM then processes this augmented in-\\nput to generate a coherent response not only fluent but factually grounded.\\n7. Final Output:\\nBy moving beyond the opaque outputs of traditional models, the final output\\nof RAG systems offer several advantages: they minimize the risk of generating\\nhallucinations or outdated information, enhance interpretability by clearly\\nlinking outputs to real-world sources, enriched with relevant and accurate\\nresponses.\\nThe RAG model framework introduces a paradigm shift in Generative AI by\\ncreating glass-box models. It greatly enhanced the ability of generative models\\nto provide accurate information, especially in knowledge-intensive domains. This\\nintegration has become the backbone of many advanced NLP applications, such\\nas chatbots, virtual assistants, and automated customer service systems.[5]\\n2.2\\nWhen to Use RAG: Considerations for Practitioners\\nChoosing between fine-tuning, using Retrieval Augmented Generation (RAG),\\nor base models can be a challenging decision for practitioners. Each approach\\noffers distinct advantages depending on the context and constraints of the use\\ncase. This section aims to outline the scenarios in which each method is most\\neffective, providing a decision framework to guide practitioners in selecting the\\nappropriate strategy.\\n2.2.1\\nFine-Tuning: Domain Expertise and Customization Fine-tuning\\ninvolves training an existing large language model (LLM) on a smaller, special-\\nized dataset to refine its knowledge for a particular domain or task. This method\\nexcels in scenarios where accuracy, tone consistency, and deep understanding of\\nniche contexts are essential. For instance, fine-tuning has been shown to improve\\na model’s performance in specialized content generation, such as technical writ-\\ning, customer support, and internal knowledge systems.\\nAdvantages: Fine-tuning embeds domain specific knowledge directly into\\nthe model, reducing the dependency on external data sources. It is particularly\\n4'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-22T01:56:19+00:00', 'source': '../datas/pdf_files/RAG.pdf', 'file_path': '../datas/pdf_files/RAG.pdf', 'total_pages': 36, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-10-22T01:56:19+00:00', 'trapped': '', 'modDate': 'D:20241022015619Z', 'creationDate': 'D:20241022015619Z', 'page': 4}, page_content='effective when dealing with stable data or when the model needs to adhere to a\\nspecific tone and style.\\nDrawbacks: Fine-tuning is computationally expensive and often requires\\nsubstantial resources for initial training. Additionally, it risks overfitting if the\\ndataset is too narrow, making the model less generalizable.\\nUse Case Examples:\\n– Medical Diagnosis: A fine-tuned model on medical datasets becomes highly\\nspecialized in understanding and generating medical advice based on specific\\nterminologies and contexts.\\n– Customer Support: For a software company, fine-tuning on company-\\nspecific troubleshooting protocols ensures high-accuracy and consistent re-\\nsponses tailored to user queries.\\n2.2.2\\nRAG: Dynamic Information and Large Knowledge Bases Retrieval-\\nAugmented Generation (RAG) combines LLMs with a retrieval mechanism that\\nallows the model to access external data sources in real-time, making it suitable\\nfor scenarios requiring up-to-date or frequently changing information. RAG sys-\\ntems are valuable for handling vast knowledge bases, where embedding all the\\ninformation directly into the model would be impractical or impossible.\\nAdvantages: RAG is ideal for applications that require access to dynamic\\ninformation, ensuring responses are grounded in real-time data and minimizing\\nhallucinations. It also provides transparency, as the source of the retrieved in-\\nformation can be linked directly.\\nDrawbacks: RAG requires complex infrastructure, including vector databases\\nand effective retrieval pipelines, and can be resource-intensive during inference.\\nUse Case Examples:\\n– Financial Advisor Chatbot: Using RAG, a chatbot can pull the latest\\nmarket trends and customer-specific portfolio data to offer personalized in-\\nvestment advice.\\n– Legal Document Analysis: RAG can retrieve relevant case laws and\\nstatutes from a constantly updated database, making it suitable for legal\\napplications where accuracy and up-to-date information are critical.\\n2.2.3\\nWhen to Use Base Models Using base models(without fine-tuning\\nor RAG) is appropriate when the task requires broad generalization, low-cost\\ndeployment, or rapid prototyping. Base models can handle simple use cases like\\ngeneric customer support or basic question answering, where specialized or dy-\\nnamic information is not required.\\n5'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-22T01:56:19+00:00', 'source': '../datas/pdf_files/RAG.pdf', 'file_path': '../datas/pdf_files/RAG.pdf', 'total_pages': 36, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-10-22T01:56:19+00:00', 'trapped': '', 'modDate': 'D:20241022015619Z', 'creationDate': 'D:20241022015619Z', 'page': 5}, page_content='Advantages: No additional training is required, making it easy to deploy\\nand maintain. It is best for general purpose tasks or when exploring potential\\napplications without high upfront costs.\\nDrawbacks: Limited performance on domain specific queries or tasks that\\nneed high levels of customization.\\nTable 1: Decision Framework for Choosing Between Fine-Tuning, RAG, and\\nBase Models\\nFactors\\nFine-Tuning\\nRAG\\nBase Models\\nNature\\nof\\nthe\\nTask\\nHighly\\nspecialized\\ntasks,\\ndomain\\nspe-\\ncific language\\nDynamic tasks need-\\ning real time infor-\\nmation retrieval\\nGeneral tasks, proto-\\ntyping, broad appli-\\ncability\\nData\\nRequire-\\nments\\nStatic or proprietary\\ndata\\nthat\\nrarely\\nchanges\\nAccess to up-to-date\\nor\\nexternal\\nlarge\\nknowledge bases\\nDoes\\nnot\\nrequire\\nspecialized or up-to-\\ndate information\\nResource\\nCon-\\nstraints\\nHigh computational\\nresources needed for\\ntraining\\nHigher inference cost\\nand\\ninfrastructure\\ncomplexity\\nLow\\nresource\\nde-\\nmand,\\nquick\\nto\\ndeploy\\nPerformance\\nGoals\\nMaximizing\\npre-\\ncision\\nand\\nadapt-\\nability\\nto\\nspecific\\nlanguage\\nProviding\\naccu-\\nrate,\\ncontext-aware\\nresponses from dy-\\nnamic sources\\nOptimizing\\nspeed\\nand\\ncost\\nefficiency\\nover precision\\nTo conclude, the decision framework outlined in Table[1] offers practitioners\\na guide to selecting the most suitable method based on their project’s specific\\nneeds. Fine-Tuning is the best option for specialized, high-precision tasks with\\nstable data; RAG should be used when access to dynamic, large-scale data is\\nnecessary; and Base Models are well-suited for general-purpose use with low\\nresource requirements.\\n2.3\\nUnderstanding the Role of PDFs in RAG\\nPDFs are paramount for RAG applications because they are widely used for\\ndistributing high-value content like research papers, legal documents, technical\\nmanuals, and financial reports, all of which contain dense, detailed information\\nessential for training RAG models. PDFs come in various forms, allowing access\\nto a wide range of data types—from scientific data and technical diagrams to le-\\ngal terms and financial figures. This diversity makes PDFs an invaluable resource\\nfor extracting rich, contextually relevant information. Additionally, the consis-\\ntent formatting of PDFs ensures accurate text extraction and context preserva-\\ntion, which is fundamental for generating precise responses. PDFs also include\\nmetadata (like author, keywords, and creation date) and annotations (such as\\n6'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-22T01:56:19+00:00', 'source': '../datas/pdf_files/RAG.pdf', 'file_path': '../datas/pdf_files/RAG.pdf', 'total_pages': 36, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-10-22T01:56:19+00:00', 'trapped': '', 'modDate': 'D:20241022015619Z', 'creationDate': 'D:20241022015619Z', 'page': 6}, page_content='highlights and comments) that provide extra context, helping RAG models pri-\\noritize sections and better understand document structure, ultimately enhancing\\nretrieval and generation accuracy.\\n2.3.1\\nChallenges of Working with PDFs In RAG applications, accurate\\ntext extraction from PDFs is essential for effective retrieval and generation. How-\\never, PDFs often feature complex layouts—such as multiple columns, headers,\\nfooters, and embedded images—that complicate the extraction process. These\\ncomplexities challenge RAG systems, which rely on clean, structured text for\\nhigh-quality retrieval. Text extraction accuracy from PDFs decreases dramati-\\ncally in documents with intricate layouts, such as multi-column formats or those\\nwith numerous figures and tables. This decline necessitates advanced extraction\\ntechniques and machine learning models tailored to diverse document structures.\\nMoreover, the lack of standardization in PDF creation, including different en-\\ncoding methods and embedded fonts, can result in inconsistent or garbled text,\\nfurther complicating extraction and degrading RAG model performance. Addi-\\ntionally, many PDFs are scanned documents, especially in fields like law and\\nacademia, requiring Optical Character Recognition (OCR) to convert images to\\ntext. OCR can introduce errors, particularly with low-quality scans or hand-\\nwritten text, leading to inaccuracies that are problematic in RAG applications,\\nwhere precise input is essential for generating relevant responses. PDFs may also\\ncontain non-textual elements like charts, tables, and images, disrupting the lin-\\near text flow required by most RAG models. Handling these elements requires\\nspecialized tools and preprocessing to ensure the extracted data is coherent and\\nuseful for RAG tasks.\\n2.3.2\\nKey Considerations for PDF Processing in RAG Application\\nDevelopment Processing PDFs for Retrieval Augmented Generation (RAG)\\napplications requires careful handling to ensure high-quality text extraction,\\neffective retrieval, and accurate generation. Below are key considerations specif-\\nically tailored for PDF processing in RAG development.\\n1. Accurate Text Extraction:\\nSince PDFs can have complex formatting, it is essential to use reliable tools and\\nmethods to convert the PDF content into usable text for further processing.\\n– Appropriate Tool for Extraction: There are tools and libraries for ex-\\ntracting text from PDFs for most popular programming languages (i.e:\\npdfplumber or PyMuPDF (fitz) for Python). These libraries handle most\\ncommon PDF structures and formats, preserving the text’s layout and struc-\\nture as much as possible.\\n– Verify and Clean Extracted Text: After extracting text, always verify\\nit for completeness and correctness. This step is essential for catching any\\nextraction errors or artifacts from formatting.\\n7'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-22T01:56:19+00:00', 'source': '../datas/pdf_files/RAG.pdf', 'file_path': '../datas/pdf_files/RAG.pdf', 'total_pages': 36, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-10-22T01:56:19+00:00', 'trapped': '', 'modDate': 'D:20241022015619Z', 'creationDate': 'D:20241022015619Z', 'page': 7}, page_content='2. Effective Chunking for Retrieval:\\nPDF documents often contain large blocks of text, which can be challenging for\\nretrieval models to handle effectively. Chunking the text into smaller, contextu-\\nally coherent pieces can improve retrieval performance.\\n– Semantic Chunking: Instead of splitting text arbitrarily, use semantic\\nchunking based on logical divisions within the text, such as paragraphs or\\nsections. This ensures that each chunk retains its context, which is important\\nfor both retrieval accuracy and relevance.\\n– Dynamic Chunk Sizing: Adjust the chunk size according to the content\\ntype and the model’s input limitations. For example, scientific documents\\nmight be chunked by sections, while other types of documents could use\\nparagraphs as the primary chunking unit.\\n3. Preprocessing and Cleaning:\\nPreprocessing the extracted text is key for removing noise that could affect the\\nperformance of both retrieval and generative models. Proper cleaning ensures\\nthe text is consistent, relevant, and ready for further processing.\\n– Remove Irrelevant Content: Use regular expressions or NLP-based rules\\nto clean up non-relevant content like headers, footers, page numbers, and\\nany repeating text that doesn’t contribute to the document’s meaning.\\n– Normalize Text: Standardize the text format by converting it to lower-\\ncase, removing special characters, and trimming excessive whitespace. This\\nnormalization helps create consistent input for the retrieval models.\\n4. Utilizing PDF Metadata and Annotations:\\nPDFs often contain metadata (such as the author, title, and creation date) and\\nannotations that provide additional context, which can be valuable for retrieval\\ntasks in RAG applications.\\n– Extract Metadata: You can use tools specific to programming languages\\nlike PyMuPDF or pdfminer.six for Python to extract embedded metadata.\\nThis metadata can be used as features in retrieval models, adding an extra\\nlayer of context for more precise search results.\\n– Utilize Annotations: Extract and analyze annotations or comments within\\nPDFs to understand important or highlighted sections. This can help prior-\\nitize content in the retrieval process.\\n5. Error Handling and Reliability:\\nReliability in processing PDFs is essential for maintaining the stability and re-\\nliability of RAG applications. Implementing proper error handling and logging\\nhelps manage unexpected issues and ensures smooth operation.\\n– Implement Error Handling: Use try-except blocks to manage potential\\nerrors during PDF processing. This ensures the application continues run-\\nning smoothly and logs any issues for later analysis.\\n8'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-22T01:56:19+00:00', 'source': '../datas/pdf_files/RAG.pdf', 'file_path': '../datas/pdf_files/RAG.pdf', 'total_pages': 36, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-10-22T01:56:19+00:00', 'trapped': '', 'modDate': 'D:20241022015619Z', 'creationDate': 'D:20241022015619Z', 'page': 8}, page_content='– Use Logging for Monitoring: Implement logging to capture detailed in-\\nformation about the PDF processing steps, including successes, failures, and\\nany anomalies. This is important for debugging and optimizing the applica-\\ntion over time.\\nBy following these key considerations and best practices, we can effectively\\nprocess PDFs for RAG applications, ensuring high-quality text extraction, re-\\ntrieval, and generation. This approach ensures that your RAG models are strong,\\nefficient, and capable of delivering meaningful insights from complex PDF doc-\\numents.\\n3\\nStudy Design\\nThis section presents the methodology for building a Retrieval Augmented Gen-\\neration (RAG) system that integrates PDF documents as a primary knowledge\\nsource. This system combines the retrieval capabilities of information retrieval\\n(IR) techniques with the generative strengths of Large Language Models (LLMs)\\nto produce factually accurate and contextually relevant responses, grounded in\\ndomain-specific documents.\\nThe goal is to design and implement a RAG system that addresses the limita-\\ntions of traditional LLMs, which rely solely on static, pre-trained knowledge. By\\nincorporating real-time retrieval from domain-specific PDFs, the system aims to\\ndeliver responses that are not only contextually appropriate but also up-to-date\\nand factually reliable.\\nThe system begins with the collection of relevant PDFs, including research\\npapers, legal documents, and technical manuals, forming a specialized knowledge\\nbase. Using tools and libraries, the text is extracted, cleaned, and preprocessed\\nto remove irrelevant elements such as headers and footers. The cleaned text is\\nthen segmented into manageable chunks, ensuring efficient retrieval. These text\\nsegments are converted into vector embeddings using transformer-based models\\nlike BERT or Sentence Transformers, which capture the semantic meaning of\\nthe text. The embeddings are stored in a vector database optimized for fast\\nsimilarity-based retrieval.\\nThe RAG system architecture consists of two key components: a retriever,\\nwhich converts user queries into vector embeddings to search the vector database,\\nand a generator, which synthesizes the retrieved content into a coherent, factual\\nresponse. Two types of models are considered: OpenAI’s GPT models, accessed\\nthrough the Assistant API for ease of integration, and the open-source Llama\\nmodel, which offers greater customization for domain-specific tasks.\\nIn developing the system, several challenges are addressed, such as managing\\ncomplex PDF layouts (e.g., multi-column formats, embedded images) and main-\\ntaining retrieval efficiency as the knowledge base grows. These challenges were\\nhighlighted during a preliminary evaluation process, where participants pointed\\nout the difficulty of handling documents with irregular structures. Feedback from\\nthe evaluation also emphasized the need for improvements in text extraction and\\nchunking to ensure coherent retrieval.\\n9'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-22T01:56:19+00:00', 'source': '../datas/pdf_files/RAG.pdf', 'file_path': '../datas/pdf_files/RAG.pdf', 'total_pages': 36, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-10-22T01:56:19+00:00', 'trapped': '', 'modDate': 'D:20241022015619Z', 'creationDate': 'D:20241022015619Z', 'page': 9}, page_content='The design also incorporates the feedback from a diverse group of partici-\\npants during a workshop session, which focused on the practical aspects of imple-\\nmenting RAG systems. Their input highlighted the effectiveness of the system’s\\nreal-time retrieval capabilities, particularly in knowledge-intensive domains, and\\nunderscored the importance of refining the integration between retrieval and\\ngeneration to enhance the transparency and reliability of the system’s outputs.\\nThis design sets the foundation for a RAG system capable of addressing the\\nneeds of domains requiring precise, up-to-date information.\\n4\\nResults: Step-by-Step Guide to RAG\\n4.1\\nSetting Up the Environment\\nThis section walks you through the steps required to set up a development\\nenvironment for Retrieval Augmented Generation (RAG) on your local machine.\\nWe will cover the installation of Python, setting up a virtual environment and\\nconfiguring an IDE (VSCode).\\n4.1.1\\nInstalling Python If Python is not already installed on your machine,\\nfollow the steps below:\\n1. Download and Install Python\\n– Navigate to the official Python website: https://www.python.org/downloads/\\n– Download the latest version of Python for your operating system (Win-\\ndows, macOS, or Linux).\\n– During installation, ensure that you select the option Add Python to\\nPATH. This is important to run Python from the terminal or command\\nline.\\n– For Windows users, you can also:\\n• Click on Customize Installation.\\n• Select Add Python to environment variables.\\n• Click Install Now.\\n2. Verify the Installation\\n– Open the terminal (Command Prompt on Windows, Terminal on ma-\\ncOS/Linux).\\n– Run the following command to verify that Python is installed correctly:\\npython --version\\n– If Python is installed correctly, you should see output similar to Python\\n3.x.x.\\n4.1.2\\nSetting Up an IDE After installing Python, the next step is to set\\nup an Integrated Development Environment (IDE) to write and execute your\\nPython code. We recommend Visual Studio Code (VSCode), however you are\\nfree to choose editor of your own choice. Below are the setup instructions for\\nVSCode.\\n10'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-22T01:56:19+00:00', 'source': '../datas/pdf_files/RAG.pdf', 'file_path': '../datas/pdf_files/RAG.pdf', 'total_pages': 36, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-10-22T01:56:19+00:00', 'trapped': '', 'modDate': 'D:20241022015619Z', 'creationDate': 'D:20241022015619Z', 'page': 10}, page_content='1. Download and Install VSCode\\n– Visit the official VSCode website: https://code.visualstudio.com/.\\n– Select your operating system (Windows, macOS, or Linux) and follow\\nthe instructions for installation.\\n2. Install the Python Extension in VSCode\\n– Open VSCode.\\n– Click on the Extensions tab on the left-hand side (it looks like a square\\nwith four pieces).\\n– In the Extensions Marketplace, search for Python.\\n– Install the Python extension by Microsoft. This will allow VSCode to\\nsupport Python code.\\n4.1.3\\nSetting Up a Virtual Environment A virtual environment allows\\nyou to install libraries and dependencies specific to your project without affecting\\nother projects on your machine.\\n1. Open the Terminal in VSCode\\n– Press Ctrl + ‘ (or Cmd + ‘ on Mac) to open the terminal in VSCode.\\n– Alternatively, navigate to View - Terminal in the menu.\\n– In the terminal, use the mkdir command to create a new folder for your\\nproject. For example, to create a folder named my-new-project, type:\\nmkdir my-new-project\\n– Use the cd command to change directories and navigate to the folder\\nwhere your project is located. For example:\\ncd path/to/your/project/folder/my-new-project\\n2. Create a Virtual Environment\\n– For Windows, run the following commands:\\npython -m venv my_rag_env\\nmy_rag_env\\\\Scripts\\\\activate\\n– For Mac/Linux, run the following commands:\\npython3 -m venv my_rag_env\\nsource my_rag_env/bin/activate\\n3. Configure VSCode to Use the Virtual Environment\\n– Open the Command Palette by pressing Ctrl + Shift + P (or Cmd +\\nShift + P on Mac).\\n– Type Python: Select Interpreter in the Command Palette.\\n– Select your virtual environment, my rag env, from the list.\\n11'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-22T01:56:19+00:00', 'source': '../datas/pdf_files/RAG.pdf', 'file_path': '../datas/pdf_files/RAG.pdf', 'total_pages': 36, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-10-22T01:56:19+00:00', 'trapped': '', 'modDate': 'D:20241022015619Z', 'creationDate': 'D:20241022015619Z', 'page': 11}, page_content='With your virtual environment now configured, you are ready to install\\nproject specific dependencies and manage Python packages independently for\\neach approach. This setup allows you to create separate virtual environments\\nfor the two approaches outlined in Sections[4.2.1][4.2.2]. By isolating your de-\\npendencies, you can ensure that the OpenAI Assistant API-based[4.2.1] and\\nLlama-based [4.2.2] Retrieval Augmented Generation (RAG) systems are de-\\nveloped and managed in their respective environments without conflicts or de-\\npendency issues. This practice also helps maintain cleaner, more manageable\\ndevelopment workflows for both models, ensuring that each approach functions\\noptimally with its specific requirements.\\n4.2\\nTwo Approaches to RAG: Proprietary and Open source\\nThis section introduces a structured guide for developing Retrieval Augmented\\nGeneration (RAG) systems, focusing on two distinct approaches: using OpenAI’s\\nAssistant API (GPT Series) and an open-source Large Language Model (LLM)\\nLlama and thus divided into two subsections[4.2.1][4.2.2]. The objective is to\\nequip developers with the knowledge and practical steps necessary to implement\\nRAG systems effectively, while highlighting common mistakes and best practices\\nat each stage of the process. Each subsection is designed to provide practical\\ninsights into setup, development, integration, customization and optimization to\\ngenerate well-grounded and aligned outputs.\\nIn addition to the two primary approaches discussed in this guide there are\\nseveral alternative frameworks and methodologies for developing Retrieval Aug-\\nmented Generation (RAG) systems. Each of these options such as Cohere, AI21’s\\nJurassic-2, Google’s PaLM, and Meta’s OPT have their merits and trade-offs in\\nterms of deployment flexibility, cost, ease of use, and performance.\\nWe have selected OpenAI’s Assistant API (GPT Series) and Llama\\nfor this guide based on their wide adoption, proven capabilities, and distinct\\nstrengths in developing RAG systems. As highlighted in comparison Table[2]\\nOpenAI’s Assistant API provides a simple and developer-friendly black-box,\\nallowing quick integration and deployment without the need for extensive model\\nmanagement or infrastructure setup with high quality outputs. In contrast, as\\nan open-source model, Llama allows developers to have full control over the\\nmodel’s architecture, training data, and fine-tuning process, allowing for precise\\ncustomization to suit specific requirements such as demand control, flexibility,\\nand cost-efficiency. This combination makes these two options highly valuable\\nfor diverse RAG system development needs.\\n12'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-22T01:56:19+00:00', 'source': '../datas/pdf_files/RAG.pdf', 'file_path': '../datas/pdf_files/RAG.pdf', 'total_pages': 36, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-10-22T01:56:19+00:00', 'trapped': '', 'modDate': 'D:20241022015619Z', 'creationDate': 'D:20241022015619Z', 'page': 12}, page_content='Table 2: Comparison of RAG Approaches: OpenAI vs. Llama\\nFeature\\nOpenAI’s Assistant API\\n(GPT Series)\\nLlama\\n(Open-Source\\nLLM Model)\\nEase of Use\\nHigh. Simple API calls with\\nno model management\\nModerate.\\nRequires\\nsetup\\nand model management\\nCustomization\\nLimited to prompt engineer-\\ning and few-shot learning\\nHigh. Full access to model\\nfine-tuning and adaptation\\nCost\\nPay-per-use pricing model\\nUpfront infrastructure costs;\\nno API fees\\nDeployment Flexi-\\nbility\\nCloud-based;\\ndepends\\non\\nOpenAI’s infrastructure\\nHighly flexible; can be de-\\nployed locally or in any cloud\\nenvironment\\nPerformance\\nExcellent for a wide range of\\ngeneral NLP tasks\\nExcellent, particularly when\\nfine-tuned for specific do-\\nmains\\nSecurity and Data\\nPrivacy\\nData is processed on Ope-\\nnAI\\nservers;\\nprivacy\\ncon-\\ncerns may arise\\nFull control over data and\\nmodel; suitable for sensitive\\napplications\\nSupport\\nand\\nMaintenance\\nStrong support, documenta-\\ntion, and updates from Ope-\\nnAI\\nCommunity-driven; updates\\nand support depend on com-\\nmunity efforts\\nScalability\\nScalable through OpenAI’s\\ncloud infrastructure\\nScalable depending on in-\\nfrastructure setup\\nControl Over Up-\\ndates\\nLimited; depends on Ope-\\nnAI’s release cycle\\nFull control; users can decide\\nwhen and how to update or\\nmodify the model\\n4.2.1\\nUsing OpenAI’s Assistant API : GPT Series While the OpenAI\\nCompletion API is effective for simple text generation tasks, the Assistant API\\nis a superior choice for developing RAG systems. The Assistant API supports\\nmulti-modal operations (such as text, images, audio, and video inputs) by\\ncombining text generation with file searches, code execution, and API calls.\\nFor a RAG system, this means an assistant can retrieve documents, generate\\nvector embeddings, search for relevant content, augment user queries with addi-\\ntional context, and generate responses—all in a seamless, integrated workflow.\\nIt includes memory management across sessions, so the assistant remembers\\npast queries, retrieved documents, or instructions. Assistants can be configured\\nwith specialized instructions, behaviors, parameters other than custom tools that\\nmakes this API far more powerful for developing RAG systems.\\nThis subsection provides a step-by-step guide and code snippets to utilize\\nthe OpenAI’s File Search tool within the Assistant API, as illustrated in\\n13'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-22T01:56:19+00:00', 'source': '../datas/pdf_files/RAG.pdf', 'file_path': '../datas/pdf_files/RAG.pdf', 'total_pages': 36, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-10-22T01:56:19+00:00', 'trapped': '', 'modDate': 'D:20241022015619Z', 'creationDate': 'D:20241022015619Z', 'page': 13}, page_content='Fig. 2: Open AI’s Assistant API Workflow\\nFig[2] to implement RAG. The diagram shows how after the domain specific\\ndata ingestion of supported files (such as PDFs, DOCX, JSON, etc.), the data\\nprepocessing[2] and vectorization[3] is handled by Assistant API. These vectors\\nare stored in OpenAI Vector Store, which the File Search tool can query to\\nretrieve relevant content. The assistant then augments the context and generates\\naccurate responses based on specialized instructions and the retrieved informa-\\ntion. This integrated process is covered in detailed steps below:\\n1. Environment Setup and API Configuration\\nSetting up your environment and configuring access to OpenAI’s API is the\\nfoundational step.\\n(a) Create an OpenAI Account.\\n(b) Once logged in, navigate to the OpenAI API dashboard. Generate a New\\nProject API Key.\\n(c) Depending on your usage and plan, OpenAI may require you to set up\\nbilling information. Navigate to the Billing section in the dashboard to\\nadd your payment details. Refer to Appendix[7] for cost estimations.\\nStore your API key securely. A .env file is used to securely store environ-\\nment variables, such as your OpenAI API key.\\n(a) Set Up a New Project Folder and Virtual Environment: First,\\ncreate a new folder for your project. Ensure that a virtual environment\\nis already set up in this folder, as described in section[4.1.3].\\n14'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-22T01:56:19+00:00', 'source': '../datas/pdf_files/RAG.pdf', 'file_path': '../datas/pdf_files/RAG.pdf', 'total_pages': 36, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-10-22T01:56:19+00:00', 'trapped': '', 'modDate': 'D:20241022015619Z', 'creationDate': 'D:20241022015619Z', 'page': 14}, page_content='(b) Create a .env File: Inside your new folder, make a file called .env. This\\nfile will store your OpenAI API key.\\n(c) Add Your API Key: Open the .env file and paste your OpenAI API\\nkey in this format:\\nOPENAI_API_KEY= your_openai_api_key_here\\nBe sure to replace with your actual API key.\\n(d) Save the .env File: After adding your key, save the .env file in the\\nsame folder where you’ll keep your Python files.\\n(e) Install Necessary Python Packages: To make everything work, you\\nneed two tools: openai and python-dotenv. Open a terminal (or Com-\\nmand Prompt) and run this command to install them:\\npip\\ninstall\\npython -dotenv\\nopenai\\nIf you need specific version of these tools used for the code in GitHub\\nrepository, you can install them like this:\\npip\\ninstall\\npython -dotenv ==1.0.1\\nopenai ==1.37.2\\n(f) Create the Main Python File: In the same folder, create a new\\nfile called main.py. All the code snippets attached in this entire sec-\\ntion[4.2.1] should be implemented within this file.\\nTo interact with the OpenAI API and load environment variables, you need\\nto import the necessary libraries. The dotenv library will be used to load\\nenvironment variables from the .env file.\\nCode Example: Import Dependencies\\nimport os\\nimport\\nopenai\\nimport\\ntime\\nfrom\\ndotenv\\nimport\\nload_dotenv\\nNext, you need to load the environment variables from the .env file and set\\nup the OpenAI API client. This is important for authenticating your re-\\nquests to the OpenAI service and setting up the connection to interact with\\nOpenAI’s Assistant API.\\nCode Example: Set OpenAI API Key and LLM\\n# Load\\nenvironment\\nvariables\\nfrom .env file\\nload_dotenv ()\\n# Check if OPENAI_API_KEY is set\\nopenai_api_key = os.getenv(\" OPENAI_API_KEY \")\\nif not\\nopenai_api_key :\\nraise\\nEnvironmentError (\"Error: OPENAI_API_KEY is not\\nset in the\\nenvironment. Please set it in the .env\\nfile.\")\\n15'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-22T01:56:19+00:00', 'source': '../datas/pdf_files/RAG.pdf', 'file_path': '../datas/pdf_files/RAG.pdf', 'total_pages': 36, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-10-22T01:56:19+00:00', 'trapped': '', 'modDate': 'D:20241022015619Z', 'creationDate': 'D:20241022015619Z', 'page': 15}, page_content='# Set OpenAI key and model\\nopenai.api_key = openai_api_key\\nclient = openai.OpenAI(api_key=openai.api_key)\\nmodel_name = \"gpt -4o\"\\n# Any model\\nfrom GPT series\\n2. Understanding the Problem Domain and Data Requirements\\nTo develop an effective solution for managing and retrieving information, it’s\\nimportant to understand the problem domain and identify the specific data\\nrequirements and not just provide any data. For a deeper insight into the\\nchallenges of handling PDFs, refer to Section[2.3.1]. Given that this paper\\nfocuses on working with PDFs, it is important to emphasize the significance\\nof having relevant and clean data within these documents.\\nOrganize the Knowledge Base Files: After selecting the PDF(s) for\\nyour external knowledge base, create a folder named Upload in the project\\ndirectory, and place all the selected PDFs inside this folder.\\nCommon Mistakes and Best Practices\\nMistake: Irrelevant or inconsistent data Poor-structured data\\nin PDFs downgrades the quality of embeddings generated for Large\\nLanguage Models (LLMs), hindering them to understand and process\\nthe content more accurately.\\nBest Practice: Ensure data consistency and relevance\\nAll\\nPDFs uploaded to the vector store are consistent in format and\\nhighly relevant to the problem domain.\\nBest Practice: Use descriptive file names and metadata Us-\\ning descriptive file names and adding relevant metadata can help\\nwith debugging, maintenance, and retrieval tasks. Files should be\\nnamed in a way that reflects their content or relevance to the RAG\\nsystem.\\nThe following Python code defines a function to upload multiple PDF files\\nfrom a specified directory to OpenAI vector store, which is a common data\\nstructure used for storing and querying high-dimensional vectors, often for\\nmachine learning and AI applications. It ensures the directory and files are\\nvalid before proceeding with the upload and collects and returns the up-\\nloaded files’ IDs.\\nNOTE: The function is called to run only when a new vector store is created,\\nmeaning it won’t upload additional files to an existing vector store. You can\\nmodify the logic as needed to suit your requirements.\\nAlso, please be aware that the files are stored on external servers, such as\\n16'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-22T01:56:19+00:00', 'source': '../datas/pdf_files/RAG.pdf', 'file_path': '../datas/pdf_files/RAG.pdf', 'total_pages': 36, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-10-22T01:56:19+00:00', 'trapped': '', 'modDate': 'D:20241022015619Z', 'creationDate': 'D:20241022015619Z', 'page': 16}, page_content='OpenAI’s infrastructure. OpenAI has specific policies regarding data access\\nand usage to protect user privacy and data security. They state that they\\ndo not use customer data to train their models unless explicitly permitted by\\nthe user. For more details refer:https://openai.com/policies/privacy-policy/.\\nAdditionally, the data stored can be deleted easily when necessary either via\\ncode:https://platform.openai.com/docs/api-reference/files/delete or from the\\nuser interface by clicking the delete button here:\\nhttps://platform.openai.com/storage/files/.\\nCode Example: Upload PDF(s) to the OpenAI Vector Store\\ndef\\nupload_pdfs_to_vector_store (client , vector_store_id ,\\ndirectory_path):\\ntry:\\nif not os.path.exists( directory_path ):\\nraise\\nFileNotFoundError (f\"Error: Directory\\n’{\\ndirectory_path }’ does not exist.\")\\nif not os.listdir( directory_path ):\\nraise\\nValueError(f\"Error: Directory\\n’{\\ndirectory_path }’ is empty. No files to\\nupload.\")\\nfile_ids = {}\\n# Get all PDF file\\npaths\\nfrom the\\ndirectory\\nfile_paths = [os.path.join(directory_path , file)\\nfor file in os.listdir( directory_path ) if file\\n.endswith(\".pdf\")]\\n# Check if there are any PDFs to upload\\nif not\\nfile_paths:\\nraise\\nValueError(f\"Error: No PDF files\\nfound\\nin directory\\n’{ directory_path }’.\")\\n# Iterate\\nthrough\\neach file and upload to vector\\nstore\\nfor\\nfile_path in file_paths:\\nfile_name = os.path.basename(file_path)\\n# Upload the new file\\nwith open(file_path , \"rb\") as file:\\nuploaded_file = client.beta. vector_stores .\\nfiles.upload( vector_store_id =\\nvector_store_id , file=file)\\nprint(f\"Uploaded\\nfile: {file_name} with ID\\n: { uploaded_file .id}\")\\nfile_ids[file_name] = uploaded_file .id\\nprint(f\"All files\\nhave been\\nsuccessfully\\nuploaded\\nto vector\\nstore\\nwith ID: { vector_store_id }\")\\n17'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-22T01:56:19+00:00', 'source': '../datas/pdf_files/RAG.pdf', 'file_path': '../datas/pdf_files/RAG.pdf', 'total_pages': 36, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-10-22T01:56:19+00:00', 'trapped': '', 'modDate': 'D:20241022015619Z', 'creationDate': 'D:20241022015619Z', 'page': 17}, page_content='return\\nfile_ids\\nexcept\\nException as e:\\nprint(f\"Error\\nuploading\\nfiles to vector\\nstore: {e}\\n\")\\nreturn\\nNone\\n3. Creating and Managing Vector Stores in OpenAI\\nOpenAI Vector stores are used to store files for use by the file search tool in\\nAssistant API. This step involves initializing a vector store for storing vector\\nembeddings of documents and retrieving them when needed.\\nCode Example: Initialize a Vector Store for RAG\\n# Get/Create\\nVector\\nStore\\ndef\\nget_or_create_vector_store (client , vector_store_name ):\\nif not\\nvector_store_name :\\nraise\\nValueError(\"Error: ’vector_store_name ’ is\\nnot set. Please\\nprovide a valid\\nvector\\nstore\\nname.\")\\ntry:\\n# List all\\nexisting\\nvector\\nstores\\nvector_stores = client.beta. vector_stores .list ()\\n# Check if the vector\\nstore\\nwith the given\\nname\\nalready\\nexists\\nfor\\nvector_store in vector_stores .data:\\nif vector_store.name == vector_store_name :\\nprint(f\"Vector\\nStore\\n’{ vector_store_name }’\\nalready\\nexists\\nwith ID: {vector_store\\n.id}\")\\nreturn\\nvector_store\\n# Create a new vector\\nstore if it doesn ’t exist\\nvector_store = client.beta. vector_stores .create(\\nname= vector_store_name )\\nprint(f\"New vector\\nstore\\n’{ vector_store_name }’\\ncreated\\nwith ID: {vector_store.id}\")\\n# Upload\\nPDFs to the newly\\ncreated\\nvector\\nstore (\\nassuming ’Upload ’ is the\\ndirectory\\ncontaining\\nPDFs)\\nupload_pdfs_to_vector_store (client , vector_store.\\nid , ’Upload ’)\\nreturn\\nvector_store\\nexcept\\nException as e:\\n18'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-22T01:56:19+00:00', 'source': '../datas/pdf_files/RAG.pdf', 'file_path': '../datas/pdf_files/RAG.pdf', 'total_pages': 36, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-10-22T01:56:19+00:00', 'trapped': '', 'modDate': 'D:20241022015619Z', 'creationDate': 'D:20241022015619Z', 'page': 18}, page_content='print(f\"Error\\ncreating or retrieving\\nvector\\nstore:\\n{e}\")\\nreturn\\nNone\\nCommon Mistakes and Best Practices\\nMistake: Ignoring context and query augmentation strate-\\ngies Relying solely on the vector embeddings without considering\\nquery-specific context or augmentation can lead to suboptimal re-\\nsponses.\\nBest Practice: Augment Queries with Contextual Informa-\\ntion\\nIncorporate additional contextual information when form-\\ning queries to improve retrieval quality. Using techniques like rel-\\nevance feedback or pseudo-relevance feedback can help refine search\\nresults.[[6]][[1]]\\nBest Practice: Handle Naming Conflicts Gracefully\\nWhen\\ncreating vector stores, consider adding a timestamp or unique iden-\\ntifier to the vector store name to avoid naming conflicts and make it\\neasier to manage multiple vector stores.\\nBest Practice: Chunking strategy\\nBy default OpenAI uses a\\nmax chunk size tokens of 800 and chunk overlap tokens of 400 to\\nchunk the file(s) for Vector Stores. Properly sized chunks ensure that\\neach chunk contains a coherent and contextually meaningful piece of\\ninformation. If chunks are too large, they may contain unrelated\\ncontent, conversely, if chunks are too small, they may lack sufficient\\ncontext to be useful. Configure the variables accordingly the PDF(s).\\nOnce the functions to upload PDF file(s) and creating a vector store are\\ndefined you can call it to create Knowledge Base for your project by pro-\\nviding vector store name and store in a vector store object as shown below:\\nCode Example: Creating Vector Store Object\\nvector_store_name = \"\"\\n# Ensure\\nthis is set to a valid\\nname\\nvector_store = get_or_create_vector_store (client ,\\nvector_store_name )\\n4. Creating Assistant with Specialized Instructions\\nAfter setting up the vector store, the next step is to create an AI assistant\\nusing the OpenAI API. This assistant will be configured with specialized\\ninstructions and tools to perform RAG tasks effectively. Set the assistant\\nname, description and instructions properties accordingly. Refer to the\\n19'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-22T01:56:19+00:00', 'source': '../datas/pdf_files/RAG.pdf', 'file_path': '../datas/pdf_files/RAG.pdf', 'total_pages': 36, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-10-22T01:56:19+00:00', 'trapped': '', 'modDate': 'D:20241022015619Z', 'creationDate': 'D:20241022015619Z', 'page': 19}, page_content='best practices, if needed you can also play with the temperature and top\\np values as per the project needs for random or deterministic responses.\\nCode Example: Create and Configure Assistant\\n# Get/Create\\nAssistant\\ndef\\nget_or_create_assistant (client , model_name ,\\nvector_store_id ):\\nassistant_name = \"\"\\n# Ensure\\nthis is set to a valid\\nname\\ndescription = \"\"\\n# Ensure\\nPurpose of Assistant is set\\nhere\\ninstructions = \"\"\\n# Ensure\\nSpecialized\\nInstructions\\nfor\\nAssistant\\nand\\nConversation\\nStructure is set\\nhere)\\ntry:\\nassistants = client.beta.assistants.list ()\\nfor\\nassistant in assistants.data:\\nif assistant.name == assistant_name :\\nprint(\"AI Assistant\\nalready\\nexists\\nwith ID\\n:\" + assistant.id)\\nreturn\\nassistant\\nassistant = client.beta.assistants.create(\\nmodel=model_name ,\\nname=assistant_name ,\\ndescription=description ,\\ninstructions =instructions ,\\ntools =[{\"type\": \"file_search\"}],\\ntool_resources ={\"file_search\": {\"\\nvector_store_ids \": [ vector_store_id ]}},\\ntemperature =0.7 ,\\n# Temperature\\nfor\\nsampling\\ntop_p =0.9\\n# Nucleus\\nsampling\\nparameter\\n)\\nprint(\"New AI Assistant\\ncreated\\nwith ID:\" +\\nassistant.id)\\nreturn\\nassistant\\nexcept\\nException as e:\\nprint(f\"Error\\ncreating or retrieving\\nassistant: {e\\n}\")\\nreturn\\nNone\\nassistant = get_or_create_assistant (client , model_name ,\\nvector_store.id)\\n20'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-22T01:56:19+00:00', 'source': '../datas/pdf_files/RAG.pdf', 'file_path': '../datas/pdf_files/RAG.pdf', 'total_pages': 36, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-10-22T01:56:19+00:00', 'trapped': '', 'modDate': 'D:20241022015619Z', 'creationDate': 'D:20241022015619Z', 'page': 20}, page_content='Common Mistakes and Best Practices\\nBest Practice: Ask the Model to adopt a Persona Provide\\nspecialized context-Rich instructions that guide the assistant on\\nhow to handle queries, what tone to use (e.g., formal, friendly), and\\nwhich domains to prioritize. This ensures the assistant generates\\nmore accurate and contextually appropriate responses.\\nBest Practice: Use inner monologue or Conversation Struc-\\nture The idea of inner monologue as a part of instructions is to\\ninstruct the model to put parts of the output into a structured for-\\nmat. This help understand the reasoning process that a model uses\\nto arrive at a final answer.\\nBest Practice: Fine-Tune Model Parameters Based on Use\\nCase Adjust parameters such as temperature (controls random-\\nness) and top p (controls diversity) based on the application needs.\\nThis can impact the coherence and creativity of the assistant’s out-\\nputs. For a customer support assistant, a lower temperature may be\\npreferable for consistent responses, while a more creative application\\nmight benefit from a higher temperature.\\nBest Practice: Classifying Queries into Categories For tasks\\nin which lots of independent sets of instructions are needed to handle\\ndifferent cases, it can be beneficial to first classify the type of query\\nand to use that classification to determine which instructions are\\nneeded.\\n5. Creating Conversation Thread\\nCreating a thread, initializes a context-aware conversation session where the\\nAI assistant can interact with the user, retrieve relevant information from\\nthe vector store, and generate responses based on that context. Additionally,\\ntool resources can be attached to the Assistant API threads that are made\\navailable to the assistant’s tools in this thread.\\nThis capability is essentially important when there is a need to use the same\\nAI assistant with different tools for different threads. They can be dynami-\\ncally managed to suit the requirements for topic-specific threads, reusing the\\nsame Assistant across different contexts or overwriting assistant tools for a\\nspecific thread.\\nCode Example: Initialize a thread for conversation\\n# Create\\nthread\\nthread_conversation = {\\n\" tool_resources \": {\\n\"file_search\": {\\n\" vector_store_ids \": [vector_store.id]\\n21'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-22T01:56:19+00:00', 'source': '../datas/pdf_files/RAG.pdf', 'file_path': '../datas/pdf_files/RAG.pdf', 'total_pages': 36, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-10-22T01:56:19+00:00', 'trapped': '', 'modDate': 'D:20241022015619Z', 'creationDate': 'D:20241022015619Z', 'page': 21}, page_content='}\\n}\\n}\\nmessage_thread = client.beta.threads.create (**\\nthread_conversation )\\n6. Initiating a Run\\nA Run represents an execution on a thread.This step involves sending user\\ninput to the assistant, which then processes it using the associated resources,\\nretrieves information as needed, and returns a response that could include\\ndynamically fetched citations or data from relevant documents.\\nThis following code allows a user to ask questions to an assistant in a loop.\\nIt sends the user’s question, waits for the assistant to think and respond,\\nand then displays the response word by word. The process repeats until the\\nuser types ”exit” to quit.\\nCode Example: Interact with the LLM\\n# Interact\\nwith\\nassistant\\nwhile\\nTrue:\\nuser_input = input(\"Enter\\nyour\\nquestion (or type ’exit\\n’ to quit): \")\\nif user_input.lower () == ’exit ’:\\nprint(\"Exiting\\nthe\\nconversation. Goodbye!\")\\nbreak\\n# Add a message to the thread\\nwith the new user\\ninput\\nmessage_conversation = {\\n\"role\": \"user\",\\n\"content\": [\\n{\\n\"type\": \"text\",\\n\"text\": user_input\\n}\\n]\\n}\\nmessage_response = client.beta.threads.messages.create\\n(thread_id= message_thread .id , **\\nmessage_conversation )\\nrun = client.beta.threads.runs.create(\\nthread_id=message_thread .id ,\\nassistant_id=assistant.id\\n22'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-22T01:56:19+00:00', 'source': '../datas/pdf_files/RAG.pdf', 'file_path': '../datas/pdf_files/RAG.pdf', 'total_pages': 36, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-10-22T01:56:19+00:00', 'trapped': '', 'modDate': 'D:20241022015619Z', 'creationDate': 'D:20241022015619Z', 'page': 22}, page_content=')\\nresponse_text = \"\"\\ncitations = []\\nprocessed_message_ids = set()\\nwhile\\nTrue:\\nrun_status = client.beta.threads.runs.retrieve(run\\n.id , thread_id= message_thread .id)\\nif run_status.status == ’completed ’:\\nbreak\\nelif\\nrun_status.status == ’failed ’:\\nraise\\nException(f\"Run failed: {run_status.\\nerror}\")\\ntime.sleep (1)\\nwhile\\nTrue:\\nresponse_messages = client.beta.threads.messages.\\nlist(thread_id= message_thread .id)\\nnew_messages = [msg for msg in response_messages .\\ndata if msg.id not in\\nprocessed_message_ids ]\\nfor\\nmessage in new_messages :\\nif message.role == \"assistant\" and\\nmessage.\\ncontent:\\nmessage_content = message.content [0]. text\\nannotations = message_content .annotations\\nfor index , annotation in enumerate(\\nannotations):\\nmessage_content .value =\\nmessage_content .value.replace(\\nannotation.text , f\"[{ index }]\")\\nif file_citation := getattr(annotation\\n, \" file_citation \", None):\\ncited_file = client.files.retrieve\\n( file_citation .file_id)\\ncitations.append(f\"[{ index }] {\\ncited_file.filename}\")\\nwords = message_content .value.split ()\\nfor word in words:\\nprint(word , end=’ ’, flush=True)\\ntime.sleep (0.05)\\nprocessed_message_ids .add(message.id)\\nif any(msg.role == \"assistant\" and msg.content\\nfor\\nmsg in new_messages ):\\n23'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-22T01:56:19+00:00', 'source': '../datas/pdf_files/RAG.pdf', 'file_path': '../datas/pdf_files/RAG.pdf', 'total_pages': 36, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-10-22T01:56:19+00:00', 'trapped': '', 'modDate': 'D:20241022015619Z', 'creationDate': 'D:20241022015619Z', 'page': 23}, page_content='break\\ntime.sleep (1)\\nif citations:\\nprint(\"\\\\nSources:\", \", \".join(citations))\\nprint(\"\\\\n\")\\nThe flexibility of configuring the assistant and thread or assistant-level tools\\nOpenAI’s API makes this approach highly versatile for various use cases. By\\nfollowing the steps outlined in this section and adhering to the provided best\\npractices, developers can effectively build a powerful RAG system.\\n4.2.2\\nUsing Open-Source LLM Model: Llama We will utilize Ollama,\\nan open-source framework that implements the Llama model, to incorporate\\nLlama-based question generation capabilities within our application. By em-\\nploying Ollama, we can process user input and generate contextually relevant\\nquestions directly through the terminal, offering an efficient and scalable solution\\nfor natural language processing tasks in a local environment. This integration\\nwill enable seamless question generation without relying on external API ser-\\nvices, ensuring both privacy and computational efficiency.\\n1. Install the following Python libraries\\npip\\ninstall\\npymupdf\\nlangchain -huggingface\\nfaiss -cpu\\npip\\ninstall\\noLlama\\nsentence -transformers\\nsentencepiece\\npip\\ninstall\\nlangchain -community\\n2. Converting PDFs to Text Files\\nSave this script as pdf to text.py. This script converts PDF files in a given\\nfolder into text files. You have to create one folder at the same directory.\\nThe folder name should be Data. You have to keep your PDF files in this\\nData folder for the further process. Check the GitHub Link of the Code.\\nCode Example:\\nimport os\\nimport\\nfitz\\n# PyMuPDF\\nfor\\nreading\\nPDFs\\ndef\\nconvert_pdfs_to_text (pdf_folder , text_folder):\\nif not os.path.exists(text_folder):\\nos.makedirs(text_folder)\\nfor\\nfile_name in os.listdir(pdf_folder):\\nif file_name.endswith(\".pdf\"):\\nfile_path = os.path.join(pdf_folder , file_name\\n)\\n24'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-22T01:56:19+00:00', 'source': '../datas/pdf_files/RAG.pdf', 'file_path': '../datas/pdf_files/RAG.pdf', 'total_pages': 36, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-10-22T01:56:19+00:00', 'trapped': '', 'modDate': 'D:20241022015619Z', 'creationDate': 'D:20241022015619Z', 'page': 24}, page_content='text_file_name = os.path.splitext(file_name)\\n[0] + \".txt\"\\ntext_file_path = os.path.join(text_folder ,\\ntext_file_name )\\nwith fitz.open(file_path) as doc:\\ntext = \"\"\\nfor page in doc:\\ntext += page.get_text ()\\nwith open(text_file_path , \"w\", encoding=\"utf -8\\n\") as text_file:\\ntext_file.write(text)\\nprint(f\"Converted {file_name} to {\\ntext_file_name }\")\\nif __name__ == \"__main__\":\\npdf_folder = \"Data\"\\ntext_folder = \"DataTxt\"\\nconvert_pdfs_to_text (pdf_folder , text_folder)\\nFunctionality: Converts all PDFs in the Data folder to text files and saves\\nthem in the DataTxt folder.\\n3. Creating the FAISS Index\\nSave this script as txt to index.py. This script generates a FAISS index\\nfrom the text files in the DataTxt folder. Check the GitHub link for the Code.\\nCode Example:\\nimport os\\nfrom\\nlangchain_huggingface\\nimport\\nHuggingFaceEmbeddings\\nfrom\\nlangchain_community . vectorstores\\nimport\\nFAISS\\ndef\\nload_text_files (text_folder):\\ntexts = []\\nfor\\nfile_name in os.listdir(text_folder):\\nif file_name.endswith(\".txt\"):\\nfile_path = os.path.join(text_folder ,\\nfile_name)\\nwith open(file_path , \"r\", encoding=\"utf -8\") as\\nfile:\\ntexts.append(file.read ())\\nreturn\\ntexts\\ndef\\ncreate_faiss_index (text_folder , index_path ,\\nembedding_model =’sentence -transformers/all -MiniLM -L6 -\\nv2’):\\ntexts = load_text_files (text_folder)\\n25'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-22T01:56:19+00:00', 'source': '../datas/pdf_files/RAG.pdf', 'file_path': '../datas/pdf_files/RAG.pdf', 'total_pages': 36, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-10-22T01:56:19+00:00', 'trapped': '', 'modDate': 'D:20241022015619Z', 'creationDate': 'D:20241022015619Z', 'page': 25}, page_content='embeddings = HuggingFaceEmbeddings (model_name=\\nembedding_model )\\nvector_store = FAISS.from_texts(texts , embeddings)\\nvector_store.save_local(index_path)\\nprint(f\"FAISS\\nindex\\nsaved to {index_path}\")\\nif __name__ == \"__main__\":\\ntext_folder = \"DataTxt\"\\nindex_path = \"DataIndex\"\\ncreate_faiss_index (text_folder , index_path)\\nFunctionality: Creates a FAISS index and saves it in the DataIndex/ folder.\\nHere, sentence-transformers/all-MiniLM-L6-v2 is a compact, fast trans-\\nformer model that generates sentence embeddings for tasks like semantic\\nsearch and text similarity. It’s efficient and ideal for quick, accurate text\\nretrieval in applications like FAISS indexing.\\n4. Setting Up OLlama and Llama 3.1\\nBefore implementing the last script that handles user queries and generates\\nresponses using a Large Language Model (LLM), we need to select an open-\\nsource LLM. By using OLlama as the model runner, we can easily integrate\\nLlama 3.1 into our system.\\n(a) Step 1: Download OLlama\\nTo get started with OLlama, follow these steps:\\ni. Visit the official OLlama website.\\nii. Choose the version suitable for your operating system (Windows,\\nmacOS, or Linux).\\niii. Download and install the appropriate installer from the website.\\n(b) Step 2: Install Llama 3.1 Using OLlama\\nAfter installing OLlama, you can download and install Llama 3.1 by\\nrunning the following command in your PowerShell or CMD terminal:\\noLlama\\npull\\nLlama3 .1\\n(c) Step 3: Running Llama 3.1\\nOnce the Llama 3.1 model is installed, you can start using it by running\\na command similar to this:\\noLlama run Llama3 .1\\nThis command runs the Llama 3.1 model, allowing you to ask questions\\ndirectly in the terminal and interact with the model in real time.\\n26'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-22T01:56:19+00:00', 'source': '../datas/pdf_files/RAG.pdf', 'file_path': '../datas/pdf_files/RAG.pdf', 'total_pages': 36, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-10-22T01:56:19+00:00', 'trapped': '', 'modDate': 'D:20241022015619Z', 'creationDate': 'D:20241022015619Z', 'page': 26}, page_content='(d) Step 4: Test OLlama in VS Code\\ncreate a .bat file to avoid typing the full path each time. Open Notepad,\\nadd this:\\n@echo off\\n\"C:\\\\ path\\\\to\\\\OLlama\\\\oLlama.exe\" %*\\nReplace the path with your own. Save the file as oLlama.bat directly in\\nyour project folder.\\nIn the VS Code terminal, run the .bat file with the command:\\n.\\\\ oLlama.bat run Llama3 .1\\n5. Implementing RAG-Based Question Generation\\nSave this script as main.py. This script retrieves relevant documents using\\nFAISS and generates questions based on the retrieved context using OLlama\\nand Llama 3.1. Check the GitHub Code.\\nCode Example:\\nimport os\\nfrom\\nlangchain_community . vectorstores\\nimport\\nFAISS\\nfrom\\nlangchain_huggingface\\nimport\\nHuggingFaceEmbeddings\\nfrom\\nlangchain.prompts\\nimport\\nPromptTemplate\\nfrom\\nlangchain.chains\\nimport\\nRetrievalQA\\nfrom\\nlangchain_community .llms\\nimport\\nOLlama\\n# Load\\nFAISS\\nindex\\ndef\\nload_faiss_index (index_path , embedding_model ):\\n# Load the FAISS\\nindex\\nusing the same\\nembedding\\nmodel\\nembeddings = HuggingFaceEmbeddings (model_name=\\nembedding_model )\\nvector_store = FAISS.load_local(index_path , embeddings\\n, allow_dangerous_deserialization =True)\\nreturn\\nvector_store\\n# Create the RAG system\\nusing\\nFAISS and OLlama (Llama\\n3.1)\\ndef\\ncreate_rag_system (index_path , embedding_model =’\\nsentence -transformers/all -MiniLM -L6 -v2’, model_name=\"\\nLlama3 .1\"):\\n# Load the FAISS\\nindex\\nvector_store = load_faiss_index (index_path ,\\nembedding_model )\\n# Initialize\\nthe OLlama\\nmodel (Llama3 .1)\\nllm = OLlama(model=model_name)\\n# Create a more\\ndetailed\\nprompt\\ntemplate\\nprompt_template = \"\"\"\\n27'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-22T01:56:19+00:00', 'source': '../datas/pdf_files/RAG.pdf', 'file_path': '../datas/pdf_files/RAG.pdf', 'total_pages': 36, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-10-22T01:56:19+00:00', 'trapped': '', 'modDate': 'D:20241022015619Z', 'creationDate': 'D:20241022015619Z', 'page': 27}, page_content='You are an expert\\nassistant\\nwith\\naccess to the\\nfollowing\\ncontext\\nextracted\\nfrom\\ndocuments. Your\\njob is to answer the user ’s question as accurately\\nas possible , using the\\ncontext\\nbelow.\\nContext:\\n{context}\\nGiven\\nthis\\ninformation , please\\nprovide a comprehensive\\nand\\nrelevant\\nanswer to the\\nfollowing\\nquestion:\\nQuestion: {question}\\nIf the\\ncontext\\ndoes not\\ncontain\\nenough\\ninformation ,\\nclearly\\nstate\\nthat the\\ninformation is not\\navailable in the\\ncontext\\nprovided.\\nIf possible , provide a step -by -step\\nexplanation\\nand\\nhighlight\\nkey\\ndetails.\\n\"\"\"\\n# Create a template\\nfor\\nformatting\\nthe input for the\\nmodel\\nprompt = PromptTemplate (\\ninput_variables =[\"context\", \"question\"],\\ntemplate= prompt_template\\n)\\n# Create a RetrievalQA\\nchain\\nthat\\ncombines\\nthe vector\\nstore\\nwith the model\\nqa_chain = RetrievalQA. from_chain_type (\\nllm=llm ,\\nchain_type=\"stuff\",\\nretriever=vector_store .as_retriever (),\\nchain_type_kwargs ={\"prompt\": prompt}\\n)\\nreturn\\nqa_chain\\n# Function to run the RAG system\\nwith a user\\nquestion\\ndef\\nget_answer(question , qa_chain):\\nanswer = qa_chain.run(question)\\nreturn\\nanswer\\nif __name__ == \"__main__\":\\n# Path to the FAISS\\nindex\\ndirectory\\nindex_path = \"DataIndex\"\\n# Initialize\\nthe RAG system\\nrag_system = create_rag_system (index_path)\\n# Get user\\ninput and\\ngenerate\\nthe answer\\n28'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-22T01:56:19+00:00', 'source': '../datas/pdf_files/RAG.pdf', 'file_path': '../datas/pdf_files/RAG.pdf', 'total_pages': 36, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-10-22T01:56:19+00:00', 'trapped': '', 'modDate': 'D:20241022015619Z', 'creationDate': 'D:20241022015619Z', 'page': 28}, page_content='while\\nTrue:\\nuser_question = input(\"Ask your\\nquestion (or type\\n’exit ’ to quit): \")\\nif user_question .lower () == \"exit\":\\nprint(\"Exiting\\nthe RAG system.\")\\nbreak\\nanswer = get_answer(user_question , rag_system)\\nprint(f\"Answer: {answer}\")\\nFunctionality: The script takes a user query from the terminal. It retrieves\\nrelevant documents using FAISS. Then it generates a answer using the re-\\ntrieved context with OLlama and Llama 3.1.\\nCommon Mistakes and Best Practices\\nIncompatible embeddings\\nThe FAISS index is typically cre-\\nated using a specific embeddings model. If a different embed-\\ndings model is used during querying (e.g., a different version of\\nsentence-transformers), it may lead to retrieval mismatches or\\nerrors. Always ensure that the same model is used both during in-\\ndexing and querying.\\nModel version issues Using an incorrect or unsupported model\\nversion (e.g., referencing a non-existent version like Llama 4.5) can\\nlead to failures during the model loading process. Always verify that\\nthe model version you are using is supported and available.\\nOverly general prompts Prompts that are too broad or generic\\ncan result in vague or irrelevant responses from the model. Craft\\nprecise and targeted prompts to ensure more accurate and relevant\\nanswers.\\nIgnoring context Language models can generate incorrect or hal-\\nlucinated responses when the retrieved documents lack the necessary\\ninformation, leading the model to fill in gaps inaccurately. Always\\nensure sufficient context is provided in the query.\\nMemory leaks Extended use of FAISS and OLlama in continuous\\nloops without proper memory management can result in memory\\nleaks, gradually consuming system resources. Monitor memory usage\\nand free up resources after each loop to avoid system slowdowns.\\nModel re-initialization Reloading or re-initializing models unnec-\\nessarily can slow down your system. Reuse initialized models when-\\never possible to improve system efficiency and reduce overhead.\\nOLlama (Llama 3.1) is a local language model that runs entirely on the\\nuser’s machine, ensuring data privacy and faster response times depending\\non the system’s hardware. The accuracy of its outputs depends on the qual-\\n29'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-22T01:56:19+00:00', 'source': '../datas/pdf_files/RAG.pdf', 'file_path': '../datas/pdf_files/RAG.pdf', 'total_pages': 36, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-10-22T01:56:19+00:00', 'trapped': '', 'modDate': 'D:20241022015619Z', 'creationDate': 'D:20241022015619Z', 'page': 29}, page_content='ity of its training data, and it can be further improved by fine-tuning with\\ndomain-specific knowledge. Fine-tuning involves retraining the model with\\nspecialized datasets, allowing it to internalize specific organizational knowl-\\nedge for more precise and relevant responses. This process keeps the model\\nupdated and tailored to the user’s needs while maintaining privacy.\\n5\\nPreliminary Evaluation of the Guide\\n5.1\\nFeedback Process Overview\\nThis experience report underwent an informal evaluation process aimed at gath-\\nering feedback for the section: Using OpenAI’s Assistant API : GPT Se-\\nries.[4.2.1] Although the feedback session was not formally structured, it still\\nprovided valuable insights that helped validate the ideas presented in this sec-\\ntion and refine the guide based on it. The feedback gathered from participants\\ndemonstrates that the workshop was successful. A majority of attendees were\\nable to follow the provided guide and successfully implemented their RAG mod-\\nels by the end of the session.\\n5.2\\nParticipants\\n(a) Job Titles of Participants\\n(b) Primary Area of Expertise\\nFig. 3: Demographic Information from Participants\\nWe collected feedback from a small but diverse group of participants during a\\nworkshop. A total of 8 individuals completed a demographics form, which pro-\\nvided us with an understanding of the participants’ backgrounds and technical\\nexpertise. The group consisted of individuals with varying levels of experience\\n30'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-22T01:56:19+00:00', 'source': '../datas/pdf_files/RAG.pdf', 'file_path': '../datas/pdf_files/RAG.pdf', 'total_pages': 36, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-10-22T01:56:19+00:00', 'trapped': '', 'modDate': 'D:20241022015619Z', 'creationDate': 'D:20241022015619Z', 'page': 30}, page_content='in machine learning, natural language processing (NLP), and using tools for Re-\\ntrieval Augmented Generation (RAG). The participants although had familiarity\\nwith Python language and OpenAI models.\\n5.3\\nKey Feedback Points\\nDuring the session, participants shared their thoughts on how much their un-\\nderstanding of RAG systems improve after the workshop, which aspect of the\\nworkshop did they find most valuable, challenges they faced and what sugges-\\ntions or comments they want to provide for future improvement.Below are the\\nkey points highlighted by the participants.\\nFig. 4: Participants’ Familiarity with RAG Systems.\\nPrior to attending the workshop, the majority of participants reported a rea-\\nsonable level of familiarity with RAG systems.This indicated that the audience\\nhad a foundational understanding of the concepts presented, allowing for more in\\ndepth discussions during the workshop. After the workshop, there was a notable\\nimprovement in participants’ understanding of RAG systems.\\nFig. 5: Participants’ Improvement in Understanding RAG Systems.\\nThe majority of participants highlighted the practical coding exercises as the\\nmost valuable aspect of the workshop, which helped them better understand the\\n31'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-22T01:56:19+00:00', 'source': '../datas/pdf_files/RAG.pdf', 'file_path': '../datas/pdf_files/RAG.pdf', 'total_pages': 36, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-10-22T01:56:19+00:00', 'trapped': '', 'modDate': 'D:20241022015619Z', 'creationDate': 'D:20241022015619Z', 'page': 31}, page_content='Fig. 6: Most Valuable Aspects of the Workshop.\\nimplementation of RAG systems. Additionally, several participants mentioned\\nthe discussions with peers and instructors as a key takeaway.\\n5.4\\nIncorporating Feedback to Improve the Guide\\nThe evaluation also revealed opportunities for improvement of guide, particularly\\nin enhancing the clarity of instructions and streamlining the implementation\\nprocess. The most common issues raised were technical, most of them related\\nto copying from PDF file generating errors as number of lines. In addition to\\nthat, we implemented error handling to throw meaningful errors to the user in\\nthe code snippets provided to seamlessly run the code.\\nFig. 7: Feedback on challenges faced during the implementation of the guide\\nSeveral participants also provided suggestions for future improvement. One\\nnotable suggestion was to include a warning regarding sensitive data in OpenAI\\nvector store. The detailed comment are shown in Fig[8].\\nIn conclusion, the evaluation process proved valuable in validating the ap-\\nproach outlined in the guide. By testing it in a hands on workshop environment,\\nand in an open discussion sessions of how the developed RAG models improved\\n32'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-22T01:56:19+00:00', 'source': '../datas/pdf_files/RAG.pdf', 'file_path': '../datas/pdf_files/RAG.pdf', 'total_pages': 36, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-10-22T01:56:19+00:00', 'trapped': '', 'modDate': 'D:20241022015619Z', 'creationDate': 'D:20241022015619Z', 'page': 32}, page_content='Fig. 8: Comments and suggestions for improving the guide\\ntrustworthiness in specific scenarios, we were able to collect meaningful feed-\\nback and directly address areas of difficulty faced by practitioners. The feedback\\ndriven improvements have not only made the guide more user friendly but also\\ndemonstrated the importance of continuous iteration based on real world use.\\n6\\nDiscussion\\nPractitioners in fields like healthcare, legal analysis, and customer support, often\\nstruggle with static models that rely on outdated or limited knowledge. RAG\\nmodels provide practical solutions with pulling in real time data from provided\\nsources. The ability to explain and trace how RAG models reach their answers\\nalso builds trust where accountability and decision making based on real evidence\\nis important.\\nIn this paper, we developed a RAG guide that we tested in a workshop setting,\\nwhere participants set up and deployed RAG systems following the approaches\\nmentioned. This contribution is practical, as it helps practitioners implement\\nRAG models to address real world challenges with dynamic data and improved\\naccuracy. The guide provides users clear, actionable steps to integrate RAG into\\ntheir workflows, contributing to the growing toolkit of AI driven solutions.\\nWith that, RAG also opens new research avenues that can shape the future\\nof AI and NLP technologies. As these models and tools improve, there are many\\npotential areas for growth, such as finding better ways to search for information,\\nadapting to new data automatically, and handling more than just text (like\\nimages or audio). Recent advancements in tools and technologies have further\\naccelerated the development and deployment of RAG models. As RAG models\\ncontinue to evolve, several emerging trends are shaping the future of this field.\\n1. Haystack: An open-source framework that integrates dense and sparse re-\\ntrieval methods with large-scale language models. Haystack supports real-\\ntime search applications and can be used to develop RAG models that per-\\nform tasks such as document retrieval, question answering, and summariza-\\ntion [4].\\n2. Elasticsearch with Vector Search: Enhanced support for dense vector\\nsearch capabilities, allowing RAG models to perform more sophisticated re-\\ntrieval tasks. Elasticsearch’s integration with frameworks like Faiss enables\\n33'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-22T01:56:19+00:00', 'source': '../datas/pdf_files/RAG.pdf', 'file_path': '../datas/pdf_files/RAG.pdf', 'total_pages': 36, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-10-22T01:56:19+00:00', 'trapped': '', 'modDate': 'D:20241022015619Z', 'creationDate': 'D:20241022015619Z', 'page': 33}, page_content='hybrid retrieval systems that combine the strengths of both dense and sparse\\nsearch methods, optimizing retrieval speed and accuracy for large datasets[3].\\n3. Integration with Knowledge Graphs: Researchers are exploring ways to\\nintegrate RAG models with structured knowledge bases such as knowledge\\ngraphs. This integration aims to improve the factual accuracy and reasoning\\ncapabilities of the models, making them more reliable for knowledge-intensive\\ntasks[8].\\n4. Adaptive Learning and Continual Fine-Tuning: There is a growing\\ninterest in adaptive learning techniques that allow RAG models to contin-\\nuously fine-tune themselves based on new data and user feedback. This ap-\\nproach aims to keep models up-to-date and relevant in rapidly changing\\ninformation environments[7].\\n5. Cross-Lingual and Multimodal Capabilities: Future RAG models are\\nexpected to expand their capabilities across different languages and modal-\\nities. Incorporating cross-lingual retrieval and multimodal data processing\\ncan make RAG models more versatile and applicable to a wider range of\\nglobal and multimedia tasks[2].\\nFuture research will likely focus on enhancing their adaptability, cross-lingual\\ncapabilities, and integration with diverse data sources to address increasingly\\ncomplex information needs.\\n7\\nConclusions\\nThe development of Retrieval Augmented Generation (RAG) systems offers a\\nnew way to improve large language models by grounding their outputs in real-\\ntime, relevant information. This paper covers the main steps for building RAG\\nsystems that use PDF documents as the data source. With clear examples and\\ncode snippets, it connects theory with practice and highlights challenges like\\nhandling complex PDFs and extracting useful text. It also looks at the options\\navailable, with examples of using proprietary APIs like OpenAI’s GPT and, as\\nan alternative, open-source models like Llama 3.1, helping developers choose the\\nbest tools for their needs.\\nBy following the recommendations in this guide, developers can avoid com-\\nmon mistakes and ensure their RAG systems retrieve relevant information and\\ngenerate accurate, fact-based responses. As technology advances in adaptive\\nlearning, multi-modal capabilities, and retrieval methods, RAG systems will play\\na key role in industries like healthcare, legal research, and technical documen-\\ntation. This guide offers a solid foundation for optimizing RAG systems and\\nextending the potential of generative AI in practical applications.\\n34'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-22T01:56:19+00:00', 'source': '../datas/pdf_files/RAG.pdf', 'file_path': '../datas/pdf_files/RAG.pdf', 'total_pages': 36, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-10-22T01:56:19+00:00', 'trapped': '', 'modDate': 'D:20241022015619Z', 'creationDate': 'D:20241022015619Z', 'page': 34}, page_content='References\\n1. Avi Arampatzis, Georgios Peikos, and Symeon Symeonidis. Pseudo relevance feed-\\nback optimization. Information Retrieval Journal, 24(4–5):269–297, May 2021.\\n2. Md Chowdhury, John Smith, Rajesh Kumar, and Sang-Woo Lee. Cross-lingual and\\nmultimodal retrieval-augmented generation models. IEEE Transactions on Multi-\\nmedia, 27(2):789–802, 2024.\\n3. Elasticsearch. Integrating dense vector search in elasticsearch. Elastic Technical\\nBlog, 2023.\\n4. Haystack. The haystack framework for neural search. Haystack Project Documen-\\ntation, 2023.\\n5. Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin,\\nNaman Goyal, and Sebastian Riedel. Retrieval-augmented generation for knowledge-\\nintensive nlp tasks.\\nIn Advances in Neural Information Processing Systems\\n(NeurIPS 2020), 2020.\\n6. Hang Li, Ahmed Mourad, Shengyao Zhuang, Bevan Koopman, and Guido Zuccon.\\nPseudo relevance feedback with deep language models and dense retrievers: Suc-\\ncesses and pitfalls. ACM Transactions on Information Systems, 41(3):1–40, April\\n2023.\\n7. Percy Liang, Wen-tau Wu, Douwe Kiela, and Sebastian Riedel. Best practices for\\ntraining large language models: Lessons from the field. IEEE Transactions on Neural\\nNetworks and Learning Systems, 34(9):2115–2130, 2023.\\n8. Chenyan Xiong, Zhuyun Dai, Jamie Callan, and Jie Liu. Knowledge-enhanced lan-\\nguage models for information retrieval and beyond. IEEE Transactions on Knowl-\\nedge and Data Engineering, 36(5):1234–1247, 2024.\\n35'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-22T01:56:19+00:00', 'source': '../datas/pdf_files/RAG.pdf', 'file_path': '../datas/pdf_files/RAG.pdf', 'total_pages': 36, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-10-22T01:56:19+00:00', 'trapped': '', 'modDate': 'D:20241022015619Z', 'creationDate': 'D:20241022015619Z', 'page': 35}, page_content='Appendix\\n1. Tampere University, “Cost Estimation for RAG Application Using GPT-4o”,\\nZenodo, Sep. 2024. doi: 10.5281/zenodo.13740032.\\n36')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0eb43811",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_core.documents.base.Document"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(pdf_documents[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2ea0e4",
   "metadata": {},
   "source": [
    "## Embedding & Vector Store DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c5c1f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "# from chromadb.settings import Settings\n",
    "import uuid\n",
    "from typing import List, Tuple, Dict, Any\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a7e3e13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Embedding model: all-MiniLM-L6-v2...\n",
      "Embedding model loaded successfully. Embedding dimension: 384\n"
     ]
    }
   ],
   "source": [
    "class EmbeddingManager:\n",
    "    # Handles embedding generation using SentenceTransformer\n",
    "    def __init__(self, model_name: str = 'all-MiniLM-L6-v2'):\n",
    "        '''Initialize the embedding model. from the huggingface sentence-transformers library'''\n",
    "        self.model_name = model_name\n",
    "        self.model = None\n",
    "        self._load_model()\n",
    "\n",
    "    def _load_model(self):\n",
    "        try:\n",
    "            print(f\"Loading Embedding model: {self.model_name}...\")\n",
    "            self.model = SentenceTransformer(self.model_name)\n",
    "            print(\"Embedding model loaded successfully. Embedding dimension:\", self.model.get_sentence_embedding_dimension())\n",
    "        except Exception as e:\n",
    "            print(\"Error loading embedding model:\", str(e))\n",
    "\n",
    "    def generate_embeddings(self, texts: List[str]) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Generate embeddings for a list of texts\n",
    "        \n",
    "        Args:\n",
    "            texts: List of text strings to embed\n",
    "            \n",
    "        Returns:\n",
    "            numpy array of embeddings with shape (len(texts), embedding_dim)\n",
    "        \"\"\"\n",
    "        if not self.model:\n",
    "            raise ValueError(\"Model not loaded\")\n",
    "        \n",
    "        print(f\"Generating embeddings for {len(texts)} texts...\")\n",
    "        embeddings = self.model.encode(texts, show_progress_bar=True)\n",
    "        print(f\"Generated embeddings with shape: {embeddings.shape}\")\n",
    "        return embeddings\n",
    "\n",
    "# Initialize the EmbeddingManager\n",
    "embedding_manager = EmbeddingManager(model_name='all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "030a92cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9ad9edf0",
   "metadata": {},
   "source": [
    "## Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1cd7a57a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing ChromaDB client...\n",
      "Vector store initialized successfully. Collection name: pdf_documents\n",
      "Number of documents in collection: 188\n"
     ]
    }
   ],
   "source": [
    "class VectorStore:\n",
    "    # Manages the ChromaDB vector store\n",
    "    def __init__(self, collection_name: str = \"pdf_documents\", persist_directory: str = \"../datas/vector_store\"):\n",
    "        '''Initialize the ChromaDB client and collection.'''\n",
    "        self.collection_name = collection_name\n",
    "        self.persist_directory = persist_directory\n",
    "        self.client = None\n",
    "        self.collection = None\n",
    "        self._initialize_store()\n",
    "\n",
    "    def _initialize_store(self):\n",
    "        try:\n",
    "            print(\"Initializing ChromaDB client...\")\n",
    "            os.makedirs(self.persist_directory, exist_ok=True)\n",
    "            self.client = chromadb.PersistentClient(path=self.persist_directory)\n",
    "            \n",
    "            self.collection = self.client.get_or_create_collection(\n",
    "                name=self.collection_name,\n",
    "                metadata={\"description\": \"Collection of PDF document embeddings\"}\n",
    "                )\n",
    "            print(\"Vector store initialized successfully. Collection name:\", self.collection_name)\n",
    "            print(\"Number of documents in collection:\", self.collection.count())\n",
    "        except Exception as e:\n",
    "            print(\"Error initializing ChromaDB client:\", str(e))\n",
    "\n",
    "    def add_documents(self, documents: List[Any], embeddings: np.ndarray):\n",
    "        '''Add documents and their embeddings to the collection.'''\n",
    "        if len(documents) != len(embeddings):\n",
    "            raise ValueError(\"Number of documents and embeddings must match.\")\n",
    "        \n",
    "        print(f\"Adding {len(documents)} documents to the vector store...\")\n",
    "        ids = []\n",
    "        metadatas = []\n",
    "        documents_text = []\n",
    "        embeddings_list = []\n",
    "\n",
    "        for i, (doc, embedding) in enumerate(zip(documents, embeddings)):\n",
    "            doc_id = f\"doc_{uuid.uuid4().hex[:8]}_{i}\"\n",
    "            ids.append(doc_id)\n",
    "\n",
    "            metadata = dict(doc.metadata)\n",
    "            metadata[\"doc_index\"] = i\n",
    "            metadata[\"context_length\"] = len(doc.page_content)\n",
    "            metadatas.append(metadata)\n",
    "\n",
    "            documents_text.append(doc.page_content)\n",
    "            embeddings_list.append(embedding.tolist())\n",
    "\n",
    "        try:\n",
    "            self.collection.add(\n",
    "                ids=ids,\n",
    "                metadatas=metadatas,\n",
    "                documents=documents_text,\n",
    "                embeddings=embeddings_list\n",
    "            )\n",
    "            print(f\"Successfully added {len(documents)} documents to the vector store.\")\n",
    "            print(\"Documents added successfully. Total documents in collection:\", self.collection.count())\n",
    "        except Exception as e:\n",
    "            print(\"Error adding documents to vector store:\", str(e))\n",
    "\n",
    "vector_store = VectorStore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9a057c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pdf_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1cb88325",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings for 94 texts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 3/3 [00:10<00:00,  3.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings with shape: (94, 384)\n",
      "Adding 94 documents to the vector store...\n",
      "Successfully added 94 documents to the vector store.\n",
      "Documents added successfully. Total documents in collection: 188\n"
     ]
    }
   ],
   "source": [
    "# Convert documents to embeddings\n",
    "texts = [doc.page_content for doc in pdf_documents]\n",
    "embeddings = embedding_manager.generate_embeddings(texts)\n",
    "\n",
    "# Store in the vector db\n",
    "vector_store.add_documents(pdf_documents, embeddings)\n",
    "# texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1517e55b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ae43fea7",
   "metadata": {},
   "source": [
    "# RAG Pipeline Retriever from VectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "483f1554",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGRetreiver:\n",
    "    '''\n",
    "    Handles Query based Retrieval from the Vector Store\n",
    "    '''\n",
    "    def __init__(self, vector_store: VectorStore, embedding_manager: EmbeddingManager):\n",
    "        '''\n",
    "        Initialize the RAG Retriever with a vector store and embedding manager.\n",
    "\n",
    "        Args:\n",
    "            vector_store: Instance of VectorStore\n",
    "            embedding_manager: Instance of EmbeddingManager\n",
    "        '''\n",
    "        self.vector_store = vector_store\n",
    "        self.embedding_manager = embedding_manager\n",
    "\n",
    "    def retrieve(self, query: str, top_k: int = 5, score_threshold: float = 0.0) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Retrieve relevant documents for a query\n",
    "        \n",
    "        Args:\n",
    "            query: The search query\n",
    "            top_k: Number of top results to return\n",
    "            score_threshold: Minimum similarity score threshold\n",
    "            \n",
    "        Returns:\n",
    "            List of dictionaries containing retrieved documents and metadata\n",
    "        \"\"\"\n",
    "        print(f\"Retrieving documents for query: '{query}'\")\n",
    "        print(f\"Top K: {top_k}, Score threshold: {score_threshold}\")\n",
    "        \n",
    "        # Generate query embedding\n",
    "        query_embedding = self.embedding_manager.generate_embeddings([query])[0]\n",
    "        \n",
    "        # Search in vector store\n",
    "        try:\n",
    "            results = self.vector_store.collection.query(\n",
    "                query_embeddings=[query_embedding.tolist()],\n",
    "                n_results=top_k\n",
    "            )\n",
    "            \n",
    "            # Process results\n",
    "            retrieved_docs = []\n",
    "            \n",
    "            if results['documents'] and results['documents'][0]:\n",
    "                documents = results['documents'][0]\n",
    "                metadatas = results['metadatas'][0]\n",
    "                distances = results['distances'][0]\n",
    "                ids = results['ids'][0]\n",
    "                \n",
    "                for i, (doc_id, document, metadata, distance) in enumerate(zip(ids, documents, metadatas, distances)):\n",
    "                    # Convert distance to similarity score (ChromaDB uses cosine distance)\n",
    "                    similarity_score = 1 - distance\n",
    "                    \n",
    "                    if similarity_score >= score_threshold:\n",
    "                        retrieved_docs.append({\n",
    "                            'id': doc_id,\n",
    "                            'content': document,\n",
    "                            'metadata': metadata,\n",
    "                            'similarity_score': similarity_score,\n",
    "                            'distance': distance,\n",
    "                            'rank': i + 1\n",
    "                        })\n",
    "                \n",
    "                print(f\"Retrieved {len(retrieved_docs)} documents (after filtering)\")\n",
    "            else:\n",
    "                print(\"No documents found\")\n",
    "            \n",
    "            return retrieved_docs\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error during retrieval: {e}\")\n",
    "            return []\n",
    "        \n",
    "rag_retriever = RAGRetreiver(vector_store, embedding_manager)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c08f5252",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving documents for query: 'what is object detection'\n",
      "Top K: 5, Score threshold: 0.0\n",
      "Generating embeddings for 1 texts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 29.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings with shape: (1, 384)\n",
      "Retrieved 5 documents (after filtering)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'id': 'doc_5fee55da_18',\n",
       "  'content': 'THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION\\n19\\n[92] S. Ren, K. He, R. Girshick, and J. Sun, “Faster r-cnn: Towards real-time\\nobject detection with region proposal networks,” IEEE Trans. Pattern\\nAnal. Mach. Intell., vol. 39, no. 6, pp. 1137–1149, 2017.\\n[93] C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna, “Rethink-\\ning the inception architecture for computer vision,” in CVPR, 2016.\\n[94] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan,\\nP. Doll´ar, and C. L. Zitnick, “Microsoft coco: Common objects in\\ncontext,” in ECCV, 2014.\\n[95] S. Bell, C. Lawrence Zitnick, K. Bala, and R. Girshick, “Inside-outside\\nnet: Detecting objects in context with skip pooling and recurrent neural\\nnetworks,” in CVPR, 2016.\\n[96] A. Arnab and P. H. S. Torr, “Pixelwise instance segmentation with a\\ndynamically instantiated network,” in CVPR, 2017.\\n[97] J. Dai, K. He, and J. Sun, “Instance-aware semantic segmentation via\\nmulti-task network cascades,” in CVPR, 2016.\\n[98] Y. Li, H. Qi, J. Dai, X. Ji, and Y. Wei, “Fully convolutional instance-\\naware semantic segmentation,” in CVPR, 2017.\\n[99] M. Jaderberg, K. Simonyan, A. Zisserman, and K. Kavukcuoglu,\\n“Spatial transformer networks,” in CVPR, 2015.\\n[100] S. Brahmbhatt, H. I. Christensen, and J. Hays, “Stuffnet: Using stuffto\\nimprove object detection,” in WACV, 2017.\\n[101] T. Kong, A. Yao, Y. Chen, and F. Sun, “Hypernet: Towards accurate\\nregion proposal generation and joint object detection,” in CVPR, 2016.\\n[102] A. Pentina, V. Sharmanska, and C. H. Lampert, “Curriculum learning\\nof multiple tasks,” in CVPR, 2015.\\n[103] J. Yim, H. Jung, B. Yoo, C. Choi, D. Park, and J. Kim, “Rotating your\\nface using multi-task deep neural network,” in CVPR, 2015.\\n[104] J. Li, X. Liang, J. Li, T. Xu, J. Feng, and S. Yan, “Multi-stage object\\ndetection with group recursive learning,” arXiv:1608.05159, 2016.\\n[105] Z. Cai, Q. Fan, R. S. Feris, and N. Vasconcelos, “A uniﬁed multi-scale\\ndeep convolutional neural network for fast object detection,” in ECCV,\\n2016.\\n[106] Y. Zhu, R. Urtasun, R. Salakhutdinov, and S. Fidler, “segdeepm:\\nExploiting segmentation and context in deep neural networks for object\\ndetection,” in CVPR, 2015.\\n[107] W. Byeon, T. M. Breuel, F. Raue, and M. Liwicki, “Scene labeling\\nwith lstm recurrent neural networks,” in CVPR, 2015.\\n[108] B. Moysset, C. Kermorvant, and C. Wolf, “Learning to detect and\\nlocalize many objects from few examples,” arXiv:1611.05664, 2016.\\n[109] X. Zeng, W. Ouyang, B. Yang, J. Yan, and X. Wang, “Gated bi-\\ndirectional cnn for object detection,” in ECCV, 2016.\\n[110] S. Gidaris and N. Komodakis, “Object detection via a multi-region and\\nsemantic segmentation-aware cnn model,” in CVPR, 2015.\\n[111] M. Schuster and K. K. Paliwal, “Bidirectional recurrent neural net-\\nworks,” IEEE Trans. Signal Process., vol. 45, pp. 2673–2681, 1997.\\n[112] S. Zagoruyko, A. Lerer, T.-Y. Lin, P. O. Pinheiro, S. Gross, S. Chin-\\ntala, and P. Doll´ar, “A multipath network for object detection,”\\narXiv:1604.02135, 2016.\\n[113] A. Shrivastava, A. Gupta, and R. Girshick, “Training region-based\\nobject detectors with online hard example mining,” in CVPR, 2016.\\n[114] S. Ren, K. He, R. Girshick, X. Zhang, and J. Sun, “Object detection\\nnetworks on convolutional feature maps,” IEEE Trans. Pattern Anal.\\nMach. Intell., vol. 39, no. 7, pp. 1476–1481, 2017.\\n[115] W. Ouyang, X. Wang, C. Zhang, and X. Yang, “Factors in ﬁnetuning\\ndeep model for object detection with long-tail distribution,” in CVPR,\\n2016.\\n[116] S. Hong, B. Roh, K.-H. Kim, Y. Cheon, and M. Park, “Pvanet:\\nLightweight deep neural networks for real-time object detection,”\\narXiv:1611.08588, 2016.\\n[117] W. Shang, K. Sohn, D. Almeida, and H. Lee, “Understanding and\\nimproving convolutional neural networks via concatenated rectiﬁed\\nlinear units,” in ICML, 2016.\\n[118] C. Szegedy, A. Toshev, and D. Erhan, “Deep neural networks for object\\ndetection,” in NIPS, 2013.\\n[119] P. O. Pinheiro, R. Collobert, and P. Doll´ar, “Learning to segment object\\ncandidates,” in NIPS, 2015.\\n[120] C. Szegedy, S. Reed, D. Erhan, D. Anguelov, and S. Ioffe, “Scalable,\\nhigh-quality object detection,” arXiv:1412.1441, 2014.\\n[121] M. Everingham, L. Van Gool, C. Williams, J. Winn, and A. Zisserman,\\n“The pascal visual object classes challenge 2012 (voc2012) results\\n(2012),” in http://www.pascal-network.org/challenges/VOC/voc2011/\\nworkshop/index.html, 2011.\\n[122] M. D. Zeiler and R. Fergus, “Visualizing and understanding convolu-\\ntional networks,” in ECCV, 2014.\\n[123] S. Xie, R. B. Girshick, P. Doll´ar, Z. Tu, and K. He, “Aggregated residual\\ntransformations for deep neural networks,” in CVPR, 2017.\\n[124] J. Dai, H. Qi, Y. Xiong, Y. Li, G. Zhang, H. Hu, and Y. Wei,\\n“Deformable convolutional networks,” arXiv:1703.06211, 2017.\\n[125] C. Rother, L. Bordeaux, Y. Hamadi, and A. Blake, “Autocollage,” ACM\\nTrans. on Graphics, vol. 25, no. 3, pp. 847–852, 2006.\\n[126] C. Jung and C. Kim, “A uniﬁed spectral-domain approach for saliency\\ndetection and its application to automatic object segmentation,” IEEE\\nTrans. Image Process., vol. 21, no. 3, pp. 1272–1283, 2012.\\n[127] W.-C. Tu, S. He, Q. Yang, and S.-Y. Chien, “Real-time salient object\\ndetection with a minimum spanning tree,” in CVPR, 2016.\\n[128] J. Yang and M.-H. Yang, “Top-down visual saliency via joint crf and\\ndictionary learning,” IEEE Trans. Pattern Anal. Mach. Intell., vol. 39,\\nno. 3, pp. 576–588, 2017.\\n[129] P. L. Rosin, “A simple method for detecting salient regions,” Pattern\\nRecognition, vol. 42, no. 11, pp. 2363–2371, 2009.\\n[130] T. Liu, Z. Yuan, J. Sun, J. Wang, N. Zheng, X. Tang, and H.-Y. Shum,\\n“Learning to detect a salient object,” IEEE Trans. Pattern Anal. Mach.\\nIntell., vol. 33, no. 2, pp. 353–367, 2011.\\n[131] J. Long, E. Shelhamer, and T. Darrell, “Fully convolutional networks\\nfor semantic segmentation,” in CVPR, 2015.\\n[132] D. Gao, S. Han, and N. Vasconcelos, “Discriminant saliency, the detec-\\ntion of suspicious coincidences, and applications to visual recognition,”\\nIEEE Trans. Pattern Anal. Mach. Intell., vol. 31, pp. 989–1005, 2009.\\n[133] S. Xie and Z. Tu, “Holistically-nested edge detection,” in ICCV, 2015.\\n[134] M. K¨ummerer, L. Theis, and M. Bethge, “Deep gaze i: Boost-\\ning saliency prediction with feature maps trained on imagenet,”\\narXiv:1411.1045, 2014.\\n[135] X. Huang, C. Shen, X. Boix, and Q. Zhao, “Salicon: Reducing the\\nsemantic gap in saliency prediction by adapting deep neural networks,”\\nin ICCV, 2015.\\n[136] L. Wang, H. Lu, X. Ruan, and M.-H. Yang, “Deep networks for saliency\\ndetection via local estimation and global search,” in CVPR, 2015.\\n[137] H. Cholakkal, J. Johnson, and D. Rajan, “Weakly supervised top-down\\nsalient object detection,” arXiv:1611.05345, 2016.\\n[138] R. Zhao, W. Ouyang, H. Li, and X. Wang, “Saliency detection by\\nmulti-context deep learning,” in CVPR, 2015.\\n[139] C¸ . Bak, A. Erdem, and E. Erdem, “Two-stream convolutional networks\\nfor dynamic saliency prediction,” arXiv:1607.04730, 2016.\\n[140] S. He, R. W. Lau, W. Liu, Z. Huang, and Q. Yang, “Supercnn: A su-\\nperpixelwise convolutional neural network for salient object detection,”\\nInt. J. of Comput. Vision, vol. 115, no. 3, pp. 330–344, 2015.\\n[141] X. Li, L. Zhao, L. Wei, M.-H. Yang, F. Wu, Y. Zhuang, H. Ling, and\\nJ. Wang, “Deepsaliency: Multi-task deep neural network model for\\nsalient object detection,” IEEE Trans. Image Process., vol. 25, no. 8,\\npp. 3919–3930, 2016.\\n[142] Y. Tang and X. Wu, “Saliency detection via combining region-level\\nand pixel-level predictions with cnns,” in ECCV, 2016.\\n[143] G. Li and Y. Yu, “Deep contrast learning for salient object detection,”\\nin CVPR, 2016.\\n[144] X. Wang, H. Ma, S. You, and X. Chen, “Edge preserving and\\nmulti-scale contextual neural network for salient object detection,”\\narXiv:1608.08029, 2016.\\n[145] M. Cornia, L. Baraldi, G. Serra, and R. Cucchiara, “A deep multi-level\\nnetwork for saliency prediction,” in ICPR, 2016.\\n[146] G. Li and Y. Yu, “Visual saliency detection based on multiscale deep\\ncnn features,” IEEE Trans. Image Process., vol. 25, no. 11, pp. 5012–\\n5024, 2016.\\n[147] J. Pan, E. Sayrol, X. Giro-i Nieto, K. McGuinness, and N. E. O’Connor,\\n“Shallow and deep convolutional networks for saliency prediction,” in\\nCVPR, 2016.\\n[148] J. Kuen, Z. Wang, and G. Wang, “Recurrent attentional networks for\\nsaliency detection,” in CVPR, 2016.\\n[149] Y. Tang, X. Wu, and W. Bu, “Deeply-supervised recurrent convolutional\\nneural network for saliency detection,” in ACM MM, 2016.\\n[150] X. Li, Y. Li, C. Shen, A. Dick, and A. Van Den Hengel, “Contextual\\nhypergraph modeling for salient object detection,” in ICCV, 2013.\\n[151] M.-M. Cheng, N. J. Mitra, X. Huang, P. H. Torr, and S.-M. Hu, “Global\\ncontrast based salient region detection,” IEEE Trans. Pattern Anal.\\nMach. Intell., vol. 37, no. 3, pp. 569–582, 2015.\\n[152] H. Jiang, J. Wang, Z. Yuan, Y. Wu, N. Zheng, and S. Li, “Salient object\\ndetection: A discriminative regional feature integration approach,” in\\nCVPR, 2013.\\n[153] G. Lee, Y.-W. Tai, and J. Kim, “Deep saliency with encoded low level\\ndistance map and high level features,” in CVPR, 2016.\\n[154] Z. Luo, A. Mishra, A. Achkar, J. Eichel, S. Li, and P.-M. Jodoin,\\n“Non-local deep features for salient object detection,” in CVPR, 2017.',\n",
       "  'metadata': {'total_pages': 21,\n",
       "   'file_path': '../datas/pdf_files/object_detection.pdf',\n",
       "   'title': '',\n",
       "   'doc_index': 18,\n",
       "   'author': '',\n",
       "   'creator': 'LaTeX with hyperref package',\n",
       "   'format': 'PDF 1.5',\n",
       "   'subject': '',\n",
       "   'producer': 'pdfTeX-1.40.17',\n",
       "   'moddate': '2019-04-17T00:45:22+00:00',\n",
       "   'creationDate': 'D:20190417004522Z',\n",
       "   'context_length': 9267,\n",
       "   'page': 18,\n",
       "   'trapped': '',\n",
       "   'keywords': '',\n",
       "   'source': '../datas/pdf_files/object_detection.pdf',\n",
       "   'modDate': 'D:20190417004522Z',\n",
       "   'creationdate': '2019-04-17T00:45:22+00:00'},\n",
       "  'similarity_score': 0.09672093391418457,\n",
       "  'distance': 0.9032790660858154,\n",
       "  'rank': 1},\n",
       " {'id': 'doc_c4f043c4_18',\n",
       "  'content': 'THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION\\n19\\n[92] S. Ren, K. He, R. Girshick, and J. Sun, “Faster r-cnn: Towards real-time\\nobject detection with region proposal networks,” IEEE Trans. Pattern\\nAnal. Mach. Intell., vol. 39, no. 6, pp. 1137–1149, 2017.\\n[93] C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna, “Rethink-\\ning the inception architecture for computer vision,” in CVPR, 2016.\\n[94] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan,\\nP. Doll´ar, and C. L. Zitnick, “Microsoft coco: Common objects in\\ncontext,” in ECCV, 2014.\\n[95] S. Bell, C. Lawrence Zitnick, K. Bala, and R. Girshick, “Inside-outside\\nnet: Detecting objects in context with skip pooling and recurrent neural\\nnetworks,” in CVPR, 2016.\\n[96] A. Arnab and P. H. S. Torr, “Pixelwise instance segmentation with a\\ndynamically instantiated network,” in CVPR, 2017.\\n[97] J. Dai, K. He, and J. Sun, “Instance-aware semantic segmentation via\\nmulti-task network cascades,” in CVPR, 2016.\\n[98] Y. Li, H. Qi, J. Dai, X. Ji, and Y. Wei, “Fully convolutional instance-\\naware semantic segmentation,” in CVPR, 2017.\\n[99] M. Jaderberg, K. Simonyan, A. Zisserman, and K. Kavukcuoglu,\\n“Spatial transformer networks,” in CVPR, 2015.\\n[100] S. Brahmbhatt, H. I. Christensen, and J. Hays, “Stuffnet: Using stuffto\\nimprove object detection,” in WACV, 2017.\\n[101] T. Kong, A. Yao, Y. Chen, and F. Sun, “Hypernet: Towards accurate\\nregion proposal generation and joint object detection,” in CVPR, 2016.\\n[102] A. Pentina, V. Sharmanska, and C. H. Lampert, “Curriculum learning\\nof multiple tasks,” in CVPR, 2015.\\n[103] J. Yim, H. Jung, B. Yoo, C. Choi, D. Park, and J. Kim, “Rotating your\\nface using multi-task deep neural network,” in CVPR, 2015.\\n[104] J. Li, X. Liang, J. Li, T. Xu, J. Feng, and S. Yan, “Multi-stage object\\ndetection with group recursive learning,” arXiv:1608.05159, 2016.\\n[105] Z. Cai, Q. Fan, R. S. Feris, and N. Vasconcelos, “A uniﬁed multi-scale\\ndeep convolutional neural network for fast object detection,” in ECCV,\\n2016.\\n[106] Y. Zhu, R. Urtasun, R. Salakhutdinov, and S. Fidler, “segdeepm:\\nExploiting segmentation and context in deep neural networks for object\\ndetection,” in CVPR, 2015.\\n[107] W. Byeon, T. M. Breuel, F. Raue, and M. Liwicki, “Scene labeling\\nwith lstm recurrent neural networks,” in CVPR, 2015.\\n[108] B. Moysset, C. Kermorvant, and C. Wolf, “Learning to detect and\\nlocalize many objects from few examples,” arXiv:1611.05664, 2016.\\n[109] X. Zeng, W. Ouyang, B. Yang, J. Yan, and X. Wang, “Gated bi-\\ndirectional cnn for object detection,” in ECCV, 2016.\\n[110] S. Gidaris and N. Komodakis, “Object detection via a multi-region and\\nsemantic segmentation-aware cnn model,” in CVPR, 2015.\\n[111] M. Schuster and K. K. Paliwal, “Bidirectional recurrent neural net-\\nworks,” IEEE Trans. Signal Process., vol. 45, pp. 2673–2681, 1997.\\n[112] S. Zagoruyko, A. Lerer, T.-Y. Lin, P. O. Pinheiro, S. Gross, S. Chin-\\ntala, and P. Doll´ar, “A multipath network for object detection,”\\narXiv:1604.02135, 2016.\\n[113] A. Shrivastava, A. Gupta, and R. Girshick, “Training region-based\\nobject detectors with online hard example mining,” in CVPR, 2016.\\n[114] S. Ren, K. He, R. Girshick, X. Zhang, and J. Sun, “Object detection\\nnetworks on convolutional feature maps,” IEEE Trans. Pattern Anal.\\nMach. Intell., vol. 39, no. 7, pp. 1476–1481, 2017.\\n[115] W. Ouyang, X. Wang, C. Zhang, and X. Yang, “Factors in ﬁnetuning\\ndeep model for object detection with long-tail distribution,” in CVPR,\\n2016.\\n[116] S. Hong, B. Roh, K.-H. Kim, Y. Cheon, and M. Park, “Pvanet:\\nLightweight deep neural networks for real-time object detection,”\\narXiv:1611.08588, 2016.\\n[117] W. Shang, K. Sohn, D. Almeida, and H. Lee, “Understanding and\\nimproving convolutional neural networks via concatenated rectiﬁed\\nlinear units,” in ICML, 2016.\\n[118] C. Szegedy, A. Toshev, and D. Erhan, “Deep neural networks for object\\ndetection,” in NIPS, 2013.\\n[119] P. O. Pinheiro, R. Collobert, and P. Doll´ar, “Learning to segment object\\ncandidates,” in NIPS, 2015.\\n[120] C. Szegedy, S. Reed, D. Erhan, D. Anguelov, and S. Ioffe, “Scalable,\\nhigh-quality object detection,” arXiv:1412.1441, 2014.\\n[121] M. Everingham, L. Van Gool, C. Williams, J. Winn, and A. Zisserman,\\n“The pascal visual object classes challenge 2012 (voc2012) results\\n(2012),” in http://www.pascal-network.org/challenges/VOC/voc2011/\\nworkshop/index.html, 2011.\\n[122] M. D. Zeiler and R. Fergus, “Visualizing and understanding convolu-\\ntional networks,” in ECCV, 2014.\\n[123] S. Xie, R. B. Girshick, P. Doll´ar, Z. Tu, and K. He, “Aggregated residual\\ntransformations for deep neural networks,” in CVPR, 2017.\\n[124] J. Dai, H. Qi, Y. Xiong, Y. Li, G. Zhang, H. Hu, and Y. Wei,\\n“Deformable convolutional networks,” arXiv:1703.06211, 2017.\\n[125] C. Rother, L. Bordeaux, Y. Hamadi, and A. Blake, “Autocollage,” ACM\\nTrans. on Graphics, vol. 25, no. 3, pp. 847–852, 2006.\\n[126] C. Jung and C. Kim, “A uniﬁed spectral-domain approach for saliency\\ndetection and its application to automatic object segmentation,” IEEE\\nTrans. Image Process., vol. 21, no. 3, pp. 1272–1283, 2012.\\n[127] W.-C. Tu, S. He, Q. Yang, and S.-Y. Chien, “Real-time salient object\\ndetection with a minimum spanning tree,” in CVPR, 2016.\\n[128] J. Yang and M.-H. Yang, “Top-down visual saliency via joint crf and\\ndictionary learning,” IEEE Trans. Pattern Anal. Mach. Intell., vol. 39,\\nno. 3, pp. 576–588, 2017.\\n[129] P. L. Rosin, “A simple method for detecting salient regions,” Pattern\\nRecognition, vol. 42, no. 11, pp. 2363–2371, 2009.\\n[130] T. Liu, Z. Yuan, J. Sun, J. Wang, N. Zheng, X. Tang, and H.-Y. Shum,\\n“Learning to detect a salient object,” IEEE Trans. Pattern Anal. Mach.\\nIntell., vol. 33, no. 2, pp. 353–367, 2011.\\n[131] J. Long, E. Shelhamer, and T. Darrell, “Fully convolutional networks\\nfor semantic segmentation,” in CVPR, 2015.\\n[132] D. Gao, S. Han, and N. Vasconcelos, “Discriminant saliency, the detec-\\ntion of suspicious coincidences, and applications to visual recognition,”\\nIEEE Trans. Pattern Anal. Mach. Intell., vol. 31, pp. 989–1005, 2009.\\n[133] S. Xie and Z. Tu, “Holistically-nested edge detection,” in ICCV, 2015.\\n[134] M. K¨ummerer, L. Theis, and M. Bethge, “Deep gaze i: Boost-\\ning saliency prediction with feature maps trained on imagenet,”\\narXiv:1411.1045, 2014.\\n[135] X. Huang, C. Shen, X. Boix, and Q. Zhao, “Salicon: Reducing the\\nsemantic gap in saliency prediction by adapting deep neural networks,”\\nin ICCV, 2015.\\n[136] L. Wang, H. Lu, X. Ruan, and M.-H. Yang, “Deep networks for saliency\\ndetection via local estimation and global search,” in CVPR, 2015.\\n[137] H. Cholakkal, J. Johnson, and D. Rajan, “Weakly supervised top-down\\nsalient object detection,” arXiv:1611.05345, 2016.\\n[138] R. Zhao, W. Ouyang, H. Li, and X. Wang, “Saliency detection by\\nmulti-context deep learning,” in CVPR, 2015.\\n[139] C¸ . Bak, A. Erdem, and E. Erdem, “Two-stream convolutional networks\\nfor dynamic saliency prediction,” arXiv:1607.04730, 2016.\\n[140] S. He, R. W. Lau, W. Liu, Z. Huang, and Q. Yang, “Supercnn: A su-\\nperpixelwise convolutional neural network for salient object detection,”\\nInt. J. of Comput. Vision, vol. 115, no. 3, pp. 330–344, 2015.\\n[141] X. Li, L. Zhao, L. Wei, M.-H. Yang, F. Wu, Y. Zhuang, H. Ling, and\\nJ. Wang, “Deepsaliency: Multi-task deep neural network model for\\nsalient object detection,” IEEE Trans. Image Process., vol. 25, no. 8,\\npp. 3919–3930, 2016.\\n[142] Y. Tang and X. Wu, “Saliency detection via combining region-level\\nand pixel-level predictions with cnns,” in ECCV, 2016.\\n[143] G. Li and Y. Yu, “Deep contrast learning for salient object detection,”\\nin CVPR, 2016.\\n[144] X. Wang, H. Ma, S. You, and X. Chen, “Edge preserving and\\nmulti-scale contextual neural network for salient object detection,”\\narXiv:1608.08029, 2016.\\n[145] M. Cornia, L. Baraldi, G. Serra, and R. Cucchiara, “A deep multi-level\\nnetwork for saliency prediction,” in ICPR, 2016.\\n[146] G. Li and Y. Yu, “Visual saliency detection based on multiscale deep\\ncnn features,” IEEE Trans. Image Process., vol. 25, no. 11, pp. 5012–\\n5024, 2016.\\n[147] J. Pan, E. Sayrol, X. Giro-i Nieto, K. McGuinness, and N. E. O’Connor,\\n“Shallow and deep convolutional networks for saliency prediction,” in\\nCVPR, 2016.\\n[148] J. Kuen, Z. Wang, and G. Wang, “Recurrent attentional networks for\\nsaliency detection,” in CVPR, 2016.\\n[149] Y. Tang, X. Wu, and W. Bu, “Deeply-supervised recurrent convolutional\\nneural network for saliency detection,” in ACM MM, 2016.\\n[150] X. Li, Y. Li, C. Shen, A. Dick, and A. Van Den Hengel, “Contextual\\nhypergraph modeling for salient object detection,” in ICCV, 2013.\\n[151] M.-M. Cheng, N. J. Mitra, X. Huang, P. H. Torr, and S.-M. Hu, “Global\\ncontrast based salient region detection,” IEEE Trans. Pattern Anal.\\nMach. Intell., vol. 37, no. 3, pp. 569–582, 2015.\\n[152] H. Jiang, J. Wang, Z. Yuan, Y. Wu, N. Zheng, and S. Li, “Salient object\\ndetection: A discriminative regional feature integration approach,” in\\nCVPR, 2013.\\n[153] G. Lee, Y.-W. Tai, and J. Kim, “Deep saliency with encoded low level\\ndistance map and high level features,” in CVPR, 2016.\\n[154] Z. Luo, A. Mishra, A. Achkar, J. Eichel, S. Li, and P.-M. Jodoin,\\n“Non-local deep features for salient object detection,” in CVPR, 2017.',\n",
       "  'metadata': {'title': '',\n",
       "   'context_length': 9267,\n",
       "   'total_pages': 21,\n",
       "   'file_path': '../datas/pdf_files/object_detection.pdf',\n",
       "   'trapped': '',\n",
       "   'source': '../datas/pdf_files/object_detection.pdf',\n",
       "   'moddate': '2019-04-17T00:45:22+00:00',\n",
       "   'keywords': '',\n",
       "   'creator': 'LaTeX with hyperref package',\n",
       "   'modDate': 'D:20190417004522Z',\n",
       "   'page': 18,\n",
       "   'subject': '',\n",
       "   'creationdate': '2019-04-17T00:45:22+00:00',\n",
       "   'format': 'PDF 1.5',\n",
       "   'author': '',\n",
       "   'producer': 'pdfTeX-1.40.17',\n",
       "   'doc_index': 18,\n",
       "   'creationDate': 'D:20190417004522Z'},\n",
       "  'similarity_score': 0.09672093391418457,\n",
       "  'distance': 0.9032790660858154,\n",
       "  'rank': 2},\n",
       " {'id': 'doc_4ae1bf0b_0',\n",
       "  'content': 'THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION\\n1\\nObject Detection with Deep Learning: A Review\\nZhong-Qiu Zhao, Member, IEEE, Peng Zheng,\\nShou-tao Xu, and Xindong Wu, Fellow, IEEE\\nAbstract—Due to object detection’s close relationship with\\nvideo analysis and image understanding, it has attracted much\\nresearch attention in recent years. Traditional object detection\\nmethods are built on handcrafted features and shallow trainable\\narchitectures. Their performance easily stagnates by constructing\\ncomplex ensembles which combine multiple low-level image\\nfeatures with high-level context from object detectors and scene\\nclassiﬁers. With the rapid development in deep learning, more\\npowerful tools, which are able to learn semantic, high-level,\\ndeeper features, are introduced to address the problems existing\\nin traditional architectures. These models behave differently\\nin network architecture, training strategy and optimization\\nfunction, etc. In this paper, we provide a review on deep\\nlearning based object detection frameworks. Our review begins\\nwith a brief introduction on the history of deep learning and\\nits representative tool, namely Convolutional Neural Network\\n(CNN). Then we focus on typical generic object detection\\narchitectures along with some modiﬁcations and useful tricks\\nto improve detection performance further. As distinct speciﬁc\\ndetection tasks exhibit different characteristics, we also brieﬂy\\nsurvey several speciﬁc tasks, including salient object detection,\\nface detection and pedestrian detection. Experimental analyses\\nare also provided to compare various methods and draw some\\nmeaningful conclusions. Finally, several promising directions and\\ntasks are provided to serve as guidelines for future work in\\nboth object detection and relevant neural network based learning\\nsystems.\\nIndex Terms—deep learning, object detection, neural network\\nI. INTRODUCTION\\nT\\nO gain a complete image understanding, we should not\\nonly concentrate on classifying different images, but\\nalso try to precisely estimate the concepts and locations of\\nobjects contained in each image. This task is referred as object\\ndetection [1][S1], which usually consists of different subtasks\\nsuch as face detection [2][S2], pedestrian detection [3][S2]\\nand skeleton detection [4][S3]. As one of the fundamental\\ncomputer vision problems, object detection is able to provide\\nvaluable information for semantic understanding of images\\nand videos, and is related to many applications, including\\nimage classiﬁcation [5], [6], human behavior analysis [7][S4],\\nface recognition [8][S5] and autonomous driving [9], [10].\\nMeanwhile, Inheriting from neural networks and related learn-\\ning systems, the progress in these ﬁelds will develop neural\\nnetwork algorithms, and will also have great impacts on object\\ndetection techniques which can be considered as learning\\nsystems. [11]–[14][S6]. However, due to large variations in\\nviewpoints, poses, occlusions and lighting conditions, it’s difﬁ-\\ncult to perfectly accomplish object detection with an additional\\nZhong-Qiu Zhao, Peng Zheng and Shou-Tao Xu are with the College of\\nComputer Science and Information Engineering, Hefei University of Technol-\\nogy, China. Xindong Wu is with the School of Computing and Informatics,\\nUniversity of Louisiana at Lafayette, USA.\\nManuscript received August xx, 2017; revised xx xx, 2017.\\nobject localization task. So much attention has been attracted\\nto this ﬁeld in recent years [15]–[18].\\nThe problem deﬁnition of object detection is to determine\\nwhere objects are located in a given image (object localization)\\nand which category each object belongs to (object classiﬁca-\\ntion). So the pipeline of traditional object detection models\\ncan be mainly divided into three stages: informative region\\nselection, feature extraction and classiﬁcation.\\nInformative region selection. As different objects may appear\\nin any positions of the image and have different aspect ratios\\nor sizes, it is a natural choice to scan the whole image with a\\nmulti-scale sliding window. Although this exhaustive strategy\\ncan ﬁnd out all possible positions of the objects, its short-\\ncomings are also obvious. Due to a large number of candidate\\nwindows, it is computationally expensive and produces too\\nmany redundant windows. However, if only a ﬁxed number of\\nsliding window templates are applied, unsatisfactory regions\\nmay be produced.\\nFeature extraction. To recognize different objects, we need\\nto extract visual features which can provide a semantic and\\nrobust representation. SIFT [19], HOG [20] and Haar-like [21]\\nfeatures are the representative ones. This is due to the fact\\nthat these features can produce representations associated with\\ncomplex cells in human brain [19]. However, due to the diver-\\nsity of appearances, illumination conditions and backgrounds,\\nit’s difﬁcult to manually design a robust feature descriptor to\\nperfectly describe all kinds of objects.\\nClassiﬁcation. Besides, a classiﬁer is needed to distinguish\\na target object from all the other categories and to make the\\nrepresentations more hierarchical, semantic and informative\\nfor visual recognition. Usually, the Supported Vector Machine\\n(SVM) [22], AdaBoost [23] and Deformable Part-based Model\\n(DPM) [24] are good choices. Among these classiﬁers, the\\nDPM is a ﬂexible model by combining object parts with\\ndeformation cost to handle severe deformations. In DPM, with\\nthe aid of a graphical model, carefully designed low-level\\nfeatures and kinematically inspired part decompositions are\\ncombined. And discriminative learning of graphical models\\nallows for building high-precision part-based models for a\\nvariety of object classes.\\nBased on these discriminant local feature descriptors and\\nshallow learnable architectures, state of the art results have\\nbeen obtained on PASCAL VOC object detection competition\\n[25] and real-time embedded systems have been obtained with\\na low burden on hardware. However, small gains are obtained\\nduring 2010-2012 by only building ensemble systems and\\nemploying minor variants of successful methods [15]. This fact\\nis due to the following reasons: 1) The generation of candidate\\nbounding boxes with a sliding window strategy is redundant,\\ninefﬁcient and inaccurate. 2) The semantic gap cannot be\\narXiv:1807.05511v2  [cs.CV]  16 Apr 2019',\n",
       "  'metadata': {'source': '../datas/pdf_files/object_detection.pdf',\n",
       "   'keywords': '',\n",
       "   'total_pages': 21,\n",
       "   'modDate': 'D:20190417004522Z',\n",
       "   'format': 'PDF 1.5',\n",
       "   'file_path': '../datas/pdf_files/object_detection.pdf',\n",
       "   'page': 0,\n",
       "   'subject': '',\n",
       "   'author': '',\n",
       "   'title': '',\n",
       "   'doc_index': 0,\n",
       "   'trapped': '',\n",
       "   'moddate': '2019-04-17T00:45:22+00:00',\n",
       "   'creationdate': '2019-04-17T00:45:22+00:00',\n",
       "   'producer': 'pdfTeX-1.40.17',\n",
       "   'creationDate': 'D:20190417004522Z',\n",
       "   'creator': 'LaTeX with hyperref package',\n",
       "   'context_length': 6347},\n",
       "  'similarity_score': 0.09054291248321533,\n",
       "  'distance': 0.9094570875167847,\n",
       "  'rank': 3},\n",
       " {'id': 'doc_ba9e49b3_0',\n",
       "  'content': 'THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION\\n1\\nObject Detection with Deep Learning: A Review\\nZhong-Qiu Zhao, Member, IEEE, Peng Zheng,\\nShou-tao Xu, and Xindong Wu, Fellow, IEEE\\nAbstract—Due to object detection’s close relationship with\\nvideo analysis and image understanding, it has attracted much\\nresearch attention in recent years. Traditional object detection\\nmethods are built on handcrafted features and shallow trainable\\narchitectures. Their performance easily stagnates by constructing\\ncomplex ensembles which combine multiple low-level image\\nfeatures with high-level context from object detectors and scene\\nclassiﬁers. With the rapid development in deep learning, more\\npowerful tools, which are able to learn semantic, high-level,\\ndeeper features, are introduced to address the problems existing\\nin traditional architectures. These models behave differently\\nin network architecture, training strategy and optimization\\nfunction, etc. In this paper, we provide a review on deep\\nlearning based object detection frameworks. Our review begins\\nwith a brief introduction on the history of deep learning and\\nits representative tool, namely Convolutional Neural Network\\n(CNN). Then we focus on typical generic object detection\\narchitectures along with some modiﬁcations and useful tricks\\nto improve detection performance further. As distinct speciﬁc\\ndetection tasks exhibit different characteristics, we also brieﬂy\\nsurvey several speciﬁc tasks, including salient object detection,\\nface detection and pedestrian detection. Experimental analyses\\nare also provided to compare various methods and draw some\\nmeaningful conclusions. Finally, several promising directions and\\ntasks are provided to serve as guidelines for future work in\\nboth object detection and relevant neural network based learning\\nsystems.\\nIndex Terms—deep learning, object detection, neural network\\nI. INTRODUCTION\\nT\\nO gain a complete image understanding, we should not\\nonly concentrate on classifying different images, but\\nalso try to precisely estimate the concepts and locations of\\nobjects contained in each image. This task is referred as object\\ndetection [1][S1], which usually consists of different subtasks\\nsuch as face detection [2][S2], pedestrian detection [3][S2]\\nand skeleton detection [4][S3]. As one of the fundamental\\ncomputer vision problems, object detection is able to provide\\nvaluable information for semantic understanding of images\\nand videos, and is related to many applications, including\\nimage classiﬁcation [5], [6], human behavior analysis [7][S4],\\nface recognition [8][S5] and autonomous driving [9], [10].\\nMeanwhile, Inheriting from neural networks and related learn-\\ning systems, the progress in these ﬁelds will develop neural\\nnetwork algorithms, and will also have great impacts on object\\ndetection techniques which can be considered as learning\\nsystems. [11]–[14][S6]. However, due to large variations in\\nviewpoints, poses, occlusions and lighting conditions, it’s difﬁ-\\ncult to perfectly accomplish object detection with an additional\\nZhong-Qiu Zhao, Peng Zheng and Shou-Tao Xu are with the College of\\nComputer Science and Information Engineering, Hefei University of Technol-\\nogy, China. Xindong Wu is with the School of Computing and Informatics,\\nUniversity of Louisiana at Lafayette, USA.\\nManuscript received August xx, 2017; revised xx xx, 2017.\\nobject localization task. So much attention has been attracted\\nto this ﬁeld in recent years [15]–[18].\\nThe problem deﬁnition of object detection is to determine\\nwhere objects are located in a given image (object localization)\\nand which category each object belongs to (object classiﬁca-\\ntion). So the pipeline of traditional object detection models\\ncan be mainly divided into three stages: informative region\\nselection, feature extraction and classiﬁcation.\\nInformative region selection. As different objects may appear\\nin any positions of the image and have different aspect ratios\\nor sizes, it is a natural choice to scan the whole image with a\\nmulti-scale sliding window. Although this exhaustive strategy\\ncan ﬁnd out all possible positions of the objects, its short-\\ncomings are also obvious. Due to a large number of candidate\\nwindows, it is computationally expensive and produces too\\nmany redundant windows. However, if only a ﬁxed number of\\nsliding window templates are applied, unsatisfactory regions\\nmay be produced.\\nFeature extraction. To recognize different objects, we need\\nto extract visual features which can provide a semantic and\\nrobust representation. SIFT [19], HOG [20] and Haar-like [21]\\nfeatures are the representative ones. This is due to the fact\\nthat these features can produce representations associated with\\ncomplex cells in human brain [19]. However, due to the diver-\\nsity of appearances, illumination conditions and backgrounds,\\nit’s difﬁcult to manually design a robust feature descriptor to\\nperfectly describe all kinds of objects.\\nClassiﬁcation. Besides, a classiﬁer is needed to distinguish\\na target object from all the other categories and to make the\\nrepresentations more hierarchical, semantic and informative\\nfor visual recognition. Usually, the Supported Vector Machine\\n(SVM) [22], AdaBoost [23] and Deformable Part-based Model\\n(DPM) [24] are good choices. Among these classiﬁers, the\\nDPM is a ﬂexible model by combining object parts with\\ndeformation cost to handle severe deformations. In DPM, with\\nthe aid of a graphical model, carefully designed low-level\\nfeatures and kinematically inspired part decompositions are\\ncombined. And discriminative learning of graphical models\\nallows for building high-precision part-based models for a\\nvariety of object classes.\\nBased on these discriminant local feature descriptors and\\nshallow learnable architectures, state of the art results have\\nbeen obtained on PASCAL VOC object detection competition\\n[25] and real-time embedded systems have been obtained with\\na low burden on hardware. However, small gains are obtained\\nduring 2010-2012 by only building ensemble systems and\\nemploying minor variants of successful methods [15]. This fact\\nis due to the following reasons: 1) The generation of candidate\\nbounding boxes with a sliding window strategy is redundant,\\ninefﬁcient and inaccurate. 2) The semantic gap cannot be\\narXiv:1807.05511v2  [cs.CV]  16 Apr 2019',\n",
       "  'metadata': {'file_path': '../datas/pdf_files/object_detection.pdf',\n",
       "   'keywords': '',\n",
       "   'creator': 'LaTeX with hyperref package',\n",
       "   'creationdate': '2019-04-17T00:45:22+00:00',\n",
       "   'source': '../datas/pdf_files/object_detection.pdf',\n",
       "   'creationDate': 'D:20190417004522Z',\n",
       "   'author': '',\n",
       "   'context_length': 6347,\n",
       "   'moddate': '2019-04-17T00:45:22+00:00',\n",
       "   'format': 'PDF 1.5',\n",
       "   'page': 0,\n",
       "   'total_pages': 21,\n",
       "   'modDate': 'D:20190417004522Z',\n",
       "   'trapped': '',\n",
       "   'producer': 'pdfTeX-1.40.17',\n",
       "   'doc_index': 0,\n",
       "   'subject': '',\n",
       "   'title': ''},\n",
       "  'similarity_score': 0.09054291248321533,\n",
       "  'distance': 0.9094570875167847,\n",
       "  'rank': 4},\n",
       " {'id': 'doc_0b1eda5d_20',\n",
       "  'content': 'THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION\\n21\\n[218] S. Sukhbaatar, A. Szlam, J. Weston, and R. Fergus, “End-to-end\\nmemory networks,” in NIPS, 2015.\\n[219] P. Dabkowski and Y. Gal, “Real time image saliency for black box\\nclassiﬁers,” in NIPS, 2017.\\n[220] B. Yang, J. Yan, Z. Lei, and S. Z. Li, “Craft objects from images,” in\\nCVPR, 2016.\\n[221] I. Croitoru, S.-V. Bogolin, and M. Leordeanu, “Unsupervised learning\\nfrom video to detect foreground objects in single images,” in ICCV,\\n2017.\\n[222] C. Wang, W. Ren, K. Huang, and T. Tan, “Weakly supervised object\\nlocalization with latent category learning,” in ECCV, 2014.\\n[223] D. P. Papadopoulos, J. R. R. Uijlings, F. Keller, and V. Ferrari,\\n“Training object class detectors with click supervision,” in CVPR, 2017.\\n[224] J. Huang, V. Rathod, C. Sun, M. Zhu, A. K. Balan, A. Fathi, I. Fischer,\\nZ. Wojna, Y. S. Song, S. Guadarrama, and K. Murphy, “Speed/accuracy\\ntrade-offs for modern convolutional object detectors,” in CVPR, 2017.\\n[225] Q. Li, S. Jin, and J. Yan, “Mimicking very efﬁcient network for object\\ndetection,” in CVPR, 2017.\\n[226] G. Hinton, O. Vinyals, and J. Dean, “Distilling the knowledge in a\\nneural network,” Comput. Sci., vol. 14, no. 7, pp. 38–39, 2015.\\n[227] A. Romero, N. Ballas, S. E. Kahou, A. Chassang, C. Gatta, and\\nY. Bengio, “Fitnets: Hints for thin deep nets,” Comput. Sci., 2014.\\n[228] X. Chen, K. Kundu, Y. Zhu, A. G. Berneshawi, H. Ma, S. Fidler, and\\nR. Urtasun, “3d object proposals for accurate object class detection,”\\nin NIPS, 2015.\\n[229] J. Dong, X. Fei, and S. Soatto, “Visual-inertial-semantic scene repre-\\nsentation for 3d object detection,” in CVPR, 2017.\\n[230] K. Kang, H. Li, T. Xiao, W. Ouyang, J. Yan, X. Liu, and X. Wang,\\n“Object detection in videos with tubelet proposal networks,” in CVPR,\\n2017.\\nZhong-Qiu Zhao is a professor at Hefei Univer-\\nsity of Technology, China. He obtained the Ph.D.\\ndegree in Pattern Recognition & Intelligent System\\nat University of Science and Technology, China, in\\n2007. From April 2008 to November 2009, he held a\\npostdoctoral position in image processing in CNRS\\nUMR6168 Lab Sciences de lInformation et des\\nSyst`emes, France. From January 2013 to December\\n2014, he held a research fellow position in image\\nprocessing at the Department of Computer Science\\nof Hongkong Baptist University, Hongkong, China.\\nHis research is about pattern recognition, image processing, and computer\\nvision.\\nPeng Zheng is a Ph.D. candidate at Hefei Uni-\\nversity of Technology since 2010. He received his\\nBachelor’s degree in 2010 from Hefei University of\\nTechnology. His interests cover pattern recognition,\\nimage processing and computer vision.\\nShou-tao Xu is a Master student at Hefei University\\nof Technology. His research interests cover pattern\\nrecognition, image processing, deep learning and\\ncomputer vision.\\nXindong Wu is an Alfred and Helen Lamson En-\\ndowed Professor in Computer Science, University\\nof Louisiana at Lafayette (USA), and a Fellow of\\nthe IEEE and the AAAS. He received his Ph.D.\\ndegree in Artiﬁcial Intelligence from the University\\nof Edinburgh, Britain. His research interests include\\ndata mining, knowledge-based systems, and Web in-\\nformation exploration. He is the Steering Committee\\nChair of the IEEE International Conference on Data\\nMining (ICDM), the Editor-in-Chief of Knowledge\\nand Information Systems (KAIS, by Springer), and\\na Series Editor of the Springer Book Series on Advanced Information and\\nKnowledge Processing (AI&KP). He was the Editor-in-Chief of the IEEE\\nTransactions on Knowledge and Data Engineering (TKDE, by the IEEE\\nComputer Society) between 2005 and 2008.',\n",
       "  'metadata': {'modDate': 'D:20190417004522Z',\n",
       "   'title': '',\n",
       "   'producer': 'pdfTeX-1.40.17',\n",
       "   'context_length': 3671,\n",
       "   'moddate': '2019-04-17T00:45:22+00:00',\n",
       "   'file_path': '../datas/pdf_files/object_detection.pdf',\n",
       "   'creationdate': '2019-04-17T00:45:22+00:00',\n",
       "   'format': 'PDF 1.5',\n",
       "   'trapped': '',\n",
       "   'creationDate': 'D:20190417004522Z',\n",
       "   'subject': '',\n",
       "   'source': '../datas/pdf_files/object_detection.pdf',\n",
       "   'creator': 'LaTeX with hyperref package',\n",
       "   'page': 20,\n",
       "   'author': '',\n",
       "   'keywords': '',\n",
       "   'total_pages': 21,\n",
       "   'doc_index': 20},\n",
       "  'similarity_score': 0.060172438621520996,\n",
       "  'distance': 0.939827561378479,\n",
       "  'rank': 5}]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_retriever.retrieve('what is object detection')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "670f17bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving documents for query: 'explain attention is all you need'\n",
      "Top K: 5, Score threshold: 0.0\n",
      "Generating embeddings for 1 texts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  2.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings with shape: (1, 384)\n",
      "Retrieved 2 documents (after filtering)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Attention Visualizations\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\\nthe verb ‘making’, completing the phrase ‘making...more difficult’. Attentions here shown only for\\nthe word ‘making’. Different colors represent different heads. Best viewed in color.\\n13'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_retriever.retrieve('explain attention is all you need')[0][\"content\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf61351",
   "metadata": {},
   "source": [
    "## Integration VectorDB Context Pipeline with LLM Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d46f7283",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple RAG Pipeline with LLM Output\n",
    "from langchain_groq import ChatGroq\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize GROQ LLM\n",
    "api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "llm = ChatGroq(api_key=api_key, temperature=0.1,model=\"moonshotai/kimi-k2-instruct-0905\", max_tokens=1024)\n",
    "\n",
    "## Build RAG Function : Retrieve context + response generation\n",
    "def rag_qa(query, retreiver, llm, top_k=3):\n",
    "    # Retrieve the context\n",
    "    results = retreiver.retrieve(query, top_k=top_k)\n",
    "    context = \"\\n\\n\".join([doc['content'] for doc in results]) if results else \"\"\n",
    "    if not context:\n",
    "        return \"No relevant context found to answer the question.\"\n",
    "    \n",
    "    # Generate the answer using LLM\n",
    "    prompt = f'''Use the following context to answer the question concisely.\\n\\n\n",
    "        Context: \n",
    "        {context}\n",
    "\n",
    "        Question: {query}\n",
    "        Answer:\n",
    "    '''\n",
    "    response = llm.invoke(prompt.format(context=context, query=query))\n",
    "\n",
    "    return response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5a823ed0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving documents for query: 'vision transformer'\n",
      "Top K: 3, Score threshold: 0.0\n",
      "Generating embeddings for 1 texts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  4.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings with shape: (1, 384)\n",
      "Retrieved 2 documents (after filtering)\n",
      "Answer: Vision Transformer (ViT) splits an image into fixed-size patches, linearly embeds each patch, adds 1-D position embeddings and a learnable classification token, then processes the sequence through a standard Transformer encoder whose output token is fed to an MLP classification head.\n"
     ]
    }
   ],
   "source": [
    "answer = rag_qa(\"vision transformer\", rag_retriever, llm, top_k=3)\n",
    "print(\"Answer:\", answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d8defa",
   "metadata": {},
   "source": [
    "## Enhanced RAG Pipeline Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5e42aee3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving documents for query: 'vision transformer'\n",
      "Top K: 3, Score threshold: 0.01\n",
      "Generating embeddings for 1 texts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 21.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings with shape: (1, 384)\n",
      "Retrieved 2 documents (after filtering)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: Vision Transformer (ViT) splits an image into fixed-size patches, linearly embeds each patch, adds 1-D position embeddings and a learnable classification token, then feeds the sequence to a standard Transformer encoder; the encoder’s output for the class token is used for classification via an MLP head.\n",
      "Sources: [{'source': '../datas/pdf_files/ViT.pdf', 'page': 2, 'score': 0.09102499485015869, 'preview': 'Published as a conference paper at ICLR 2021\\nTransformer Encoder\\nMLP \\nHead\\nVision Transformer (ViT)\\n*\\nLinear Projection of Flattened Patches\\n* Extra learnable\\n     [ cl ass]  embedding\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n0\\nPatch + Position \\nEmbedding\\nClass\\nBird\\nBall\\nCar\\n...\\nEmbedded \\nPatches\\nMulti-Head \\nAttention\\nNor...'}, {'source': '../datas/pdf_files/ViT.pdf', 'page': 2, 'score': 0.09102499485015869, 'preview': 'Published as a conference paper at ICLR 2021\\nTransformer Encoder\\nMLP \\nHead\\nVision Transformer (ViT)\\n*\\nLinear Projection of Flattened Patches\\n* Extra learnable\\n     [ cl ass]  embedding\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n0\\nPatch + Position \\nEmbedding\\nClass\\nBird\\nBall\\nCar\\n...\\nEmbedded \\nPatches\\nMulti-Head \\nAttention\\nNor...'}]\n",
      "Confidence: 0.09102499485015869\n",
      "Context Preview: Published as a conference paper at ICLR 2021\n",
      "Transformer Encoder\n",
      "MLP \n",
      "Head\n",
      "Vision Transformer (ViT)\n",
      "*\n",
      "Linear Projection of Flattened Patches\n",
      "* Extra learnable\n",
      "     [ cl ass]  embedding\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "0\n",
      "Patch + Position \n",
      "Embedding\n",
      "Class\n",
      "Bird\n",
      "Ball\n",
      "Car\n",
      "...\n",
      "Embedded \n",
      "Patches\n",
      "Multi-Head \n",
      "Attention\n",
      "Nor\n"
     ]
    }
   ],
   "source": [
    "# --- Enhanced RAG Pipeline Features ---\n",
    "def rag_advanced(query, retriever, llm, top_k=5, min_score=0.2, return_context=False):\n",
    "    \"\"\"\n",
    "    RAG pipeline with extra features:\n",
    "    - Returns answer, sources, confidence score, and optionally full context.\n",
    "    \"\"\"\n",
    "    results = retriever.retrieve(query, top_k=top_k, score_threshold=min_score)\n",
    "    if not results:\n",
    "        return {'answer': 'No relevant context found.', 'sources': [], 'confidence': 0.0, 'context': ''}\n",
    "    \n",
    "    # Prepare context and sources\n",
    "    context = \"\\n\\n\".join([doc['content'] for doc in results])\n",
    "    sources = [{\n",
    "        'source': doc['metadata'].get('source_file', doc['metadata'].get('source', 'unknown')),\n",
    "        'page': doc['metadata'].get('page', 'unknown'),\n",
    "        'score': doc['similarity_score'],\n",
    "        'preview': doc['content'][:300] + '...'\n",
    "    } for doc in results]\n",
    "    confidence = max([doc['similarity_score'] for doc in results])\n",
    "    \n",
    "    # Generate answer\n",
    "    prompt = f\"\"\"Use the following context to answer the question concisely.\\nContext:\\n{context}\\n\\nQuestion: {query}\\n\\nAnswer:\"\"\"\n",
    "    response = llm.invoke([prompt.format(context=context, query=query)])\n",
    "    \n",
    "    output = {\n",
    "        'answer': response.content,\n",
    "        'sources': sources,\n",
    "        'confidence': confidence\n",
    "    }\n",
    "    if return_context:\n",
    "        output['context'] = context\n",
    "    return output\n",
    "\n",
    "# Example usage:\n",
    "result = rag_advanced(\"vision transformer\", rag_retriever, llm, top_k=3, min_score=0.01, return_context=True)\n",
    "print(\"Answer:\", result['answer'])\n",
    "print(\"Sources:\", result['sources'])\n",
    "print(\"Confidence:\", result['confidence'])\n",
    "print(\"Context Preview:\", result['context'][:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a368e4da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving documents for query: 'vision transformer'\n",
      "Top K: 3, Score threshold: 0.01\n",
      "Generating embeddings for 1 texts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings with shape: (1, 384)\n",
      "Retrieved 2 documents (after filtering)\n",
      "Streaming answer:\n",
      "Use the following context to answer the question concisely.\n",
      "Context:\n",
      "Published a"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s a conference paper at ICLR 2021\n",
      "Transformer Encoder\n",
      "MLP \n",
      "Head\n",
      "Vision Transform"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "er (ViT)\n",
      "*\n",
      "Linear Projection of Flattened Patches\n",
      "* Extra learnable\n",
      "     [ cl ass]  embedding\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "0\n",
      "Patch + Position \n",
      "Embedding\n",
      "Class\n",
      "Bird\n",
      "Ball\n",
      "Car\n",
      "...\n",
      "Embedded \n",
      "Patches\n",
      "Multi-Head \n",
      "Attention\n",
      "Norm\n",
      "MLP\n",
      "Norm\n",
      "+\n",
      "L x\n",
      "+\n",
      "Transformer Encoder\n",
      "Figure 1: Model overview. We split an image into ﬁxed-size patches, linearly embed each of them,\n",
      "add position embeddings, and feed the resulting sequence of vectors to a standard Transformer\n",
      "encoder. In order to perform classiﬁcation, we use the standard approach of adding an extra learnable\n",
      "“classiﬁcation token” to the sequence. The illustration of the Transformer encoder was inspired by\n",
      "Vaswani et al. (2017).\n",
      "3\n",
      "METHOD\n",
      "In model design we follow the original Transformer (Vaswani et al., 2017) as closely as possible.\n",
      "An advantage of this intentionally simple setup is that scalable NLP Transformer architectures – and\n",
      "their efﬁcient implementations – can be used almost out of the box.\n",
      "3.1\n",
      "VISION TRANSFORMER (VIT)\n",
      "An overview of the model is depicted in Figure 1. The standard Transformer receives as input a 1D\n",
      "sequence of token embeddings. To handle 2D images, we reshape the image x ∈RH×W ×C into a\n",
      "sequence of ﬂattened 2D patches xp ∈RN×(P 2·C), where (H, W) is the resolution of the original\n",
      "image, C is the number of channels, (P, P) is the resolution of each image patch, and N = HW/P 2\n",
      "is the resulting number of patches, which also serves as the effective input sequence length for the\n",
      "Transformer. The Transformer uses constant latent vector size D through all of its layers, so we\n",
      "ﬂatten the patches and map to D dimensions with a trainable linear projection (Eq. 1). We refer to\n",
      "the output of this projection as the patch embeddings.\n",
      "Similar to BERT’s [class] token, we prepend a learnable embedding to the sequence of embed-\n",
      "ded patches (z0\n",
      "0 = xclass), whose state at the output of the Transformer encoder (z0\n",
      "L) serves as the\n",
      "image representation y (Eq. 4). Both during pre-training and ﬁne-tuning, a classiﬁcation head is at-\n",
      "tached to z0\n",
      "L. The classiﬁcation head is implemented by a MLP with one hidden layer at pre-training\n",
      "time and by a single linear layer at ﬁne-tuning time.\n",
      "Position embeddings are added to the patch embeddings to retain positional information. We use\n",
      "standard learnable 1D position embeddings, since we have not observed signiﬁcant performance\n",
      "gains from using more advanced 2D-aware position embeddings (Appendix D.4). The resulting\n",
      "sequence of embedding vectors serves as input to the encoder.\n",
      "The Transformer encoder (Vaswani et al., 2017) consists of alternating layers of multiheaded self-\n",
      "attention (MSA, see Appendix A) and MLP blocks (Eq. 2, 3). Layernorm (LN) is applied before\n",
      "every block, and residual connections after every block (Wang et al., 2019; Baevski & Auli, 2019).\n",
      "3\n",
      "\n",
      "Published as a conference paper at ICLR 2021\n",
      "Transformer Encoder\n",
      "MLP \n",
      "Head\n",
      "Vision Transformer (ViT)\n",
      "*\n",
      "Linear Projection of Flattened Patches\n",
      "* Extra learnable\n",
      "     [ cl ass]  embedding\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "0\n",
      "Patch + Position \n",
      "Embedding\n",
      "Class\n",
      "Bird\n",
      "Ball\n",
      "Car\n",
      "...\n",
      "Embedded \n",
      "Patches\n",
      "Multi-Head \n",
      "Attention\n",
      "Norm\n",
      "MLP\n",
      "Norm\n",
      "+\n",
      "L x\n",
      "+\n",
      "Transformer Encoder\n",
      "Figure 1: Model overview. We split an image into ﬁxed-size patches, linearly embed each of them,\n",
      "add position embeddings, and feed the resulting sequence of vectors to a standard Transformer\n",
      "encoder. In order to perform classiﬁcation, we use the standard approach of adding an extra learnable\n",
      "“classiﬁcation token” to the sequence. The illustration of the Transformer encoder was inspired by\n",
      "Vaswani et al. (2017).\n",
      "3\n",
      "METHOD\n",
      "In model design we follow the original Transformer (Vaswani et al., 2017) as closely as possible.\n",
      "An advantage of this intentionally simple setup is that scalable NLP Transformer architectures – and\n",
      "their efﬁcient implementations – can be used almost out of the box.\n",
      "3.1\n",
      "VISION TRANSFORMER (VIT)\n",
      "An overview of the model is depicted in Figure 1. The standard Transformer receives as input a 1D\n",
      "sequence of token embeddings. To handle 2D images, we reshape the image x ∈RH×W ×C into a\n",
      "sequence of ﬂattened 2D patches xp ∈RN×(P 2·C), where (H, W) is the resolution of the original\n",
      "image, C is the number of channels, (P, P) is the resolution of each image patch, and N = HW/P 2\n",
      "is the resulting number of patches, which also serves as the effective input sequence length for the\n",
      "Transformer. The Transformer uses constant latent vector size D through all of its layers, so we\n",
      "ﬂatten the patches and map to D dimensions with a trainable linear projection (Eq. 1). We refer to\n",
      "the output of this projection as the patch embeddings.\n",
      "Similar to BERT’s [class] token, we prepend a learnable embedding to the sequence of embed-\n",
      "ded patches (z0\n",
      "0 = xclass), whose state at the output of the Transformer encoder (z0\n",
      "L) serves as the\n",
      "image representation y (Eq. 4). Both during pre-training and ﬁne-tuning, a classiﬁcation head is at-\n",
      "tached to z0\n",
      "L. The classiﬁcation head is implemented by a MLP with one hidden layer at pre-training\n",
      "time and by a single linear layer at ﬁne-tuning time.\n",
      "Position embeddings are added to the patch embeddings to retain positional information. We use\n",
      "standard learnable 1D position embeddings, since we have not observed signiﬁcant performance\n",
      "gains from using more advanced 2D-aware position embeddings (Appendix D.4). The resulting\n",
      "sequence of embedding vectors serves as input to the encoder.\n",
      "The Transformer encoder (Vaswani et al., 2017) consists of alternating layers of multiheaded self-\n",
      "attention (MSA, see Appendix A) and MLP blocks (Eq. 2, 3). Layernorm (LN) is applied before\n",
      "every block, and residual connections after every block (Wang et al., 2019; Baevski & Auli, 2019).\n",
      "3\n",
      "\n",
      "Question: vision transformer\n",
      "\n",
      "Answer:\n",
      "\n",
      "Final Answer: Vision Transformer (ViT) splits an image into fixed-size patches, linearly embeds each patch, adds 1-D position embeddings and a learnable classification token, then feeds the resulting sequence to a standard Transformer encoder whose output token is used for classification.\n",
      "\n",
      "Citations:\n",
      "[1] ../datas/pdf_files/ViT.pdf (page 2)\n",
      "[2] ../datas/pdf_files/ViT.pdf (page 2)\n",
      "Summary: Vision Transformer divides an image into fixed patches, embeds them with position info and a class token, then processes the sequence through a standard Transformer encoder for classification.\n",
      "History: {'question': 'vision transformer', 'answer': 'Vision Transformer (ViT) splits an image into fixed-size patches, linearly embeds each patch, adds 1-D position embeddings and a learnable classification token, then feeds the resulting sequence to a standard Transformer encoder whose output token is used for classification.', 'sources': [{'source': '../datas/pdf_files/ViT.pdf', 'page': 2, 'score': 0.09102499485015869, 'preview': 'Published as a conference paper at ICLR 2021\\nTransformer Encoder\\nMLP \\nHead\\nVision Transformer (ViT)\\n*\\nLinear Projection ...'}, {'source': '../datas/pdf_files/ViT.pdf', 'page': 2, 'score': 0.09102499485015869, 'preview': 'Published as a conference paper at ICLR 2021\\nTransformer Encoder\\nMLP \\nHead\\nVision Transformer (ViT)\\n*\\nLinear Projection ...'}], 'summary': 'Vision Transformer divides an image into fixed patches, embeds them with position info and a class token, then processes the sequence through a standard Transformer encoder for classification.'}\n"
     ]
    }
   ],
   "source": [
    "# --- Advanced RAG Pipeline: Streaming, Citations, History, Summarization ---\n",
    "from typing import List, Dict, Any\n",
    "import time\n",
    "\n",
    "class AdvancedRAGPipeline:\n",
    "    def __init__(self, retriever, llm):\n",
    "        self.retriever = retriever\n",
    "        self.llm = llm\n",
    "        self.history = []  # Store query history\n",
    "\n",
    "    def query(self, question: str, top_k: int = 5, min_score: float = 0.2, stream: bool = False, summarize: bool = False) -> Dict[str, Any]:\n",
    "        # Retrieve relevant documents\n",
    "        results = self.retriever.retrieve(question, top_k=top_k, score_threshold=min_score)\n",
    "        if not results:\n",
    "            answer = \"No relevant context found.\"\n",
    "            sources = []\n",
    "            context = \"\"\n",
    "        else:\n",
    "            context = \"\\n\\n\".join([doc['content'] for doc in results])\n",
    "            sources = [{\n",
    "                'source': doc['metadata'].get('source_file', doc['metadata'].get('source', 'unknown')),\n",
    "                'page': doc['metadata'].get('page', 'unknown'),\n",
    "                'score': doc['similarity_score'],\n",
    "                'preview': doc['content'][:120] + '...'\n",
    "            } for doc in results]\n",
    "            # Streaming answer simulation\n",
    "            prompt = f\"\"\"Use the following context to answer the question concisely.\\nContext:\\n{context}\\n\\nQuestion: {question}\\n\\nAnswer:\"\"\"\n",
    "            if stream:\n",
    "                print(\"Streaming answer:\")\n",
    "                for i in range(0, len(prompt), 80):\n",
    "                    print(prompt[i:i+80], end='', flush=True)\n",
    "                    time.sleep(0.05)\n",
    "                print()\n",
    "            response = self.llm.invoke([prompt.format(context=context, question=question)])\n",
    "            answer = response.content\n",
    "\n",
    "        # Add citations to answer\n",
    "        citations = [f\"[{i+1}] {src['source']} (page {src['page']})\" for i, src in enumerate(sources)]\n",
    "        answer_with_citations = answer + \"\\n\\nCitations:\\n\" + \"\\n\".join(citations) if citations else answer\n",
    "\n",
    "        # Optionally summarize answer\n",
    "        summary = None\n",
    "        if summarize and answer:\n",
    "            summary_prompt = f\"Summarize the following answer in 2 sentences:\\n{answer}\"\n",
    "            summary_resp = self.llm.invoke([summary_prompt])\n",
    "            summary = summary_resp.content\n",
    "\n",
    "        # Store query history\n",
    "        self.history.append({\n",
    "            'question': question,\n",
    "            'answer': answer,\n",
    "            'sources': sources,\n",
    "            'summary': summary\n",
    "        })\n",
    "\n",
    "        return {\n",
    "            'question': question,\n",
    "            'answer': answer_with_citations,\n",
    "            'sources': sources,\n",
    "            'summary': summary,\n",
    "            'history': self.history\n",
    "        }\n",
    "\n",
    "# Example usage:\n",
    "adv_rag = AdvancedRAGPipeline(rag_retriever, llm)\n",
    "result = adv_rag.query(\"vision transformer\", top_k=3, min_score=0.01, stream=True, summarize=True)\n",
    "print(\"\\nFinal Answer:\", result['answer'])\n",
    "print(\"Summary:\", result['summary'])\n",
    "print(\"History:\", result['history'][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b065d14",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
